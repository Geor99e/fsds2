---
title: "Reproducible Analysis"
---

The *Reproducible Analysis* is worth {{< var assess.group-code-weight >}} of the [Group Assessment](./group.qmd).

The *Reproducible Analysis* must be written using Python in a Quarto Markdown Document (QMD file). You are free to draw on concepts and methods covered in both *Quantitative Methods* and *GIS*, but must still write the code in Python (e.g. adapting something from R in the GIS module to Python). 

## Format 

You will be submitting a *runnable* markdown document (`.qmd` file) that includes: your group's name and all student ids. We will supply a template closer to the deadline.

## How We Measure Reproducibility

We will assess reproducibility by selecting "Restart Kernel and Run All" using the `{{< var docker.name >}}` Docker environment. If you have made use of another Docker image then you must clearly signpost this at the start of your notebook so that we know to select a different image. We will *not* install libraries 'by-hand' in an *ad hoc* manner order to test the reproducibility of your work.

::: {.callout-warning}
## Reproducibility 

 To ensure reproducibility, markers must be able to run `quarto render <your file.qmd>` and **reproduce your _entire analysis_**. This includes downloading and extracting data (we will provide a function for the main data set), cleaning, transformation, clustering... charts, tables, etc.
:::

::: {.callout-tip} 
## For Large Workflows

  If you need to provide supplementary or partially-processed data (see section below) then you can provide this via Dropbox, OneDrive (public sharing link), or some other *robust* cloud solution that will be accessible from the marker's system.
:::

If you have made use of one or more libraries that are not part of the Docker image then you can install these using `! pip install`; however, if you take this approach then you should *also* 'place nice' by checking first to see if the library is already installed using `try... except` code that you can find on Stack Overflow and elsewhere (you will need to look this up).

## Data and Resources Used

It is also up to you to ensure that all relevant data are available via a valid URL for downloading and running. You may host your data anywhere you like, but please bear in mind that the markers will be based in the U.K. so some servers may be inaccessible. For very small data sets we'd recommend a GitHub repo, but for larger ones a Dropbox or OneDrive link would be more appropriate (you will need to check that the link you've created gives permission to anyone to download).

## Time-Consuming Code 

If your analysis has a particularly time-consuming stage (e.g. Named-Entity Recognition or Part-of-Speech tagging) then you can provide partially-processed data: comment out the code up to the point where you have generated the 'expensive' data set but leave it in the markdown document. That way we can see how you generated the data without it being part of the reproducibility stage.

## Other Requirements

You must maintain a copy of the submission in GitHub so that we can review contributions if necessary. 

<!-- 

## Supporting Documents

- A [marking rubric](./Group_Rubric.pdf) is available.
- A [proposal outline](https://github.com/jreades/fsds/blob/master/assessments/Project_Outline.qmd) template is available (see: [example](Project_Outline.pdf)).

-->
