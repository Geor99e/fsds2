---
title: "Practical 4: Object-Oriented Programming"
subtitle: "Getting to grips with Functions & Packages"
jupyter: python3
filters:
  - qna
  - quarto
---

| Complete | Part 1: Foundations | Part 2: Data | Part 3: Analysis |     |
| :------- | :------------------ | :----------- | :--------------- | --: |
| 30% | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9617; | &#9617;&#9617;&#9617;&#9617;&#9617;&#9617; | &#9617;&#9617;&#9617;&#9617;&#9617;&#9617; | 4/10

In this notebook the priorities here should be **Tasks 1â€“4** because they take you through the process of building a function incrementally by gradually adding complexity and sophistication rather than doing everything in one go. You will get an extra chance to revisit the differences between LoLs and DoLs because you will undoubtedly encounter and make use of these data structures even *after* you become a skillfull Python programmer.

**Task 5** is something you will probably want to revist when you get stuck into the group work because packages of functions are useful when multiple people are working on the same code. So if someone edits 'their' package it doesn't trigger conflicts with anyone else's code: you can work a *bit* in isolation without creating conflicts in Git that needs to be continually resolved. 

**Task 6** will help you to understand *how* to build your own classes in greater detail, but it is enough to understand that classes exist in hierarchies (as this was also covered in the live session).

::: {.callout-warning}

This is a very long practical and it's _not_ expected that you will get through it in the alloted time.

::: 

::: {.callout-tip}

#### Group Sign-Up

You should now make it a priority [Sign Up](https://forms.office.com/e/2ij0sWHnpR)!

:::

Practical 3 is hard, so I want to provide _another_ chance for the concepts to bed in before we use them in an *object-oriented way through Pandas*. Yes, Week 5 will show how we combine concepts covered over the preceding two weeks in *practice* to do data science. 

So remember the finding from last week: if we don't really care about column order, then a dictionary of lists would be a nice way to handle data. And why should we care about column order? With our CSV file we saw what a pain it was to fix things when even a tiny thing like the layout of the columns changed. But if, instead, we could just reference the 'Description' column in the data set then it doesn't matter where that column actually is *and* we would know that all the descriptions would be *text*, while all the populations or prices would be *numbers*. Why is that? 

::: {.callout-note}

#### &#128279; Connections

This task briefly recaps the final part of [Practical 2](https://jreades.github.io/fsds/sessions/week3.html#practical) and builds on the [DOLs to Data](https://jreades.github.io/fsds/sessions/week3.html#lectures) and [Functions](https://jreades.github.io/fsds/sessions/week3.html#lectures) lectures.

:::


# Why 'Obvious' is Not Always 'Right' (Revisited)

## The Way That Doesn't Work

::: {.callout-tip collapse="true"}

#### Difficulty level: Low (this time around).

:::

Here are four rows of 'data' for city sizes organised by _row_ as a list-of-lists. Try printing out *just* the cities contained in the data:

```{python}
#| echo: true
myData = [
    ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], 
    ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], 
    ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], 
    ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']
]
```

### Print a List of Cities

:::: {.panel-tabset}

#### Question 

Print out a list of every city in the data set:

```python
cities = []

???

print("The cities in the data set are: " + ", ".join(cities))
```

#### Answer

Print out a list of every city in the data set:

```{python}
#| echo: true
cities = []

col    = myData[0].index('Name')
for i in range(1, len(myData)):
    cities.append(myData[i][col])

print("The cities in the data set are: " + ", ".join(cities))
```

::::

### Is Edinburgh in the List?

Now write code to find out if `Edinburgh` is included in the list of data:

:::: {.panel-tabset}

### Question 

```python
col   = myData[0].index('Name')
found = False
for i in range(1, len(??)):
    if myData[i][col] == ??:
        print("Found Edinburgh in the data set!")
        found = True
        break

if found == ??:
    print("Didn't find Edinburgh in the data set.")
```

### Answer

```{python}
#| echo: true
col   = myData[0].index('Name')
found = False
for i in range(1, len(myData)):
    if myData[i][col] == 'Edinburgh':
        print("Found Edinburgh in the data set!")
        found = True
        break

if found == False:
    print("Didn't find Edinburgh in the data set.")
```

::::

## The Way That Does Work

::: {.callout-tip collapse="true"}

**Difficulty level**: Low (this time around).

:::

Compare that code to how it works for a dictionary-of-lists organised by _column_. Now try printing out the cities in the data:

```{python}
#| echo: true
myData = {
    'id'         : [0, 1, 2, 3, 4, 5],
    'Name'       : ['Greater London', 'Greater Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],
    'Rank'       : [1, 2, 3, 4, 5, 6],
    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],
    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],
    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],
}
```

### Print a List of Cities

Print out a list of every city in the data set:

:::: {.panel-tabset}

#### Question

```python
print(", ".join(??))
```

#### Answer

```{python}
#| echo: true
print(", ".join(myData['Name']))
```

::::

### Is Edinburgh in the List?

Now write code to find out if `Edinburgh` is included in the list of data:

:::: {.panel-tabset}

#### Question

```python
if 'Edinburgh' in ??:
    print("Found Edinburgh in the data set!")
else:
    print("Didn't find Edinburgh in the data set.")
```

#### Answer

```{python}
if 'Edinburgh' in myData['Name']:
    print("Found Edinburgh in the data set!")
else:
    print("Didn't find Edinburgh in the data set.")
```

::::

See how even basic questions like "Is Edinburgh in our list of data?" are suddenly easy to answer? We no longer need to loop over the entire data set in order to find one data point. In addition, we know that everything in the 'Name' column will be a string, and that everything in the 'Longitude' column is a float, while the 'Population' column contains integers. So that's made life easier already. But let's test this out and see how it works.

# Appending a Column

To give you a sense of how scaleable this approach to data is let's add a new column where the city population is standardised to a z-score. Remember that the format for the z-score (when dealing with a sample) is: 

$$
z = \frac{x - \bar{x}}{s}
$$

The population standard deviation (by which I mean, if you are dealing with *all* the data, and not a subsample as we are here) is:

$$
z = \frac{x - \mu}{\sigma}
$$

It's the same thing, just different notation.

## Calculate Mean & Std Dev

::: {.callout-tip collapse="true"}

#### Difficulty level: Low-ish.

:::

Let's start by calculating the sample mean and standard deviation (use Google: `Python numpy mean...`):

:::: {.panel-tabset}

#### Question

```python
import numpy as np
# Use numpy functions to calculate mean and standard deviation
mean = np.??(myData['Population'])
std  = np.??(??)
print(f"City distribution has a mean {mean:,.0f} and standard deviation of {std:,.2f}.")
```

#### Answer

```{python}
#| echo: true
import numpy as np
# Use numpy functions to calculate mean and standard deviation
mean = np.mean(myData['Population'])
std  = np.std(myData['Population'])
print(f"City distribution has a mean {mean:,.0f} and standard deviation of {std:,.2f}.")
```

::::

`numpy` gives us a way to calculate the mean and standard deviation _quickly_ and without having to reinvent the wheel. The other potentially new thing here is `{std:,.2f}`. This is about [string formatting](https://www.w3schools.com/python/ref_string_format.asp) and the main thing to recognise is that this means 'format this float with commas separating the thousands/millions and 2 digits to the right'. The link I've provided uses the slightly older approach of `<str>.format()` but the formatting approach is the same.

## For Loops Without For Loops

::: {.callout-warning collapse="true"}

#### Difficulty level: Medium.

:::

Now we're going to see something called a **List Comprehension**.

In Python you will see code like this a lot: `[x for x in list]`. This syntax is known as a 'list comprehension' and is basically a `for` loop on one line with the output being assigned to a list. So we can apply an operation (converting to a string, subtracting a value, etc.) to every item in a list without writing out a full for loop.

Here's a quick example just to show you what's going on:

```{python}
#| echo: true
demo = range(0,10) # <- a *range* of numbers between 0 and 9 (stop at 10)
print([x**2 for x in demo]) # square every element of demo
```

Now let's apply this to our problem. We calculated the the mean and standard deviation above, so now we want to apply the z-score formula to every element of the Population list... 

:::: {.panel-tabset}

#### Question

```python
rs = [(x - ??)/?? for x in myData['Population']] # rs == result set
print([f"{x:.3f}" for x in rs])
```

#### Answer

```{python}
#| echo: true
rs = [(x - mean)/std for x in myData['Population']] # rs == result set
print([f"{x:.3f}" for x in rs])
```

::::

## Appending

::: {.callout-tip collapse="true"}

#### Difficulty level: trivial

:::

And now let's add it to the data set:

```{python}
#| echo: true
myData['Std. Population'] = rs
print(myData['Std. Population'])
```

And just to show how everything is in a single data structure:

```{python}
#| echo: true
for c in myData['Name']:
    idx = myData['Name'].index(c)
    print(f"{c} has a population of {myData['Population'][idx]:,} and standardised score of {myData['Std. Population'][idx]:.3f}")
```

# 'Functionalising'

Let's start trying to pull what we've learned over the past two weeks together by creating a a set of functions that will help us to:

1. Download a file from a URL (checking if it has already _been_ downloaded to save bandwidth).
2. Parse it as a CSV file and...
3. Convert it to a Dictionary-of-Lists
4. Perform some simple calculations using the resulting data.

To be honest, there's not going to be much about writing our _own_ objects here, but we will be making use of them and, conceptually, an understanding of objects and classes is going to be super-useful for understanding what we're doing in the remainder of the term!

## Downloading from a URL

Let's focus on the first part *first* because that's the precondition for everything else. If we can get the 'download a file from a URL' working then the rest will gradually fall into place through *iterative* improvments!

### Finding an Existing Answer

::: {.callout-tip collapse="true"}

#### Difficulty level: Low

:::

First, let's be sensibly lazy--we've already written code to read a file from the Internet and turn it into a list of lists. So I've copy+pasted that into the code block below since we're going to start from this point; however, just to help you check your own understanding, I've removed a few bits and replaced them with `??`. Sorry, it's good practice. ðŸ˜ˆ

:::: {.panel-tabset}

#### Question

```python
from urllib.request import urlopen
import csv

url = "https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv"

urlData = [] # Somewhere to store the data

response = urlopen(url) #Â Get the data using the urlopen function
csvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader

for row in csvfile:
    urlData.append(??)

print("urlData has " + str(len(urlData)) + " rows and " + str(len(urlData[0])) + " columns.")
print(urlData[-1]) # Check it worked!
```

You should get `urlData has 11 rows and 4 columns.` and a row that looks like this: `['Bangor', '18808', '53.228', '-4.128']`.

#### Answer

```{python}
#| echo: true
from urllib.request import urlopen
import csv

url = "https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv"

urlData = [] # Somewhere to store the data

response = urlopen(url) #Â Get the data using the urlopen function
csvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader

for row in csvfile:
    urlData.append(row)

print("urlData has " + str(len(urlData)) + " rows and " + str(len(urlData[0])) + " columns.")
print(urlData[-1]) # Check it worked!
```

::::

### Getting Organised

::: {.callout-tip collapse="true"}

#### Difficulty level: Low

:::

Let's take the code above and modify it so that it is:

1. A function that takes two arguments: a URL; and a destination filename.
2. Implemented as a function that checks if a file exists already before downloading it again.

You will find that the `os` module helps here because of the `path` function. And you will [need to Google](https://lmgtfy.app/?q=check+if+file+exists+python) how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an _explanation_ included of why a particular answer is a 'good one'. I also look at which answers got the most votes (not always the same as the one that was the 'accepted answer'). In this particular case, I also found [this answer](https://careerkarma.com/blog/python-check-if-file-exists/) useful.

I would start by setting my inputs:

```{python}
#| echo: true
import os
url = "https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv"
out = os.path.join('data','Wikipedia-Cities.csv') # Print `out` if you aren't sure what this has done!
```

### Sketching the Function

::: {.callout-tip collapse="true"}

#### Difficulty level: Low, if you've watched the videos...

:::

Then I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:

:::: {.panel-tabset}

#### Question

```python
from urllib.request import urlopen

def get_url(src, dest):
    
    # Check if dest exists -- if it does
    # then we can skip downloading the file,
    # otherwise we have to download it!
    if os.path.isfile(??):
        print(f"{dest} found!")
    else:
        print(f"{dest} *not* found!")
        
get_url(url, out)
```

#### Answer

```{python}
from urllib.request import urlopen

def get_url(src, dest):

    # Check if dest exists -- if it does
    # then we can skip downloading the file,
    # otherwise we have to download it!
    if os.path.isfile(dest):
        print(f"{dest} found!")
    else:
        print(f"{dest} *not* found!")

get_url(url, out)
```

::::

### Fleshing Out the Function 

::: {.callout-warning collapse="true"}

#### Difficulty level: Medium

If you really explore what's going on in the function rather than just running it and moving on.

:::

I would then flesh out the code so that it downloads the file if it isn't found and then, either way, returns the *local* file path for our CSV reader to extract:

```{python}
def get_url(src, dest):
    
    # Check if dest does *not* exist -- that
    # would mean we had to download it!
    if os.path.isfile(dest):
        print(f"{dest} found locally!")
    else:
        print(f"{dest} not found, downloading!")
        
        #Â Get the data using the urlopen function
        response = urlopen(src) 
        filedata = response.read().decode('utf-8')
        
        # Extract the part of the dest(ination) that is *not*
        # the actual filename--have a look at how 
        # os.path.split works using `help(os.path.split)`
        path = list(os.path.split(dest)[:-1])
        
        # Create any missing directories in dest(ination) path
        # -- os.path.join is the reverse of split (as you saw above)
        # but it doesn't work with lists... so I had to google how 
        # to use the 'splat' operator! os.makedirs creates missing 
        # directories in a path automatically.
        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)
        
        with open(dest, 'w') as f:
            f.write(filedata)
            
        print(f"Data written to {dest}!")
    
    return dest
        
# Using the `return contents` line we make it easy to 
# see what our function is up to.
src = get_url(url, out)
```

::: {.callout-warning}

#### Stop! 

It really would be a good idea to put in the effort to make sense of how this function works. There is a lot going on here and understanding how this function works will help you to understand how to code. You should notice that we don't try to check if the data file contains any useful data! So if you download or create an empty file while testing, you won't necessarily get an error until you try to turn it into data afterwards!

:::

## Parse the CSV File

::: {.callout-tip collapse="true"}

#### Difficulty: Low

:::

Now we turn to the next task: parsing the file if it's a CSV. This implies that it *might* not be so that's something we should also consider!

:::: {.panel-tabset}

#### Question

```python
import csv

def read_csv(src):
    
    csvdata = []
    with open(src, 'r') as f:
        csvr = csv.??(f)
        
        for r in csvr:
            csvdata.append(??)
    
    # Return list of lists
    return ??

read_csv(src)
#read_csv('foo.bar') # <- Notice what happens if you try to run this code
#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!
```

You should get:

```
[['City', 'Population', 'Latitude', 'Longitude'],
 ['Perth', '45770', '56.39583', '-3.43333'],
 ['Armagh', '14777', '54.3499', '-6.6546'],
 ['Dundee', '147268', '56.462', '-2.9707'],
 ['Colchester', '194706', '51.88861', '0.90361'],
 ['Salisbury', '40302', '51.07', '-1.79'],
 ['Portsmouth', '205056', '50.80583', '-1.08722'],
 ['Wakefield', '325837', '53.683', '-1.499'],
 ['Bradford', '522452', '53.792', '-1.754'],
 ['Lancaster', '138375', '54.047', '-2.801'],
 ['Bangor', '18808', '53.228', '-4.128']]
```

#### Answer

```{python}
#| echo: true
import csv

def read_csv(src):

    csvdata = []
    with open(src, 'r') as f:
        csvr = csv.reader(f)

        for r in csvr:
            csvdata.append(r)

    # Return list of lists
    return csvdata

read_csv(src)
#read_csv('foo.bar') # <- Notice what happens if you try to run this code
#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!
```

::::

## Convert the CSV into a DoL

::: {.callout-warning collapse="true"}

#### Difficulty: Medium.

:::

Now we can focus on converting the CSV data to a dictionary-of-lists! We're going to start with the *same* function name but expand what the function *does*. This kind of *iteration* is common in software development.

:::: {.panel-tabset}

#### Question

```python
def read_csv(src):
    
    csvdata = {} # An empty dictionary-of-lists
    
    with open(??, 'r') as f:
        csvr = csv.reader(f)
        
        #Â Read in our column names and
        # initialise the dictionary-of-lists
        csvcols = next(csvr) 
        for c in csvcols:
            csvdata[c] = []
        
        # Notice this code is still the same, 
        # we just used next(csvr) to get the 
        # header row first!
        for r in ??: 
            # Although you can often assume that the order 
            # of the keys is the same, Python doesn't 
            #Â guarantee it; this way we will always make
            # the correct assignment.
            for idx, c in enumerate(csvcols):
                csvdata[??].append(r[idx])
    
    # Return dictionary of lists
    return csvdata

read_csv(src)
```

You should get something that starts:
```
{'City': ['Perth',
  'Armagh',
  'Dundee',
  'Colchester',
  'Salisbury',
  'Portsmouth',
  'Wakefield',
  'Bradford',
  'Lancaster',
  'Bangor'],
 'Population': ['45770',
```

#### Answer

```{python}
#| echo: true
import csv

def read_csv(src):

    csvdata = {} # An empty dictionary-of-lists

    with open(src, 'r') as f:
        csvr = csv.reader(f)

        #Â Read in our column names and
        # initialise the dictionary-of-lists
        csvcols = next(csvr)
        for c in csvcols:
            csvdata[c] = []

        # Notice this code is still the same,
        # we just used next(csvr) to get the
        # header row first!
        for r in csvr:
            # Although you can often assume that the order
            # of the keys is the same, Python doesn't
            #Â guarantee it; this way we will always make
            # the correct assignment.
            for idx, c in enumerate(csvcols):
                csvdata[c].append(r[idx])

    # Return dictionary of lists
    return csvdata

read_csv(src)
```

::::

## Adding Docstring

::: {.callout-tip collapse="true"}

#### Difficulty: Low

:::

We've assumed that the first row of our data set is always a _header_ (i.e. list of column names). If it's not then this code is going to have problems. A _robust_ function would allow us to specify column names, skip rows, etc. when we create the data structure, but let's not get caught up in that level of detail. Notice that I've also, for the first time:

1. Used the docstring support offered by Python. You'll be able to use `help(...)` and get back the docstring help!
2. Provided hints to Python about the expected input and output data types. This can help to ensure consistency and is also critical in testing / continuous integration when working with others on a codebase.

```{python}
#| echo: true
def read_csv(src:str) -> dict:
    """
    Converts a CSV file to a dictionary-of-lists (dol),
    using the first row to create column names.
    
    :param src: a local CSV file
    :returns: a dictionary-of-lists
    """
    csvdata = {} # An empty dictionary-of-lists
    
    with open(src, 'r') as f:
        csvr = csv.reader(f)
        
        #Â Read in our column names and
        # initialise the dictionary-of-lists
        csvcols = next(csvr) 
        for c in csvcols:
            csvdata[c] = []
        
        # Notice this code is still the same, 
        # we just used next(csvr) to get the 
        # header row first!
        for r in csvr: 
            # Although you can often assume that the order 
            # of the keys is the same, Python doesn't 
            #Â guarantee it; this way we will always make
            # the correct assignment.
            for idx, c in enumerate(csvcols):
                csvdata[c].append(r[idx])
    
    # Return dictionary of lists
    return csvdata

ds = read_csv(src)
```

```{python}
#| echo: true
help(read_csv)
```

```{python}
#| echo: true
print("Columns are: " + ", ".join(ds.keys()))
print(f"First two cities are: {ds['City'][:2]}")
print(f"First two populations are: {ds['Population'][:2]}")
print(f"First two latitudes are: {ds['Latitude'][:2]}")
```

# 'Functionalising' (Part 2)

Now that we've got a function that *basically* works, we're going to spend a little time thinking ahead about the kinds errors that we might expect our function to encounter 'in the wild' and how we can make its output more immediately useful for an analyst.

## Fixing Data Types

::: {.callout-warning collapse="true"}

#### Difficulty: Medium.

:::

If you look closely at the above, you'll see that *everything* is a string, including the latitudes, longitudes, and populations, which are clearly numeric data types. Here's a 'simple' way to specify a `dtype` list to hold the _data type_ for each column. I'm also going to introduce you the `zip` function here as it has many uses with geographic data (especially converting lat/long to points).

### Demonstrating Zip

::: {.callout-tip collapse="true"}

#### Difficulty: Low

:::

:::: {.panel-tabset}

#### Question

```python
cols  = ['City', 'Population', 'Latitude', 'Longitude'] # <- Column name
dtype = [??, ??, ??, float]                        # <- Column data type

# 'Zips up' these two lists into an iterator! So this will 
# take element 0 from *each* list and pass them to `col` as
# a list-like 'tuple' (meaning there is a col[0] and a col[1]).
for col in zip(cols, dtype):
    colname = col[0]
    coltype = col[1]
    
    # Notice the more advanced formatting here:
    # - `>12` means right-align with up to 12 characters of whitespace; notice the last line!
    # - `coltype.__name__` gives us the name of the data type, rather than a '<class...>' output.
    print(f"Column {colname:>12} should be type: {coltype.__name__}")
```

#### Answer

```{python}
#| echo: true
cols  = ['City', 'Population', 'Latitude', 'Longitude'] # <- Column name
dtype = [str, int, float, float]                        # <- Column data type

# 'Zips up' these two lists into an iterator! So this will
# take element 0 from *each* list and pass them to `col` as
# a list-like 'tuple' (meaning there is a col[0] and a col[1]).
for col in zip(cols, dtype):
    colname = col[0]
    coltype = col[1]

    # Notice the more advanced formatting here:
    # - `>12` means right-align with up to 12 characters of whitespace; notice the last line!
    # - `coltype.__name__` gives us the name of the data type, rather than a '<class...>' output.
    print(f"Column {colname:>12} should be type: {coltype.__name__}")
```

::::

### A Function to Convert Data Types

::: {.callout-tip collapse="true"}

#### Difficulty: Low, as I've provided a function.

:::

```{python}
#| echo: true
# Convert the raw data to data of the appropriate
# type: 'column data' (cdata) -> 'column type' (ctype)
def to_type(cdata, ctype):
    # If a string
    if isinstance(cdata, str):
        try:
            if ctype==bool:
                return cdata==True
            else:
                return ctype(cdata)
        except TypeError:
            return cdata
    
    # Not a string (assume list)
    else: 
        fdata = []
        for c in cdata:
            try:
                if ctype==bool:
                    fdata.append( c=='True' )
                else:
                    fdata.append( ctype(c) )
            except:
                fdata.append( c )
        return fdata
```

For example: 

```{python}
#| echo: true
to_type(['31.4','14','2412312.23'],int)
```

Now apply this! We'll copy the data to new data structure only so that we know we're not overwriting `ds` until we're sure that the code works.

:::: {.panel-tabset}

#### Question

```python 
ds2 = {}
for col in zip(cols, dtype):
    colname = col[0]
    coltype = col[1]
    ds2[ ?? ] = to_type( ds[??], coltype )
```

#### Answer

```{python}
# Now apply this! We'll copy the data to
# new data structure only so that we know
# we're not overwriting `ds` until we're sure
# that the code works.
ds2 = {}
for col in zip(cols, dtype):
    colname = col[0]
    coltype = col[1]
    print(f"ds2[ {colname} ] = to_type( ds[{colname}], {coltype})")
    ds2[ colname ] = to_type( ds[colname], coltype )
```

::::

Compare the output here to the output from these two cells *carefully* (what has changed?):

```{python}
#| echo: true
print("DS columns are: " + ", ".join(ds.keys()))
print(f"First two cities are: {ds['City'][:2]}")
print(f"First two populations are: {ds['Population'][:2]}")
print(f"First two latitudes are: {ds['Latitude'][:2]}")
```

```{python}
#| echo: true
print("DS2 columns are: " + ", ".join(ds2.keys()))
print(f"First two cities are: {ds2['City'][:2]}")
print(f"First two populations are: {ds2['Population'][:2]}")
print(f"First two latitudes are: {ds2['Latitude'][:2]}")
```

## Checking Basic Functionality

::: {.callout-warning collapse="true"}

#### Difficulty: Medium

:::

Now that we've got our data structure all set up correctly (appropriate names, data types, etc.) let's see if it works by testing out some of our previous operations:

:::: {.panel-tabset}

#### Question

```python
import numpy as np # We'll need this to apply functions to lists easily

print(f"Average population is {np.mean(ds2['Population']):,.2f}")
print(f"Westernmost city is {ds2['City'][np.where(ds2['Longitude']==np.??(ds2['??']))[0][0]]}")
print(f"Northernmost city is {ds2['City'][np.where(ds2['Latitude']==np.??(ds2['??']))[0][0]]}")
print(f"Southernmost city is {ds2['City'][np.where(ds2['Latitude']==np.??(ds2['??']))[0][0]]}")
```

You should get the following:

```
Average population is 165,335.10
Westernmost city is Armagh
Northernmost city is Dundee
Southernmost city is Portsmouth
```

#### Answer

```{python}
import numpy as np # We'll need this to apply functions to lists easily

print(f"Average population is {np.mean(ds2['Population']):,.2f}")
print(f"Westernmost city is {ds2['City'][np.where(ds2['Longitude']==np.min(ds2['Longitude']))[0][0]]}")
print(f"Northernmost city is {ds2['City'][np.where(ds2['Latitude']==np.max(ds2['Latitude']))[0][0]]}")
print(f"Southernmost city is {ds2['City'][np.where(ds2['Latitude']==np.min(ds2['Latitude']))[0][0]]}")
```

::::

There are a few things to understand here:

1. Where we want to find something else in the data *based on that value* then things get a little more complex: we use `np.min` to find the smallest value in the data, but we don't know *where* in the list that value actually *was* so we use `np.where` to find out which indexes match the minimum value. In this data set it's easy because there's one, and only one value that matches. But you'd need to do some clever thinking about how to handle ties.
2. `np.where` returns a complex data structure (list-of-lists, essentially) so we need to pull the value we need out of that data in order to actually find the list index we need and look up the `City` name associated with, for example, the minium value in the data. That bit (`[0][0]`) is a bit clunky, but right now we're not too worried about it.

**Notice** that we do *all* of this without using a `for` loop or variables to keep track of what we've found... You *could* also use a `for` loop to answer each of these questions (see the example below), but hopefully you see how the above solution is more *elegant* (it's also faster):

```python
min_long = 180
min_idx  = -1

for l in ds2['Longitude']:
    if l < min_long: min_long = l
print(f"Minimum longitude is {min_long}")

for idx, l2 in enumerate(ds2['Longitude']):
    if l2==min_long:
        min_idx = idx
        break

print(f"Westernmost city is {ds2['City'][min_idx]}")
```

## Was it Worth It?

::: {.callout-tip collapse="true"}

#### Difficulty: Low, it should just _work_.

:::

At this point it's worth asking: was all this *worth* it? Let's see! 

The best way to test is to use a *different* data set and see if we've solved the 'hard-coding' problem.

```{python}
#| echo: true
url = "https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities.csv"
out = os.path.join('data','Wikipedia-Cities-full.csv')

cols  = ['City', 'Population', 'Latitude', 'Longitude']
dtype = [str, int, float, float]

untyped_dol = read_csv(get_url(url, out))

typed_dol = {}
for col in zip(cols, dtype):
    colname = col[0]
    coltype = col[1]
    typed_dol[ colname ] = to_type(untyped_dol[colname], coltype)
```

```{python}
#| echo: true
print(f"Average population is {np.mean(typed_dol['Population']):,.2f}")
print(f"Westernmost city is {typed_dol['City'][np.where(typed_dol['Longitude']==np.min(typed_dol['Longitude']))[0][0]]}")
print(f"Northernmost city is {typed_dol['City'][np.where(typed_dol['Latitude']==np.max(typed_dol['Latitude']))[0][0]]}")
print(f"Southernmost city is {typed_dol['City'][np.where(typed_dol['Latitude']==np.min(typed_dol['Latitude']))[0][0]]}")
```


So we used all the same code as for the subset of the data but changed *nothing*. And this is even though the column order changed (print out the first row of each file if you don't believe me) *and* the number of columns changed *and* our city column now contains commas! So what this has given us is a much more flexible way not only to *access* the data, but also to *work* with it!

## More Functions!

::: {.callout-warning collapse="true"}

#### Difficulty: Medium

:::

Here is the skeleton of a function to replace the `np.where(column==np.[min|max|...](column))[0][0]` code. There are a few ways to do this: 

1. You could use if/else/elif and do different things based on testing against the specified string
2. You could try to find a function in `numpy` that matches the specified string
3. You could try to `eval` the code, but I really wouldn't recommend this for security reasons

I have gone with a combination of 1 and 2, but you will need to really read the code to understand how it works. I've left the docstring for you to complete. This isn't the best function since it makes some assumptions about the types of data that it might be passed to the function: this is where *Object-Oriented Programming* could come to the rescue! If we had different column *types* (e.g. String, Float, Int) then we could have different *versions* of `find_val` that performed the same *function* (find a value) but did this completely differently depending on the data. This is what *methods* do!

:::: {.panel-tabset}

#### Question

```python
import numpy as np

def find_val(col:list, val:str):
    """
    ??
    """
    if val in dir(np) and callable(getattr(np, val)):
        func = getattr(np, val) # <--- What does this do???
        if val in ['min','max']:
            return np.??(col==func(col))[0][0]
        else:
            return func(col)
    else:
        return np.nan
    
print(find_val(typed_dol['Latitude'], 'min'))
print(find_val(typed_dol['Latitude'], 'median'))
```

#### Answer

```{python}
import numpy as np

def find_val(col:list, val:str):
    """
    Finds a value in a (numeric) list using a numpy function. The search
    function must exist in numpy (e.g. np.min, np.max, np.median) since
    that is the package that we search.

    :param col: a list of numeric values
    :param val: what kind of value to search for (in the sense of min, max, median, etc.)
    :returns: the result of the search (e.g. the minimum value in the list as located by the numpy function)
    """
    if val in dir(np) and callable(getattr(np, val)):
        func = getattr(np, val)
        if val in ['min','max']:
            return np.where(col==func(col))[0][0]
        else:
            return func(col)
    else:
        return np.nan

print(find_val(typed_dol['Latitude'], 'min'))
print(find_val(typed_dol['Latitude'], 'median'))
```

::::

```{python}
#| echo: true
print(f"Average population is {find_val(typed_dol['Population'],'mean'):,.2f}")
print(f"Westernmost city is {typed_dol['City'][find_val(typed_dol['Longitude'],'min')]}")
print(f"Northernmost city is {typed_dol['City'][find_val(typed_dol['Latitude'],'max')]}")
print(f"Southernmost city is {typed_dol['City'][find_val(typed_dol['Latitude'],'min')]}")
```

So we have streaminlined the code still further and implemented a fairly generic 'helper' function that uses `numpy` to perform calculations on a column of data (assuming it's numeric). We could, of course, extend this further, to handle strings and other data types, but we're going to see a *better* way to do all of this *next* week.

# Creating a Package from Functions

::: {.callout-note}

**&#128279; Connections**: This is building on both [Functions](https://jreades.github.io/code-camp/lessons/Functions.html) and [Packages](https://jreades.github.io/code-camp/lessons/Packages.html) sessions from Code Camp as well as the [Functions](https://jreades.github.io/fsds/sessions/week3.html#lectures) and [Packages](https://jreades.github.io/fsds/sessions/week3.html#lectures) lectures. 

:::

Below is code to create a package called `dtools` (i.e. data tools) by converting the notebook into a Python script file called `__init__.py` that sits in the `dtools` directory. This is the first step to creating a package from code that is *already* working in a Notebook.

## Create a Directory

::: {.callout-tip collapse="true"}

#### Difficulty: Low. 

:::

When creating your own package, everything goes into a directory whose name is the name of the package. In other words, if you wanted to create a package called `my_stuff` then you'd need to *first* create a directory called `my_staff` in the *current working directory* (i.e. wherever you are keeping your Notbooks).

We can create the directory using `mkdir` (the `!` means 'run this [Linux] command'):

```{python}
#| echo: true
!mkdir -p 'dtools'
```

## Create the init File

::: {.callout-warning collapse="true"}

#### Difficulty: Medium

:::

More complex packages will have lots of stuff going on inside their 'root' directory, but we're keeping it simple and only need to create and fill in *one* file inside the `dtools` directory: `__init__.py`. This is a convention.

You can create an empty file with that name in `dtools` using a command line function like `!touch dtools/__init__.py` or if you prefer then there's `File` > `New` > `Python File` but you'll need to **make sure it has created the file in the right place** (that's *within* the `dtools` directory).

Either way, the next step is then to copy all of the functions you've just created into the `__init__.py` file.


#### Aside: nbconvert 

Sometimes, notebooks become so complex or take such a long time to run that they're *better* expressed as Python scripts (`.py` files). We don't *have* to step through the notebook copying and pasting the bits we need since, using `nbconvert` you can convert an entire notebook in one go like this:

```bash
!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \
    --to python --output=dtools/notebook.py \
    Practical-04-Objects-Answers.ipynb
```

This is just so that you know the option exists.

## Extract Code to Init

::: {.callout-warning collapse="true"}

#### Difficulty: Medium

:::

You now need to find and copy+paste the following functions into `__init__.py`:

1. `get_url`
2. `read_csv` 
3. `to_type` 
4. `find_val`

And don't forget to find all the `import` statements (including `from x import y`) at the start of this notebook and copy those as well!

You should then be able to run the code below and can always compare it to my version [on GitHub](https://github.com/jreades/fsds/blob/master/practicals/dtools/__init__.py).

## Test It

::: {.callout-caution collapse="true"}

#### Difficulty: Hard.

:::

The next two lines of code allow you to repeatedly edit an imported package without having to restart the entire Python notebook. So whenever Jupyter sees a change to `dtools/__init__.py` it will reload the package and update the code without you having to do anything.

```python
%load_ext autoreload
%autoreload 2
```

### Import

And now we should be able to import the package and start using the functions that we defined...

```python
import dtools
help(dtools.read_csv)
```

### Use Read_CSV

```python
url = 'https://github.com/jreades/fsds/raw/master/data/2019-sample-crime.csv'
out = os.path.join('data','crime-sample.csv')

ds = ??.read_csv(??.get_url(url, out))
```

```python
print(f"ds has {len(ds.keys())} columns, these are: " + ", ".join(ds.keys()))
print(f"There are {len(ds['ID'])} rows of data")
```

### Use To_Type

```python
cols  = ['Latitude', 'Longitude'] 
dtype = [float, float]

typed_ds = {}
for col in zip(cols, dtype): #Â <- This only copies these two columns to typed_ds
    colname = col[0]
    coltype = col[1]
    typed_ds[ colname ] = dtools.to_type(ds[colname], coltype)
```

### Use Find_Val

```python
idx = find_val(typed_ds['Latitude'], 'min')

print(ds['Case Number'][idx])
print(ds['Description'][idx])
```

# Brain Teaser

::: {.callout-note}

**&#128279; Connections**: This will draw on what you've learned in the lectures about <a href="">Methods</a>, <a href="">Classes</a>, and <a href="">Design</a>. You will also find the Code Camp [Classes](https://jreades.github.io/code-camp/lessons/Classes.html) session useful.

:::

::: {.callout-caution collapse="true"}

#### Difficulty: &#129327;.

:::

And now for something completely different!

We want to create a set of 'ideal shapes' with methods allowing us to derive various properties of that shape:

- Diameter: which we'll define as the longest line that can be drawn across the inside of the shape.
- Volume: the total volume of the shape.
- Surface Area: the total outside area of the shape.

We will create all of these shape classes in the notebook so that we know they work and then will move them to an external package file so that they can be imported and re-used easily in other notebooks.

We're also going to make use of a few features of Python:

- You can access the class name of an instance using: `self.__class__.__name__`. And here's one key point: `self` refers to the instance, not to the class... we'll see why this matters.
- You can raise your own exceptions easily if you don't want to implement a particular method yet.
- You can have an 'abstract' base class that does nothing except provide a template for the 'real' classes so that they can be used interchangeably.

## Abstract Base Class

This class appears to do very little, but there are two things to notice:

1. It provides a constructor (`__init__`) that sets the `shape_type` to the name of the class automatically (so a `square` object has `shape_type='Square'`) and it stores the critical dimension of the shape in `self.dim`.
2. It provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from `shape`.

```{python}
#| echo: true
from math import pi

# Base class shape
class shape(object): # Inherit from base class 
    def __init__(self, dimension:float=None):
        self.shape_type = self.__class__.__name__.capitalize()
        self.dim = dimension
        return
    
    def diameter(self):
        raise Exception("Unimplmented method error.")
    
    def volume(self):
        raise Exception("Unimplmented method error.")
    
    def surface(self):
        raise Exception("Unimplmented method error.")
        
    def type(self):
        return(self.shape_type)
```

## Cube

Implements a cube:

1. The diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: $\sqrt{d^{2} + d^{2} + d^{2}}$ which we can reduce to $\sqrt{3 d^{2}}$.
2. A cube's volume is given by $d^{3}$.
3. A cube's surface area will be the sum of its six faces: $6d^{2}$.

```python
# Cube class
class cube(shape): # Inherit from shape 
    def __init__(self, dim:float):
        super().__init__(dim)
        return
    
    def diameter(self):
        return (3 * self.??**2)**(1/2)
    
    def volume(self):
        return self.dim**3
    
    def surface(self):
        return ??*(self.dim**2)
```

## Sphere

Implements a sphere:

1. The diameter is twice the critical dimension (radius): $2r$. 
2. The volume is $\frac{4}{3} \pi r^{3}$.
3. The surface area will be $4 \pi r^{2}$.

If we were writing something more general, we'd probably have spheres as a special case of an ellipsoid!

```python
# Sphere code here!
```

## Regular Pyramid

We're taking this to be a regular pyramid where all sides are equal: 

1. The diameter is a line drawn across the base between opposing corners of the base so it's just $\sqrt{d^{2} + d^{2}}$.
2. The volume is given by $VÂ =Â bÂ *Â hÂ /Â 3$ (where $b$ is the area of the base, which in this case becomes $d^{2} * h/3$).
3. The surface area will be the base + 4 equilateral triangles: $d^{2} + 4 (d^{2}\sqrt{3}/4)$ which we can reduce to $d^{2} + d^{2}\sqrt{3}$

But this requires a _height_ method that is specific to pyramids:

4. The height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): $l = \sqrt{d{^2} + d^{2}}$ and the long side ($d$ again) which gives us $\sqrt{l/2 + d^{2}}$.

Note that this has a class variable called `has_mummies` since Egyptian regular pyramids are plagued by them! 

```python
# Pyramid class
class pyramid(shape): # Inherit from shape
    
    has_mummies = True # This is for *all* regular pyramids
    
    def __init__(self, dim:float):
        super().__init__(dim)
        self.shape_type = 'Regular Pyramid'
        return
    
    def diameter(self):
        return (self.dim**?? + self.??**2)**(1/2)
    
    def height(self):
        return (self.diameter()/?? + self.dim**2)**(1/2)
    
    def volume(self):
        return self.dim**2 * self.??() / 3
    
    def surface(self):
        return self.dim**2 + self.dim**2 * 3**(1/2)
```

## Triangular Pyramid

We want triangular pyramid to inherit from regular pyramid, and all sides are equal so it's an _equilateral_ triangular pyramid. However, this is kind of a judgement call since there's very little shared between the two types of pyramid and it's arguable whether this one is actually simpler and should therefore be the parent class...

Anyway, the calculations are:

1. The diameter (longest line through the shape) will just be the edge: $d$.
2. The volume $V = b * h / 3$ where $b$ is the area of an equilateral triangle.
3. The surface area will be $4b$ where $b$ is the area of an equilateral triangle.

So we now need two new formulas:

5. The height of the pyramid using ([Pythagoras again](https://www.youtube.com/watch?v=ivF3ndmkMsE)): $h = \sqrt{6}d/3$.
6. The area of an equilateral triangle: $\frac{\sqrt{3}}{4} d^{2}$

Triangular pyramids do *not* have a problem with mummies.

```python
# Triangular pyramid code here (extends regular pyramid)!
```

## Testing Your Classes

If you've implemented everything correctly then the following code should run.

```python
# How would you test these changes?
s = sphere(10)
print(s.type())
print(f"\tVolume is: {s.volume():5.2f}")
print(f"\tDiameter is: {s.diameter():5.2f}")
print(f"\tSurface Area is: {s.surface():5.2f}")
print("")

c = cube(10)
print(c.type())
print(f"\tVolume is: {c.volume():5.2f}")
print(f"\tDiameter is: {c.diameter():5.2f}")
print(f"\tSurface Area is: {c.surface():5.2f}")
print("")

p = pyramid(10)
print(p.type())
print(f"\tVolume is: {p.volume():5.2f}")
print(f"\tDiameter is: {p.diameter():5.2f}")
print(f"\tSurface Area is: {p.surface():5.2f}")
print(f"\tHeight is: {p.height():5.2f}")
if p.has_mummies is True:
    print("\tMummies? Aaaaaaaaagh!")
else:
    print("\tPhew, no mummies!")
print("")

p2 = t_pyramid(10)
print(p2.type())
print(f"\tVolume is: {p2.volume():5.2f}")
print(f"\tDiameter is: {p2.diameter():5.2f}")
print(f"\tSurface Area is: {p2.surface():5.2f}")
print(f"\tHeight is: {p2.height():5.2f}")
if p2.has_mummies is True:
    print("\tMummies?Â Aaaaaaaaagh!")
else:
    print("\tPhew,Â noÂ mummies!")
print("")

#Â UsefulÂ demonstrationÂ ofÂ howÂ toÂ findÂ outÂ ifÂ aÂ methodÂ orÂ attributeÂ is
#Â associatedÂ withÂ aÂ particularÂ object
if hasattr(p2,'base_area'):
    print(f"ShapeÂ ofÂ type '{p2.type()}'Â hasÂ attributeÂ orÂ methodÂ 'base_area'")
else:
    print(f"ShapeÂ ofÂ typeÂ '{p2.type()}'Â doesÂ *not*Â haveÂ attributeÂ orÂ methodÂ 'base_area'")
print("")
```

I get the following output:

```
Sphere
	Volume is: 4188.79
	Diameter is: 20.00
	Surface Area is: 1256.64

Cube
	Volume is: 1000.00
	Diameter is: 17.32
	Surface Area is: 600.00

Regular Pyramid
	Volume is: 344.92
	Diameter is: 14.14
	Surface Area is: 273.21
	Height is: 10.35
	Mummies? Aaaaaaaaagh!

Triangular Pyramid
	Volume is: 117.85
	Diameter is: 10.00
	Surface Area is: 173.21
	Height is:  8.16
	Phew, no mummies!

Shape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'
```

## Packaging It Up

Wait, you're *still* working on this practical and haven't thrown up your hands in disgust yet? OK, in that case you can have *one* more thing to do: turn the whole shapes class hierarchy into a package that can be loaded via an `import` statement. 

### Cell Magic

This code allows Jupyter to reload external libraries if they are edited after you import them. When you are working on your own packages this is rather useful since you tend to make a *lot* of mistakes when packaging code up this way and it's handy not to have to restart the entire notebook every time you fix a typo or change a function.

```python
%load_ext autoreload
%autoreload 2
```

### Import Shapes

You can call your package whatever you like, but then it *has* to match what you `import` below. So remember that with `dtools` we had `dtools/__init__.py`? You'll need to do the same here so that you can import the package.


### Adding Documentation

In an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the `__init__.py` package file.

Underneath the line `class shape(object):`, add:
```
    """Abstract base class for all ideal shape classes.

    Keyword arguments:
    dimension -- the principle dimension of the shape (default None)
    """
```

Underneath the line `def type(self):`, add:
```
        """
        Returns the formatted name of the shape type. 
        
        This is set automatically, but can be overwritten by setting the attribute shape_type.
        
        :returns: the name of the class, so shapes.cube is a `Cube` shape type
        :rtype: str
        """
```

```python
import shapes # <-- Change this if you didn't call your package `shapes`!
help(shapes.shape)
help(shapes.shape.type)
```
