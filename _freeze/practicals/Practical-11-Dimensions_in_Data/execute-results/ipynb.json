{
  "hash": "5ff68ca10dcfa1a8591fc59cda0927d3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Practical 8: Dimensions in Data\"\nsubtitle: \"Transformation & Dimensionality Reduction\"\njupyter: python3\nfilters:\n  - qna\n  - quarto\n---\n\n\n\n\n\n\n\n\n| Complete | Part 1: Foundations | Part 2: Data | Part 3: Analysis |     |\n| :------- | :------------------ | :----------- | :--------------- | --: |\n| 70% | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9617;&#9617;&#9617;&#9617;&#9617; | 8/10\n\n<style type=\"text/css\">\n.longform {\n    max-height: 300px;\n    overflow-y: scroll;\n}\n</style>\n\nIn this session the focus is on MSOA-level Census data from 2011. We're going to explore this as a *possible* complement to the InsideAirbnb data. Although it's not ideal to use 2011 data with scraped from Airbnb this year, we: \n\n1. Have little choice as the 2021 data is only just starting to come through from the Census and the London Data Store hasn't been updated (still!); and \n2. Could usefully do a bit of thinking about whether the situation in 2011 might in some way help us to 'predict' the situation now...\n\nUltimately, however, you don't *need* to use this for your analysis, this practical is intended as a demonstration of how transformation and dimensionality reduction work in practice and the kinds of issues that come up.\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nThere are a _lot_ of links across sessions now, as well as some _forward links_ to stuff we've not yet covered (see: `pandas.merge`). We'll pick these up as we move through the notebook.\n\n:::\n\n# Preamble\n\nLet's start with the usual bits of code to ensure plotting works, to import packages and load the data into memory.\n\n::: {#6e8c85da .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\n\nimport umap\nfrom kneed import knee_locator\n```\n:::\n\n\nNotice here that we've moved the function from last week's notebook to a separate file `cache.py` from which we import the `cache_data` function. This should give you some ideas about how to work from script -> function -> library effectively.\n\n::: {#c84ae3a6 .cell execution_count=2}\n``` {.python .cell-code}\nfrom cache import cache_data\n```\n:::\n\n\n# Loading MSOA Census Data\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nBy this point you should be fairly familiar with the UK's census geographies as you'll have encountered them in your GIS module. But in case you need a refresher, here's what the [Office for National Statistics](https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeographies/census2021geographies) says.\n\n:::\n\n::: {.callout-tip}\n\nWe're going to mix in the London's MSOA 'Atlas' from the [London Data Store](https://data.london.gov.uk/dataset). I would _strongly_ suggest that you have a look around the London Data Store as you develop your thinking for the group assessment -- you will likely find useful additional data there!\n\n:::\n\nOnce you see how we deal with this MSOA Atlas data you will be in a position to work with any other similarly complex (in terms of the headings and indexes) data set. If you're feeling particularly ambitious you can actually do this _same_ work at the LSOA scale using the [LSOA Atlas](https://data.london.gov.uk/dataset/lsoa-atlas) and LSOA boundaries... the process should be the same, though you will have smaller samples in each LSOA than you do in the MSOAs and calculations will take a bit longer to complete. You could also search on the ONS Census web site for data from 2021.\n\nThere is a CSV file for the MSOA Atlas that would be easier to work with; however, the Excel file is useful for demonstrating how to work with multi-level indexes (an extension of last week's work). Notice that below we do two new things when reading the XLS file:\n\n1. We have to specify a sheet name because the file contains multiple sheets.\n2. We have to specify not just _one_ header, we actually have to specify three of them which generates a multi-level index (row 0 is the top-level, row 1 is the second-level, etc.).\n\n## Load MSOA Excel File\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low.\n\n:::\n\nYou might like to load the cached copy of the file into Excel so that you can see how the next bit works. You can find the rest of the MSOA Atlas [here](https://data.london.gov.uk/dataset/msoa-atlas).\n\n::: {#acfe5c42 .cell execution_count=3}\n``` {.python .cell-code}\nsrc_url   = 'https://data.london.gov.uk/download/msoa-atlas/39fdd8eb-e977-4d32-85a4-f65b92f29dcb/msoa-data.xls'\ndest_path = os.path.join('data','msoa')\n```\n:::\n\n\n:::: {.panel-tabset}\n\nYou will want to manually download the data to have a look *in* Excel while you sort out the answer below. We're mainly doing this so that you use and make sense of documentation for handling more complex data structures in Excel.\n\n#### Question\n\n```python\nexcel_atlas = pd.read_excel(\n    cache_data(src_url, dest_path), \n    ???, # Which sheet is the data in?\n    header=[0,1,2])            # Where are the column names... there's three of them!\n```\n\n#### Answer\n\n::: {#7f497aa0 .cell execution_count=4}\n``` {.python .cell-code}\nexcel_atlas = pd.read_excel(\n    cache_data(src_url, dest_path),\n    sheet_name='iadatasheet1', # Which sheet is the data in?\n    header=[0,1,2])            # Where are the column names... there's three of them!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/msoa/msoa-data.xls locally!\n\tSize is 2 MB (1,979,904 bytes)\n```\n:::\n:::\n\n\n::::\n\nNotice the format of the output and notice that all of the empty cells in the Excel sheet have come through as `Unnamed: <col_no>_level_<level_no>`:\n\n::: {#fc7ca100 .cell execution_count=5}\n``` {.python .cell-code}\nexcel_atlas.head(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>Unnamed: 0_level_0</th>\n      <th>Unnamed: 1_level_0</th>\n      <th colspan=\"7\" halign=\"left\">Age Structure (2011 Census)</th>\n      <th>Mid-year Estimate totals</th>\n      <th>...</th>\n      <th colspan=\"10\" halign=\"left\">Road Casualties</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Unnamed: 0_level_1</th>\n      <th>Unnamed: 1_level_1</th>\n      <th>All Ages</th>\n      <th>0-15</th>\n      <th>16-29</th>\n      <th>30-44</th>\n      <th>45-64</th>\n      <th>65+</th>\n      <th>Working-age</th>\n      <th>All Ages</th>\n      <th>...</th>\n      <th colspan=\"2\" halign=\"left\">2010</th>\n      <th colspan=\"4\" halign=\"left\">2011</th>\n      <th colspan=\"4\" halign=\"left\">2012</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>MSOA Code</th>\n      <th>MSOA Name</th>\n      <th>Unnamed: 2_level_2</th>\n      <th>Unnamed: 3_level_2</th>\n      <th>Unnamed: 4_level_2</th>\n      <th>Unnamed: 5_level_2</th>\n      <th>Unnamed: 6_level_2</th>\n      <th>Unnamed: 7_level_2</th>\n      <th>Unnamed: 8_level_2</th>\n      <th>2002</th>\n      <th>...</th>\n      <th>Slight</th>\n      <th>2010 Total</th>\n      <th>Fatal</th>\n      <th>Serious</th>\n      <th>Slight</th>\n      <th>2011 Total</th>\n      <th>Fatal</th>\n      <th>Serious</th>\n      <th>Slight</th>\n      <th>2012 Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E02000001</td>\n      <td>City of London 001</td>\n      <td>7375.0</td>\n      <td>620.0</td>\n      <td>1665.0</td>\n      <td>2045.0</td>\n      <td>2010.0</td>\n      <td>1035.0</td>\n      <td>5720.0</td>\n      <td>7280.0</td>\n      <td>...</td>\n      <td>334.0</td>\n      <td>374.0</td>\n      <td>0.0</td>\n      <td>46.0</td>\n      <td>359.0</td>\n      <td>405.0</td>\n      <td>2.0</td>\n      <td>51.0</td>\n      <td>361.0</td>\n      <td>414.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows Ã— 207 columns</p>\n</div>\n```\n:::\n:::\n\n\n```python\nprint(f\"Shape of the MSOA Atlas data frame is: {excel_atlas.shape[0]:,} x {excel_atlas.shape[1]:,}\")\n```\n\nYou should get: <code>Shape of the MSOA Atlas data frame is: 984 x 207</code>, but how on earth are you going to access the data?\n\n## Accessing MultiIndexes\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\nThe difficulty is conceptual, not technical.\n\n:::\n\nUntil now we have understood the pandas index as a single column-like 'thing' in a data frame, but pandas also supports hierarchical and grouped indexes that allow us to interact with data in more complex ways... should we need it. Generally:\n\n- MultiIndex == hierarchical index on *columns*\n- DataFrameGroupBy == iterable pseudo-hierarchical index on *rows*\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nWe'll be looking at **Grouping Data** in much more detail in [next week](https://jreades.github.io/fsds/sessions/week9.html#lectures), so the main thing to remember is that grouping is for rows, multi-indexing is about columns.\n\n:::\n\n### Direct Access\n\nOf course, one way to get at the data is to use `.iloc[...]` since that refers to columns by *position* and ignores the complexity of the index. Try printing out the the first five rows of the first column using `iloc`:\n\n```python\nexcel_atlas.iloc[???]\n```\n\nYou should get:\n\n::: {#e8f81f07 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n0    E02000001\n1    E02000002\n2    E02000003\n3    E02000004\n4    E02000005\nName: (Unnamed: 0_level_0, Unnamed: 0_level_1, MSOA Code), dtype: object\n```\n:::\n:::\n\n\n### Named Access\n\nBut to do it by name is a little trickier:\n\n::: {#9ecc9543 .cell execution_count=7}\n``` {.python .cell-code}\nexcel_atlas.columns.tolist()[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n[('Unnamed: 0_level_0', 'Unnamed: 0_level_1', 'MSOA Code'),\n ('Unnamed: 1_level_0', 'Unnamed: 1_level_1', 'MSOA Name'),\n ('Age Structure (2011 Census)', 'All Ages', 'Unnamed: 2_level_2'),\n ('Age Structure (2011 Census)', '0-15', 'Unnamed: 3_level_2'),\n ('Age Structure (2011 Census)', '16-29', 'Unnamed: 4_level_2')]\n```\n:::\n:::\n\n\nNotice how asking for the first five columns has given us a list of... what exactly?\n\n:::: {.panel-tabset}\n\n#### Question\n\nSo to get the **same output** by column *name* what do you need to copy from above:\n\n```python\nexcel_atlas.loc[0:5, ???]\n```\n\n#### Answer\n\n::: {#b34391e4 .cell execution_count=8}\n``` {.python .cell-code}\nexcel_atlas.loc[0:5, ('Unnamed: 0_level_0','Unnamed: 0_level_1','MSOA Code')]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n0    E02000001\n1    E02000002\n2    E02000003\n3    E02000004\n4    E02000005\n5    E02000007\nName: (Unnamed: 0_level_0, Unnamed: 0_level_1, MSOA Code), dtype: object\n```\n:::\n:::\n\n\n::::\n\nThe answer is *really* awkward, so we're going to look for a better way...\n\n### Grouped Access\n\nDespite this, *one* way that MultiIndexes can be useful is for accessing column-slices from a 'wide' dataframe. We can, for instance, select all of the Age Structure columns in one go and it will be *simpler* than what we did above.\n\n::: {#8101e01b .cell execution_count=9}\n``` {.python .cell-code}\nexcel_atlas.loc[0:5, ('Age Structure (2011 Census)')]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>All Ages</th>\n      <th>0-15</th>\n      <th>16-29</th>\n      <th>30-44</th>\n      <th>45-64</th>\n      <th>65+</th>\n      <th>Working-age</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Unnamed: 2_level_2</th>\n      <th>Unnamed: 3_level_2</th>\n      <th>Unnamed: 4_level_2</th>\n      <th>Unnamed: 5_level_2</th>\n      <th>Unnamed: 6_level_2</th>\n      <th>Unnamed: 7_level_2</th>\n      <th>Unnamed: 8_level_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7375.0</td>\n      <td>620.0</td>\n      <td>1665.0</td>\n      <td>2045.0</td>\n      <td>2010.0</td>\n      <td>1035.0</td>\n      <td>5720.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6775.0</td>\n      <td>1751.0</td>\n      <td>1277.0</td>\n      <td>1388.0</td>\n      <td>1258.0</td>\n      <td>1101.0</td>\n      <td>3923.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10045.0</td>\n      <td>2247.0</td>\n      <td>1959.0</td>\n      <td>2300.0</td>\n      <td>2259.0</td>\n      <td>1280.0</td>\n      <td>6518.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6182.0</td>\n      <td>1196.0</td>\n      <td>1277.0</td>\n      <td>1154.0</td>\n      <td>1543.0</td>\n      <td>1012.0</td>\n      <td>3974.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8562.0</td>\n      <td>2200.0</td>\n      <td>1592.0</td>\n      <td>1995.0</td>\n      <td>1829.0</td>\n      <td>946.0</td>\n      <td>5416.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8791.0</td>\n      <td>2388.0</td>\n      <td>1765.0</td>\n      <td>1867.0</td>\n      <td>1736.0</td>\n      <td>1035.0</td>\n      <td>5368.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Understanding Levels\n\nThis works because the MultiIndex tracks the columns using *levels*, with level 0 at the 'top' and level 2 (in our case) at the bottom. These are the unique *values* for the top level ('row 0'):\n\n::: {#308474f1 .cell execution_count=10}\n``` {.python .cell-code}\nexcel_atlas.columns.levels[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nIndex(['Adults in Employment (2011 Census)', 'Age Structure (2011 Census)',\n       'Car or van availability (2011 Census)',\n       'Central Heating (2011 Census)', 'Country of Birth (2011)',\n       'Dwelling type (2011)', 'Economic Activity (2011 Census)',\n       'Ethnic Group (2011 Census)', 'Health (2011 Census)', 'House Prices',\n       'Household Composition (2011)', 'Household Income Estimates (2011/12)',\n       'Household Language (2011)', 'Households (2011)', 'Incidence of Cancer',\n       'Income Deprivation (2010)', 'Land Area', 'Life Expectancy',\n       'Lone Parents (2011 Census)', 'Low Birth Weight Births (2007-2011)',\n       'Mid-year Estimate totals', 'Mid-year Estimates 2012, by age',\n       'Obesity', 'Population Density', 'Qualifications (2011 Census)',\n       'Religion (2011)', 'Road Casualties', 'Tenure (2011)',\n       'Unnamed: 0_level_0', 'Unnamed: 1_level_0'],\n      dtype='object')\n```\n:::\n:::\n\n\nThese are the *values* for those levels across the actual columns in the data frame, notice the repeated 'Age Structure (2011 Census)':\n\n::: {#cd6f19f4 .cell execution_count=11}\n``` {.python .cell-code}\nexcel_atlas.columns.get_level_values(0)[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nIndex(['Unnamed: 0_level_0', 'Unnamed: 1_level_0',\n       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n       'Age Structure (2011 Census)', 'Mid-year Estimate totals'],\n      dtype='object')\n```\n:::\n:::\n\n\nAnd here are the *values* for the second level of the index ('row 1' in the Excel file):\n\n::: {#112316ae .cell execution_count=12}\n``` {.python .cell-code}\nexcel_atlas.columns.get_level_values(1)[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nIndex(['Unnamed: 0_level_1', 'Unnamed: 1_level_1', 'All Ages', '0-15', '16-29',\n       '30-44', '45-64', '65+', 'Working-age', 'All Ages'],\n      dtype='object')\n```\n:::\n:::\n\n\nBy extension, if we drop a level 0 index then *all* of the columns that it supports at levels 1 and 2 are _also_ dropped: so when we drop `Mid-year Estimate totals` from level 0 then all 11 of the 'Mid-year Estimate totals (2002...2012)' columns are dropped in one go.\n\n::: {#c9150452 .cell execution_count=13}\n``` {.python .cell-code}\nexcel_atlas[['Mid-year Estimate totals']].head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"11\" halign=\"left\">Mid-year Estimate totals</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th colspan=\"11\" halign=\"left\">All Ages</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>2002</th>\n      <th>2003</th>\n      <th>2004</th>\n      <th>2005</th>\n      <th>2006</th>\n      <th>2007</th>\n      <th>2008</th>\n      <th>2009</th>\n      <th>2010</th>\n      <th>2011</th>\n      <th>2012</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7280.0</td>\n      <td>7115.0</td>\n      <td>7118.0</td>\n      <td>7131.0</td>\n      <td>7254.0</td>\n      <td>7607.0</td>\n      <td>7429.0</td>\n      <td>7472.0</td>\n      <td>7338.0</td>\n      <td>7412.0</td>\n      <td>7604.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6333.0</td>\n      <td>6312.0</td>\n      <td>6329.0</td>\n      <td>6341.0</td>\n      <td>6330.0</td>\n      <td>6323.0</td>\n      <td>6369.0</td>\n      <td>6570.0</td>\n      <td>6636.0</td>\n      <td>6783.0</td>\n      <td>6853.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9236.0</td>\n      <td>9252.0</td>\n      <td>9155.0</td>\n      <td>9072.0</td>\n      <td>9144.0</td>\n      <td>9227.0</td>\n      <td>9564.0</td>\n      <td>9914.0</td>\n      <td>10042.0</td>\n      <td>10088.0</td>\n      <td>10218.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#e0a8be7b .cell execution_count=14}\n``` {.python .cell-code}\ntest = excel_atlas.drop(columns=['Mid-year Estimate totals'], axis=1, level=0)\n\nprint(f\"Excel source had {excel_atlas.shape[1]} columns.\")\nprint(f\"Test now has {test.shape[1]} columns.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExcel source had 207 columns.\nTest now has 196 columns.\n```\n:::\n:::\n\n\n::: {#d6582538 .cell execution_count=15}\n``` {.python .cell-code}\n# Tidy up if the variable exists\nif 'test' in locals():\n    del(test)\n```\n:::\n\n\n### Questions\n\n- What data type is used for storing/accessing MultiIndexes? \n- *Why* is this is the appropriate data type?\n- How (conceptually) are the header rows in Excel are mapped on to levels in pandas?\n\n## Tidying Up\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low\n\nAlthough there's a lot of dealing with column names.\n\n:::\n\n### Dropping Named Levels\n\nThere's a *lot* of data in the data frame that we don't need for our Airbnb work, so let's go a bit further with the dropping of column-groups using the MultiIndex.\n\n::: {#84e56ea9 .cell execution_count=16}\n``` {.python .cell-code}\nto_drop = ['Mid-year Estimate totals','Mid-year Estimates 2012, by age','Religion (2011)',\n           'Land Area','Lone Parents (2011 Census)','Central Heating (2011 Census)','Health (2011 Census)',\n           'Low Birth Weight Births (2007-2011)','Obesity','Incidence of Cancer','Life Expectancy',\n           'Road Casualties']\ntidy = excel_atlas.drop(to_drop, axis=1, level=0)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of the MSOA Atlas data frame is now: 984 x 111\n```\n:::\n:::\n\n\nThis should drop you down to 984 x 111. Notice below that the multi-level _index_ has not changed but the multi-level _values_ remaining have!\n\n::: {#f6a6859a .cell execution_count=17}\n``` {.python .cell-code}\nprint(f\"There are {len(tidy.columns.levels[0].unique())} categories.\") # The categories\nprint(f\"But only {len(tidy.columns.get_level_values(0).unique())} values.\") #Â The actual values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are 30 categories.\nBut only 18 values.\n```\n:::\n:::\n\n\n### Selecting Columns using a List Comprehension\n\nNow we need to drop all of the percentages from the data set. These can be found at level 1, though they are specified in a number of different ways so you'll need to come up with a way to find them in the level 1 values using a list comprehension... \n\nI'd suggest looking for: \"(%)\", \"%\", and \"Percentages\". You may need to check both start and end of the string. You could also use a regular expression here instead of multiple tests. Either way *works*, but have a think about the tradeoffs between intelligibility, speed, and what *you* understand...\n\n:::: {.panel-tabset}\n\n#### Question\n\nSelection using multiple logical tests:\n\n```python\nto_drop = [x for x in tidy.columns.get_level_values(1) if (???)]\nprint(to_drop)\n```\n\nSelection using a regular expression:\n\n```python\nprint([x for x in tidy.columns.get_level_values(1) if re.search(???, x)])\n```\n\n#### Answer\n\n::: {#a995cfb3 .cell execution_count=18}\n``` {.python .cell-code}\nto_drop = [x for x in tidy.columns.get_level_values(1) if (\n    x.endswith(\"(%)\") or x.startswith(\"%\") or x.endswith(\"Percentages\") or x.endswith(\"%\"))]\nprint(to_drop)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Percentages', 'Percentages', 'Percentages', 'Percentages', 'Percentages', 'White (%)', 'Mixed/multiple ethnic groups (%)', 'Asian/Asian British (%)', 'Black/African/Caribbean/Black British (%)', 'Other ethnic group (%)', 'BAME (%)', 'United Kingdom (%)', 'Not United Kingdom (%)', '% of people aged 16 and over in household have English as a main language', '% of households where no people in household have English as a main language', 'Owned: Owned outright (%)', 'Owned: Owned with a mortgage or loan (%)', 'Social rented (%)', 'Private rented (%)', 'Household spaces with at least one usual resident (%)', 'Household spaces with no usual residents (%)', 'Whole house or bungalow: Detached (%)', 'Whole house or bungalow: Semi-detached (%)', 'Whole house or bungalow: Terraced (including end-terrace) (%)', 'Flat, maisonette or apartment (%)', 'Economically active %', 'Economically inactive %', '% of households with no adults in employment: With dependent children', '% living in income deprived households reliant on means tested benefit', '% of people aged over 60 who live in pension credit households', 'No cars or vans in household (%)', '1 car or van in household (%)', '2 cars or vans in household (%)', '3 cars or vans in household (%)', '4 or more cars or vans in household (%)']\n```\n:::\n:::\n\n\n::: {#5b431543 .cell execution_count=19}\n``` {.python .cell-code}\nprint([x for x in tidy.columns.get_level_values(1) if re.search(\"(?:%|Percentages)\", x)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Percentages', 'Percentages', 'Percentages', 'Percentages', 'Percentages', 'White (%)', 'Mixed/multiple ethnic groups (%)', 'Asian/Asian British (%)', 'Black/African/Caribbean/Black British (%)', 'Other ethnic group (%)', 'BAME (%)', 'United Kingdom (%)', 'Not United Kingdom (%)', '% of people aged 16 and over in household have English as a main language', '% of households where no people in household have English as a main language', 'Owned: Owned outright (%)', 'Owned: Owned with a mortgage or loan (%)', 'Social rented (%)', 'Private rented (%)', 'Household spaces with at least one usual resident (%)', 'Household spaces with no usual residents (%)', 'Whole house or bungalow: Detached (%)', 'Whole house or bungalow: Semi-detached (%)', 'Whole house or bungalow: Terraced (including end-terrace) (%)', 'Flat, maisonette or apartment (%)', 'Economically active %', 'Economically inactive %', '% of households with no adults in employment: With dependent children', '% living in income deprived households reliant on means tested benefit', '% of people aged over 60 who live in pension credit households', 'No cars or vans in household (%)', '1 car or van in household (%)', '2 cars or vans in household (%)', '3 cars or vans in household (%)', '4 or more cars or vans in household (%)']\n```\n:::\n:::\n\n\n::::\n\nWith both you should get: \n\n::: {.longform}\n\n::: {#fb083890 .cell execution_count=20}\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n['Percentages',\n 'Percentages',\n 'Percentages',\n 'Percentages',\n 'Percentages',\n 'White (%)',\n 'Mixed/multiple ethnic groups (%)',\n 'Asian/Asian British (%)',\n 'Black/African/Caribbean/Black British (%)',\n 'Other ethnic group (%)',\n 'BAME (%)',\n 'United Kingdom (%)',\n 'Not United Kingdom (%)',\n '% of people aged 16 and over in household have English as a main language',\n '% of households where no people in household have English as a main language',\n 'Owned: Owned outright (%)',\n 'Owned: Owned with a mortgage or loan (%)',\n 'Social rented (%)',\n 'Private rented (%)',\n 'Household spaces with at least one usual resident (%)',\n 'Household spaces with no usual residents (%)',\n 'Whole house or bungalow: Detached (%)',\n 'Whole house or bungalow: Semi-detached (%)',\n 'Whole house or bungalow: Terraced (including end-terrace) (%)',\n 'Flat, maisonette or apartment (%)',\n 'Economically active %',\n 'Economically inactive %',\n '% of households with no adults in employment: With dependent children',\n '% living in income deprived households reliant on means tested benefit',\n '% of people aged over 60 who live in pension credit households',\n 'No cars or vans in household (%)',\n '1 car or van in household (%)',\n '2 cars or vans in household (%)',\n '3 cars or vans in household (%)',\n '4 or more cars or vans in household (%)']\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nSee how regular expressions keep coming [baaaaaaaaack](https://jreades.github.io/fsds/sessions/week7.html#lectures)? That said, you can also often make use of simple string functions like `startswith` and `endswith` for this problem.\n\n:::\n\n### Drop by Level\n\nYou now need to drop these columns using the `level` keyword as part of your drop command. You have plenty of examples of how to drop values in place, but I'd suggest _first_ getting the command correct (maybe duplicate the cell below and change the code so that the result is saved to a dataframe called `test` before overwriting `tidy`?) and _then_ saving the change.\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ntidy = tidy.drop(to_drop, axis=1, level=???)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n```\n\n#### Answer\n\n::: {#2f0e40fa .cell execution_count=21}\n``` {.python .cell-code}\ntidy.drop(to_drop, axis=1, level=1, inplace=True)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of the MSOA Atlas data frame is now: 984 x 76\n```\n:::\n:::\n\n\n::::\n\nThe data frame should now be <code>984 x 76</code>. This is a _bit_ more manageable though still a _lot_ of data columns. Depending on what you decide to do for your final project you might want to revisit some of the columns that we dropped above... \n\n### Flattening the Index\n\nAlthough this ia big improvement, you'll have trouble saving or linking this data to other inputs. The problem is that Level 2 of the multi-index is mainly composed of 'Unnamed' values and so we need to merge it with Level 1 to simplify our data frame, and then merge _that_ with level 0...\n\n::: {#3f83a447 .cell execution_count=22}\n``` {.python .cell-code}\ntidy.columns.values[:3]\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\narray([('Unnamed: 0_level_0', 'Unnamed: 0_level_1', 'MSOA Code'),\n       ('Unnamed: 1_level_0', 'Unnamed: 1_level_1', 'MSOA Name'),\n       ('Age Structure (2011 Census)', 'All Ages', 'Unnamed: 2_level_2')],\n      dtype=object)\n```\n:::\n:::\n\n\nLet's use code to sort this out!\n\n::: {#d13c5b62 .cell execution_count=23}\n``` {.python .cell-code}\nnew_cols = []\nfor c in tidy.columns.values:\n    \n    #print(f\"Column label: {c}\")\n    l1 = f\"{c[0]}\"\n    l2 = f\"{c[1]}\"\n    l3 = f\"{c[2]}\"\n    \n    # The new column label\n    clabel = ''\n    \n    # Assemble new label from the levels\n    if not l1.startswith(\"Unnamed\"):\n        l1 = l1.replace(\" (2011 Census)\",'').replace(\" (2011)\",'').replace(\"Household \",'').replace(\"House Prices\",'').replace(\"Car or van availability\",'Vehicles').replace(' (2011/12)','')\n        l1 = l1.replace('Age Structure','Age').replace(\"Ethnic Group\",'').replace('Dwelling type','').replace('Income Estimates','')\n        clabel += l1\n    if not l2.startswith(\"Unnamed\"):\n        l2 = l2.replace(\"Numbers\",'').replace(\" House Price (Â£)\",'').replace(\"Highest level of qualification: \",'').replace(\"Annual Household Income (Â£)\",'hh Income').replace('Whole house or bungalow: ','').replace(' qualifications','')\n        l2 = l2.replace('At least one person aged 16 and over in household has English as a main language',\"1+ English as a main language\").replace(\"No people in household have English as a main language\",\"None have English as main language\")\n        clabel += ('-' if clabel != '' else '') + l2\n    if not l3.startswith(\"Unnamed\"):\n        clabel += ('-' if clabel != '' else '') + l3\n    \n    # Replace other commonly-occuring verbiage that inflates column name width\n    clabel = clabel.replace('--','-').replace(\" household\",' hh').replace('Owned: ','')\n    \n    #clabel = clabel.replace(' (2011 Census)','').replace(' (2011)','').replace('Sales - 2011.1','Sales - 2012')\n    #clabel = clabel.replace('Numbers - ','').replace(' (Â£)','').replace('Car or van availability','Vehicles')\n    #clabel = clabel.replace('Household Income Estimates (2011/12) - ','').replace('Age Structure','Age')\n    \n    new_cols.append(clabel)\n```\n:::\n\n\n::: {.longform}\n\n::: {#1f94c624 .cell execution_count=24}\n``` {.python .cell-code}\nnew_cols\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n['MSOA Code',\n 'MSOA Name',\n 'Age-All Ages',\n 'Age-0-15',\n 'Age-16-29',\n 'Age-30-44',\n 'Age-45-64',\n 'Age-65+',\n 'Age-Working-age',\n 'Households-All Households',\n 'Composition-Couple hh with dependent children',\n 'Composition-Couple hh without dependent children',\n 'Composition-Lone parent hh',\n 'Composition-One person hh',\n 'Composition-Other hh Types',\n 'White',\n 'Mixed/multiple ethnic groups',\n 'Asian/Asian British',\n 'Black/African/Caribbean/Black British',\n 'Other ethnic group',\n 'BAME',\n 'Country of Birth-United Kingdom',\n 'Country of Birth-Not United Kingdom',\n 'Language-1+ English as a main language',\n 'Language-None have English as main language',\n 'Tenure-Owned outright',\n 'Tenure-Owned with a mortgage or loan',\n 'Tenure-Social rented',\n 'Tenure-Private rented',\n 'Household spaces with at least one usual resident',\n 'Household spaces with no usual residents',\n 'Detached',\n 'Semi-detached',\n 'Terraced (including end-terrace)',\n 'Flat, maisonette or apartment',\n 'Population Density-Persons per hectare (2012)',\n 'Median-2005',\n 'Median-2006',\n 'Median-2007',\n 'Median-2008',\n 'Median-2009',\n 'Median-2010',\n 'Median-2011',\n 'Median-2012',\n 'Median-2013 (p)',\n 'Sales-2005',\n 'Sales-2006',\n 'Sales-2007',\n 'Sales-2008',\n 'Sales-2009',\n 'Sales-2010',\n 'Sales-2011',\n 'Sales-2011.1',\n 'Sales-2013(p)',\n 'Qualifications-No',\n 'Qualifications-Level 1',\n 'Qualifications-Level 2',\n 'Qualifications-Apprenticeship',\n 'Qualifications-Level 3',\n 'Qualifications-Level 4 and above',\n 'Qualifications-Other',\n 'Qualifications-Schoolchildren and full-time students: Age 18 and over',\n 'Economic Activity-Economically active: Total',\n 'Economic Activity-Economically active: Unemployed',\n 'Economic Activity-Economically inactive: Total',\n 'Economic Activity-Unemployment Rate',\n 'Adults in Employment-No adults in employment in hh: With dependent children',\n 'Total Mean hh Income',\n 'Total Median hh Income',\n 'Vehicles-No cars or vans in hh',\n 'Vehicles-1 car or van in hh',\n 'Vehicles-2 cars or vans in hh',\n 'Vehicles-3 cars or vans in hh',\n 'Vehicles-4 or more cars or vans in hh',\n 'Vehicles-Sum of all cars or vans in the area',\n 'Vehicles-Cars per hh']\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-caution}\n\n#### Stop\n\nMake sure you understand what is happening here before just moving on to the next thing. Try adding `print()` statements if it will help it to make sense. This sort of code comes up a _lot_ in the real world.\n\n:::\n\n::: {#74062864 .cell execution_count=25}\n``` {.python .cell-code}\ntidy.columns = new_cols  # <- Blow away complex index, replace with simple\ntidy.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSOA Code</th>\n      <th>MSOA Name</th>\n      <th>Age-All Ages</th>\n      <th>Age-0-15</th>\n      <th>Age-16-29</th>\n      <th>Age-30-44</th>\n      <th>Age-45-64</th>\n      <th>Age-65+</th>\n      <th>Age-Working-age</th>\n      <th>Households-All Households</th>\n      <th>...</th>\n      <th>Adults in Employment-No adults in employment in hh: With dependent children</th>\n      <th>Total Mean hh Income</th>\n      <th>Total Median hh Income</th>\n      <th>Vehicles-No cars or vans in hh</th>\n      <th>Vehicles-1 car or van in hh</th>\n      <th>Vehicles-2 cars or vans in hh</th>\n      <th>Vehicles-3 cars or vans in hh</th>\n      <th>Vehicles-4 or more cars or vans in hh</th>\n      <th>Vehicles-Sum of all cars or vans in the area</th>\n      <th>Vehicles-Cars per hh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E02000001</td>\n      <td>City of London 001</td>\n      <td>7375.0</td>\n      <td>620.0</td>\n      <td>1665.0</td>\n      <td>2045.0</td>\n      <td>2010.0</td>\n      <td>1035.0</td>\n      <td>5720.0</td>\n      <td>4385.0</td>\n      <td>...</td>\n      <td>38.0</td>\n      <td>59728.481886</td>\n      <td>46788.295472</td>\n      <td>3043.0</td>\n      <td>1100.0</td>\n      <td>173.0</td>\n      <td>51.0</td>\n      <td>18.0</td>\n      <td>1692.0</td>\n      <td>0.385861</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E02000002</td>\n      <td>Barking and Dagenham 001</td>\n      <td>6775.0</td>\n      <td>1751.0</td>\n      <td>1277.0</td>\n      <td>1388.0</td>\n      <td>1258.0</td>\n      <td>1101.0</td>\n      <td>3923.0</td>\n      <td>2713.0</td>\n      <td>...</td>\n      <td>319.0</td>\n      <td>31788.185996</td>\n      <td>27058.703760</td>\n      <td>1020.0</td>\n      <td>1186.0</td>\n      <td>424.0</td>\n      <td>66.0</td>\n      <td>17.0</td>\n      <td>2305.0</td>\n      <td>0.849613</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E02000003</td>\n      <td>Barking and Dagenham 002</td>\n      <td>10045.0</td>\n      <td>2247.0</td>\n      <td>1959.0</td>\n      <td>2300.0</td>\n      <td>2259.0</td>\n      <td>1280.0</td>\n      <td>6518.0</td>\n      <td>3834.0</td>\n      <td>...</td>\n      <td>268.0</td>\n      <td>43356.931547</td>\n      <td>36834.528738</td>\n      <td>1196.0</td>\n      <td>1753.0</td>\n      <td>691.0</td>\n      <td>155.0</td>\n      <td>39.0</td>\n      <td>3766.0</td>\n      <td>0.982264</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>E02000004</td>\n      <td>Barking and Dagenham 003</td>\n      <td>6182.0</td>\n      <td>1196.0</td>\n      <td>1277.0</td>\n      <td>1154.0</td>\n      <td>1543.0</td>\n      <td>1012.0</td>\n      <td>3974.0</td>\n      <td>2318.0</td>\n      <td>...</td>\n      <td>122.0</td>\n      <td>46701.436554</td>\n      <td>39668.206433</td>\n      <td>556.0</td>\n      <td>1085.0</td>\n      <td>515.0</td>\n      <td>128.0</td>\n      <td>34.0</td>\n      <td>2650.0</td>\n      <td>1.143227</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>E02000005</td>\n      <td>Barking and Dagenham 004</td>\n      <td>8562.0</td>\n      <td>2200.0</td>\n      <td>1592.0</td>\n      <td>1995.0</td>\n      <td>1829.0</td>\n      <td>946.0</td>\n      <td>5416.0</td>\n      <td>3183.0</td>\n      <td>...</td>\n      <td>307.0</td>\n      <td>34293.820288</td>\n      <td>29155.683536</td>\n      <td>1080.0</td>\n      <td>1423.0</td>\n      <td>551.0</td>\n      <td>109.0</td>\n      <td>20.0</td>\n      <td>2937.0</td>\n      <td>0.922714</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 76 columns</p>\n</div>\n```\n:::\n:::\n\n\nYou might want to have a look at _what_ the code below drops first before just running it... remember that you can pull apart any complex code into pieces:\n\n::: {#15e032a5 .cell execution_count=26}\n``` {.python .cell-code}\ntidy['MSOA Code'].isna()\ntidy[tidy['MSOA Code'].isna()].index\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nIndex([983], dtype='int64')\n```\n:::\n:::\n\n\n::: {#51a3bee1 .cell execution_count=27}\n``` {.python .cell-code}\ntidy.drop(index=tidy[tidy['MSOA Code'].isna()].index, inplace=True)\n```\n:::\n\n\n## Add Inner/Outer London Mapping\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate, since I'm not giving you many clues.\n\n:::\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nWe touched on `lambda` functions last week; it's a 'trivial' function that we don't even want to bother defining with `def`. We also used the lambda function in the context of `apply` so this is just another chance to remind yourself how this works. This is quite advanced Python, so don't panic if you don't get it right away and have to do some Googling... \n\n:::\n\nWe want to add the borough name and a 'subregion' name. We already have the borough name buried in a *separate* column, so step 1 is to extract that from the MSOA Name. Step 2 is to use the borough name as a lookup to the subregion name using a **lambda** function. The format for a lambda function is usually `lambda x: <code that does something with x and returns a value>`. Hint: you've got a dictionary and you know how to use it!\n\n### Add Boroughs\n\nWe first need to extract the borough names from one of the existing fields in the data frame... a *regex* that does *replacement* would be fastest and easiest: focus on what you *don't* need from the MSOA Name **string** and **replacing** that using a **regex**...\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ntidy['Borough'] = tidy['MSOA Name'].???\ntidy.Borough.unique()\n```\n\n#### Answer\n\n::: {#c5a110fc .cell execution_count=28}\n``` {.python .cell-code}\ntidy['Borough'] = tidy['MSOA Name'].str.replace(r' \\d+$','',regex=True)\ntidy.Borough.unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\narray(['City of London', 'Barking and Dagenham', 'Barnet', 'Bexley',\n       'Brent', 'Bromley', 'Camden', 'Croydon', 'Ealing', 'Enfield',\n       'Greenwich', 'Hackney', 'Hammersmith and Fulham', 'Haringey',\n       'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Islington',\n       'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth',\n       'Lewisham', 'Merton', 'Newham', 'Redbridge',\n       'Richmond upon Thames', 'Southwark', 'Sutton', 'Tower Hamlets',\n       'Waltham Forest', 'Wandsworth', 'Westminster'], dtype=object)\n```\n:::\n:::\n\n\n::::\n\nYou should get:\n\n::: {#cbc08518 .cell execution_count=29}\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray(['City of London', 'Barking and Dagenham', 'Barnet', 'Bexley',\n       'Brent', 'Bromley', 'Camden', 'Croydon', 'Ealing', 'Enfield',\n       'Greenwich', 'Hackney', 'Hammersmith and Fulham', 'Haringey',\n       'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Islington',\n       'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth',\n       'Lewisham', 'Merton', 'Newham', 'Redbridge',\n       'Richmond upon Thames', 'Southwark', 'Sutton', 'Tower Hamlets',\n       'Waltham Forest', 'Wandsworth', 'Westminster'], dtype=object)\n```\n:::\n:::\n\n\n### Map Boroughs to Subregions\n\nAnd now you need to understand how to *apply* the mapping ot the Borough field using a `lambda` function. It's fairly straightforward once you know the syntax: just a dictionary lookup. But as usual, you might want to first create a new cell and experiment with the output from the apply function before using it to write the `Subregion` field of the data frame...\n\n::: {#8838622f .cell execution_count=30}\n``` {.python .cell-code}\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden','City of London']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet']:\n    mapping[b]='Outer West and North West'\n```\n:::\n\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ntidy['Subregion'] = tidy.Borough.apply(???)\n```\n\n#### Answer\n\n::: {#856f97b8 .cell execution_count=31}\n``` {.python .cell-code}\ntidy['Subregion'] = tidy.Borough.apply(lambda x: mapping[x])\n```\n:::\n\n\n::::\n\n### And Save\n\nThere's a little snipped of useful code to work out here: we need to check if the `clean` directory exists in the `data` directory; if we don't then the `tidy.to_parquet()` call will fail.\n\n::: {#609b1738 .cell execution_count=32}\n``` {.python .cell-code}\nif not os.path.exists(os.path.join('data','clean')):\n    os.makedirs(os.path.join('data','clean'))\ntidy.to_parquet(os.path.join('data','clean','MSOA_Atlas.parquet'))\nprint(\"Done.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDone.\n```\n:::\n:::\n\n\n### Questions\n\n- What are the advantages to `apply` and `lambda` functions over looping and named functions? \n- When might you choose a named function over a lambda function?\n\n## Merge Data & Geography\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low, except for plotting.\n\n:::\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nWe'll cover joins (of which a `merge` is just one type) in the [final week's lectures](https://jreades.github.io/fsds/sessions/week10.html#lectures), but between what you'd done in GIS and what we have here there should be enough here for you to start being able to make sense of how they work so that you don't have to wait until Week 10 to think about how this could help you with your Group Assessment.\n\n:::\n\nFirst, we need to download the MSOA source file, which is a zipped archive of a Shapefile:\n\n::: {#3932bcf6 .cell execution_count=33}\n``` {.python .cell-code}\n# Oh look, we can read a Shapefile without needing to unzip it!\nmsoas = gpd.read_file(\n    cache_data('https://github.com/jreades/fsds/blob/master/data/src/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip?raw=true', \n               os.path.join('data','geo')), driver='ESRI Shapefile')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip locally!\n\tSize is 7 MB (7,381,177 bytes)\n```\n:::\n:::\n\n\n### Identifying Matching Columns\n\nLooking at the first few columns of each data frame, which one might allow us to link the two files together? You've done this in GIS. *Remember*: the column *names* don't need to match for us to use them in a join, it's the *values* that matter.\n\n::: {#b930e800 .cell execution_count=34}\n``` {.python .cell-code}\nprint(f\"Column names: {', '.join(tidy.columns.tolist()[:5])}\")\ntidy.iloc[:3,:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumn names: MSOA Code, MSOA Name, Age-All Ages, Age-0-15, Age-16-29\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSOA Code</th>\n      <th>MSOA Name</th>\n      <th>Age-All Ages</th>\n      <th>Age-0-15</th>\n      <th>Age-16-29</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E02000001</td>\n      <td>City of London 001</td>\n      <td>7375.0</td>\n      <td>620.0</td>\n      <td>1665.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E02000002</td>\n      <td>Barking and Dagenham 001</td>\n      <td>6775.0</td>\n      <td>1751.0</td>\n      <td>1277.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E02000003</td>\n      <td>Barking and Dagenham 002</td>\n      <td>10045.0</td>\n      <td>2247.0</td>\n      <td>1959.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Merge\n\nOne more thing: if you've got more than one choice I'd *always* go with a code over a name because one is intended for matching and other is not...\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ngdf = pd.merge(msoas, tidy, left_on=???, right_on=???, how='inner')\ngdf = gdf.drop(columns=['MSOA11CD','MSOA11NM','OBJECTID'])\n\nprint(f\"Final MSOA Atlas data frame has shape {gdf.shape[0]:,} x {gdf.shape[1]}\")\n```\n\n#### Answer\n\n::: {#3a52d060 .cell execution_count=35}\n``` {.python .cell-code}\ngdf = pd.merge(msoas, tidy, left_on='MSOA11CD', right_on='MSOA Code', how='inner')\ngdf = gdf.drop(columns=['MSOA11CD','MSOA11NM','OBJECTID'])\n\nprint(f\"Final MSOA Atlas data frame has shape {gdf.shape[0]:,} x {gdf.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal MSOA Atlas data frame has shape 983 x 86\n```\n:::\n:::\n\n\n::::\n\nYou should get <code>Final data frame has shape 983 x 86</code>.\n\n### Plot Choropleth\n\nLet's plot the median income in 2011 column using the `plasma` colour ramp... The rest is to show you how to customise a legend.\n\n```python\ncol = 'Median-2011'\nfig = gdf.plot(column=???, cmap='???', \n         scheme='FisherJenks', k=7, edgecolor='None', \n         legend=True, legend_kwds={'frameon':False, 'fontsize':8},\n         figsize=(8,7));\nplt.title(col.replace('-',' '));\n\n# Now to modify the legend: googling \"geopandas format legend\"\n# brings me to: https://stackoverflow.com/a/56591102/4041902\nleg = fig.get_legend()\nleg._loc = 3\n\nfor lbl in leg.get_texts():\n    label_text = lbl.get_text()\n    [low, hi] = label_text.split(', ')\n    new_text = f'Â£{float(low):,.0f} - Â£{float(hi):,.0f}'\n    lbl.set_text(new_text)\n\nplt.show();\n```\n\n### Save\n\n```python\ngdf.to_geoparquet(os.path.join('data','geo','MSOA_Atlas.geoparquet'))\n```\n\n### Questions\n\n- Try changing the colour scheme, classification scheme, and number of classes to see if you feel there's a *better* opeion than the one shown above... Copy the cell (click on anywhere outside the code and then hit `C` to copy. Then click on this cell *once*, and hit `V` to paste.\n\n# Splitting into Test & Train\n\n::: {.callout-note}\n\n    **&#128279; Connections**: Here you will be using a standard approach in Machine Learningknown as _test/train  split_, although we _won't_ be doing any actual ML (&#128517;). Here we are just going to use it to explore some issues raised by normalisation and standardisation in [this week's lectures](https://jreades.github.io/fsds/sessions/week8.html#lectures).\n:::\n\nA standard approach to Machine Learning, and something that is becoming more widely used elsewhere, is the splitting of a large data into set into testing and training components. Typically, you would take 80-90% of your data to 'train' your algorithm and withold between 10-20% for validation ('testing'). An even 'stricter' approach, in the sense of trying to ensure the robustness of your model against outlier effects, is [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) such as [k-folds cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n\nSci-Kit Learn is probably *the* most important reason Python has become the *de fact* language of data science. Test/train-split is used when to avoid over-fitting when we are trying to **predict** something; so here Sci-Kit Learn _expects_ that you'll have an `X` which is your **predictors** (the inputs to your model) and a `y` which is the thing you're **trying to predict** (because: $y = \\beta X + \\epsilon$). \n\nWe're not building a model here (that's for Term 2!) so we'll just 'pretend' that we're trying to predict the price of a listing and will set that up as our `y` data set *so that* we can see how the choice of normalisation/standardisation technique affects the robustness of the model against 'new' data. Notice too that you can pass a data frame directly to Sci-Kit Learn and it will split it for you.\n\n## Reload\n\n::: {.callout-tip}\n\n    In future 'runs' of this notebook you can now just pick up here and skip all of Task 1.\n:::\n\nOn subsequent runs of this notebook you might just want to start here!\n\n```python\n# Notice this handy code: we check if the data is already\n# in memory. And notice this is just a list comprehension\n# to see what is locally loaded.\nif 'gdf' not in locals():\n    gdf = gpd.read_parquet(os.path.join('data','geo','MSOA_Atlas.geoparquet'))\nprint(gdf.shape)\n```\n\n```python\ncategoricals = ['Borough','Subregion']\nfor c in categoricals:\n    gdf[c] = gdf[c].astype('category')\n```\n\n## Split\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low!\n\n:::\n\nFor our purposes this is a little bit overkill as you could also use pandas' `sample(frac=0.2)` and the indexes, but it's useful to see how this works. You use test/train split to get **four** data sets out: the training data gives you two (predictors + target as separate data sets) and the testing data gives you two as well (predictors + target as separate data sets). These are sized accoridng to the `test_size` specfied in the `test_train_split` parameters.\n\n```python\nfrom sklearn.model_selection import train_test_split \n\npdf = gdf['Median-2011'].copy() # pdf for Median *P*rice b/c we need *something*\n\n# df == *data* frame (a.k.a. predictors or independent variables)\n# pr == *predicted* value (a.k.a. dependent variable)\n# Notice we don't want the median price included in our training data\ndf_train, df_test, pr_train, pr_test = train_test_split(\n                gdf.drop(columns=['Median-2011']), pdf, test_size=0.2, random_state=44)\n```\n\nBelow you should see that the data has been split roughly on the basis of the `test_size` parameter.\n\n```python\nprint(f\"Original data size: {gdf.shape[0]:,} x {gdf.shape[1]}\")\nprint(f\"  Training data size: {df_train.shape[0]:,} x {df_train.shape[1]} ({(df_train.shape[0]/gdf.shape[0])*100:.0f}%)\")\nprint(f\"  Testing data size:  {df_test.shape[0]:,} x {df_test.shape[1]} ({(df_test.shape[0]/gdf.shape[0])*100:.0f}%)\")\n```\n\nAlso notice the indexes of each pair of data sets match:\n\n```python\nprint(\", \".join([str(x) for x in df_train.index[:10]]))\nprint(\", \".join([str(x) for x in pr_train.index[:10]]))\n```\n\n## Plot Test/Train Data\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low, but important!\n\n:::\n\n```python\nboros = gpd.read_file(os.path.join('data','geo','Boroughs.gpkg'))\n```\n\n```python\nf,axes = plt.subplots(1,2, figsize=(12,5))\ndf_train.plot(ax=???)\ndf_test.plot(ax=???)\nboros.plot(ax=???, facecolor='none', edgecolor='r', linewidth=.5, alpha=0.4)\nboros.plot(ax=???, facecolor='none', edgecolor='r', linewidth=.5, alpha=0.4)\naxes[0].set_title('Training Data')\naxes[1].set_title('Testing Data');\naxes[0].set_ylim(150000,210000)\naxes[1].set_ylim(150000,210000)\naxes[0].set_xlim(500000,565000)\naxes[1].set_xlim(500000,565000)\naxes[1].set_yticks([]);\n```\n\n### Questions\n\n- Why might it be useful to produce a *map* of a test/train split? \n- Why might it matter *more* if you were dealing with user locations or behaviours?\n\n# Normalisation\n\nThe developers of [SciKit-Learn](https://scikit-learn.org/) define [normalisation](https://scikit-learn.org/stable/modules/preprocessing.html#normalization) as \"scaling individual samples to have **unit norm**.\" There are a _lot_ of subtleties to this when you start dealing with 'sparse' data, but for the most part it's worthwhile to think of this as a rescaling of the raw data to have similar ranges in order achieve some kind of comparison. This is such a common problem that sklearn offers a range of such (re)scalers including: `MinMaxScaler`.\n\nLet's see what effect this has on the data!\n\n```python\n# Sets some handy 'keywords' to tweak the Seaborn plot\nkwds = dict(s=7,alpha=0.95,edgecolor=\"none\")\n\n# Set the *hue order* so that all plots have the *same*\n# colour on the Subregion\nho = ['Inner East','Inner West','Outer West and North West','Outer South','Outer East and North East']\n```\n\n## Select Columns\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\nOne thing you'll need to explain is why I keep writing `df[cols+['Subregion']` and why I don't just add it to the `cols` variable at the start? Don't try to answer this now, get through the rest of Tasks 3 and 4 and see what you think.\n\n```python\ncols = ['Tenure-Owned outright', 'Tenure-Owned with a mortgage or loan',\n        'Tenure-Social rented', 'Tenure-Private rented']\n```\n\n*Answer*: one part of the answer is that it makes it easy to change the columns we select without having to remember to keep `Subregion`, but the more important reason is that it allows us to re-use *this* 'definition' of `cols` elsewhere throughout the rest of this practical without needing to remember to *remove* `Subregion`.\n\n```python\ntr_raw  = df_train[cols+['Subregion']].copy() # train raw\ntst_raw = df_test[cols+['Subregion']].copy()  # test raw\n```\n\n## Fit to Data\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate if you want to understand what `reshape` is doing.\n\n:::\n\nFit the training data:\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Notice what this is doing! See if you can explain it clearly.\nscalers = [???.fit(???[x].values.reshape(-1,1)) for x in cols]\n```\n\n## Apply Transformations\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nTrain:\n\n```python\ntr_normed = tr_raw.copy()\n\nfor i, sc in enumerate(scalers):\n    # Ditto this -- can you explain what this code is doing\n    tr_normed[cols[i]] = sc.transform(df_???[cols[i]].values.reshape(-1,1))\n```\n\nTest:\n\n```python\ntst_normed = tst_raw.copy()\n\nfor i, sc in enumerate(scalers):\n    tst_normed[cols[i]] = sc.transform(df_???[cols[i]].values.reshape(-1,1))\n```\n\n::: {.callout-note}\n\n    **&#128279; Connections**: You don't _have_ to fully understand the next section, but if you are able to get your head around it then this will seriously help you to prepare for the use of more advanced techniques in modelling and programming.\n:::\n\nCheck out the properties of `tst_normed` below. If you've understood what the `MinMaxScaler` is doing then you should be able to spot something unexpected in the transformed *test* outputs. If you've *really* understood this, you'll see why this result is problematic for *models*. *Hint*: one way to think of it is an issue of **extrapolation**.\n\n```python\nfor c in cols:\n    print(f\"  Minimum: {tst_normed[c].min():.4f}\")\n    ???\n```\n\n## Plot Distributions\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\n```python\ntr_raw.columns     = [re.sub('(-|/)',\"\\n\",x) for x in tr_raw.columns.values]\ntst_raw.columns    = [re.sub('(-|/)',\"\\n\",x) for x in tst_raw.columns.values]\ntr_normed.columns  = [re.sub('(-|/)',\"\\n\",x) for x in tr_normed.columns.values]\ntst_normed.columns = [re.sub('(-|/)',\"\\n\",x) for x in tst_normed.columns.values]\n```\n\n```python\nsns.pairplot(data=tr_raw, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);\n```\n\n```python\nsns.pairplot(data=tr_normed, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);\n```\n\n### Questions\n\n- Why do I keep writing `df[cols+['Subregion']`? Why I don't just add Subregions to the `cols` variable at the start?\n- What has changed between the two plots (of `tr_raw` and `tr_normed`)?\n- What is the _potential_ problem that the use of the transformer fitted on `tr_normed` to data from `tst_normed` might cause? *Hint*: this is why I asked you to investigate the data in the empty code cell above.\n- Can you explain what this is doing: `[MinMaxScaler().fit(df_train[x].values.reshape(-1,1)) for x in cols]`?\n- Can you explain what *this* is doing: `sc.transform(df_test[cols[i]].values.reshape(-1,1))`?\n\n# Standardisation \n\nStandardisation is typically focussed on rescaling data to have a mean (or median) of 0 and standard deviation or IQR of 1. That these approaches are conceptually tied to the idea of symmetric, unimodal data such as that encountered in the standard normal distribution. Rather confusingly, many data scientists will use standardisation and normalisation largely interchangeably!\n\n```python\ncol = 'Vehicles-No cars or vans in hh'\ntr  = df_train[[col]].copy()\ntst = df_test[[col]].copy()\n```\n\n## Z-Score Standardisation\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\n```python\nstsc = StandardScaler().fit(tr[col].values.reshape(-1,1))\n\ntr[f\"Z. {col}\"]  = stsc.transform(???)\ntst[f\"Z. {col}\"] = stsc.transform(???)\n```\n\n## Inter-Quartile Standardisation\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\n```python\nrs = ???(quantile_range=(25.0, 75.0)).fit(???)\n\ntr[f\"IQR. {col}\"] = rs.transform(???)\ntst[f\"IQR. {col}\"] = rs.transform(???)\n```\n\n## Plot Distributions\n\n::: {.callout-note}\n\n    **&#128279; Connections**: The point of these next plots is simply to show that *linear* transformations (which are 'reversible') is about changing things like the magnitude/scale of our data but doesn't fundamentally change relationships *between* observations.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\n```python\nsns.jointplot(data=tr, x=f\"{col}\", y=f\"Z. {col}\", kind='kde'); # hex probably not the best choice\n```\n\n```python\nsns.jointplot(data=tr, x=f\"{col}\", y=f\"IQR. {col}\", kind='kde'); # hex probably not the best choice\n```\n\n```python\nsns.jointplot(data=tr, x=f\"Z. {col}\", y=f\"IQR. {col}\", kind='hex'); # hex probably not the best choice\n```\n\nPerhaps a little more useful...\n\n```python\nax = sns.kdeplot(tr[f\"Z. {col}\"])\nsns.kdeplot(tr[f\"IQR. {col}\"], color='r', ax=ax)\nplt.legend(loc='upper right', labels=['Standard', 'Robust']) # title='Foo'\nax.ticklabel_format(useOffset=False, style='plain')\nax.set_xlabel(\"Standardised Value for No cars or vans in hh\")\n```\n\n### Questions?\n\n- Can you see the differences between these two rescalers? \n- Can you explain why you might want to choose one over the other?\n\n# Non-Linear Transformations\n\n::: {.callout-note}\n\n    **&#128279; Connections**: Now *these* transformations are not directly revsersible because they change the actual relationships between observations in the data. We use these when we need to achieve particular 'behaviours' from our data in order to improve our model's 'predictive' ability. Even a simple regression is doing a kind of prediction ($y = \\beta X + \\epsilon$). You covered these concepts in [this week's lectures](https://jreades.github.io/fsds/sessions/week8.html#lectures).\n:::\n\nSo transformations are useful when a data series has features that make comparisons or analysis difficult, or that affect our ability to intuit meaningful difference. By manipulating the data using one or more mathematical operations we can sometimes make it more *tractable* for subsequent analysis. In other words, it's all about the _context_ of our data.\n\n[![How tall is tall?](http://img.youtube.com/vi/-VjcCr4uM6w/0.jpg)](http://www.youtube.com/watch?v=-VjcCr4uM6w)\n\nFrom above, we know the _Median Income_ data are _not_ normally distributed, but can we work out what distribution best represents _Median Income_? This can be done by comparing the shape of the histogram to the shapes of theoretical distributitions. For example:\n\n- the [log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution) distribution\n- the [exponential](https://en.wikipedia.org/wiki/Exponential_distribution) distribution\n- the [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution) distribution (for non-continuous data)\n \nFrom looking at those theoretical distributions, we might make an initial guess as to the type of distribution. There are actually _many_ other distributions encountered in real life data, but these ones are particuarly common. A wider view of this would be that [quantile and power transformations](https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation) are ways of preserving the rank of values but lose many of the other features of the relationships that might be preserved by, for instance, the standard scaler.\n\nIn the case of Median Income, taking a log-transform of the data might make it _appear_ more normal: you do **not** say that a transformation _makes_ data more normal, you say either that 'it allows us to treat the data as normally distributed' or that 'the transformed data follows a log-normal distribution'.\n\n## The Normal Distribution\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nZ-scores are often associated with the normal distribution because their interpretation implicitly assumes a normal distribution. Or to put it another way... You can always calculate z-scores for your data (it's just a formula applied to data points), but their _intuitive meaning_ is lost if your data don't have something like a normal distribution (or follow the [68-95-99.7 rule](https://en.wikipedia.org/wiki/68â€“95â€“99.7_rule)).\n\nBut... what if our data are non-normal? Well, Just because data are non-normal doesn't mean z-scores can't be calculated (we already did that above); we just have to be careful what we do with them... and sometimes we should just avoid them entirely. \n\n### Creating a Normal Distribution\n\nBelow is a function to create that theoretical normal distribution. See if you can understand what's going and add comments to the code to explain what each line does. \n\n```python\ndef normal_from_dist(series): \n    mu = ???          # mean of our data\n    sd = ???          # standard deviation of our data\n    n  = ???          # count how many observations are in our data\n    s = np.random.normal(???, ???, ???)   #use the parameters of the data just calculated to generate n random numbers, drawn from a normal distributions \n    return s                   #return this set of random numbers\n```\n\nTo make it easier to understand what the function above is doing, let's use it! We'll use the function to plot both a distribution plot with both histogram and KDE for our data, and then add a _second_ overplot distplot to the same fig showing the theoretical normal distribution (in red). We'll do this in a loop for each of the three variables we want to examine.\n\n### Visual Comparisons\n\nLooking at the output, which of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation _most_ appropriate as measures of centrality and spread?\n\n*Also*, how would you determine the _meaning_ of some of the departures from the normal distribution?\n\n```python\nselection = [x for x in df_train.columns.values if x.startswith('Composition')]\n\nfor c in selection:\n    ax = sns.kdeplot(df_train[c])\n    sns.kdeplot(normal_from_dist(df_train[c]), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title='Foo'\n    ax.ticklabel_format(useOffset=False, style='plain')\n    if ax.get_xlim()[1] > 999999:\n        plt.xticks(rotation=45)\n    plt.show()\n```\n\n### Questions\n\n- Which, if any, of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation _most_ appropriate as measures of centrality and spread?\n- How might you determine the _significance_ of some of the departures from the normal distribution?\n\n## Logarithmic Transformations\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nTo create a new series in the data frame containing the natural log of the original value itâ€™s a similar process to what we've done before, but since pandas doesn't provide a log-transform operator (i.e. you canâ€™t call `df['MedianIncome'].log()` ) we need to use the `numpy` package since pandas data series are just numpy arrays with some fancy window dressing that makes them even _more_ useful.\n\nLet's perform the transform then compare to the un-transformed data. Comment the code below to ensure that you understand what it is doing. \n\n### Apply and Plot\n\n```python\ncols = ['Median-2012','Total Mean hh Income']\n\nfor m in cols:\n    s  = df_train[m] # s == series\n    ts = ???.???(s)   # ts == transformed series\n    \n    ax = sns.kdeplot(s)\n    sns.kdeplot(normal_from_dist(s), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title also an option\n    plt.title(\"Original Data\")\n    \n    ### USEFUL FORMATTING TRICKS ###\n    # This turns off scientific notation in the ticklabels\n    ax.ticklabel_format(useOffset=False, style='plain')\n    # Notice this snippet of code\n    ax.set_xlabel(ax.get_xlabel() + \" (Raw Distribution)\")\n    # Notice this little code snippet too\n    if ax.get_xlim()[1] > 999999:\n        plt.xticks(rotation=45)\n    \n    plt.show()\n    \n    ax = sns.kdeplot(ts)\n    sns.kdeplot(normal_from_dist(ts), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n    ax.ticklabel_format(useOffset=False, style='plain')\n    ax.set_xlabel(ax.get_xlabel() + \" (Logged Distribution)\")\n    if ax.get_xlim()[1] > 999999:\n        plt.xticks(rotation=45)\n    plt.title(\"Log-Transformed Data\")\n    plt.show()\n```\n\nHopefully, you can see that the transformed data do indeed look 'more normal'; the peak of the red and blue lines are closer together and the blue line at the lower extreme is also closer to the red line, but we can check this by seeing what has happened to the z-scores.\n\n## Power Transformations \n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\n```python\ncols = ['Median-2012','Total Mean hh Income']\npt = ???(method='yeo-johnson')\n\nfor m in cols:\n    s  = df_train[m] # s == series\n    ts = pt.fit_transform(s.values.reshape(-1,1))\n    print(f\"Using lambda (transform 'exponent') of {pt.lambdas_[0]:0.5f}\")\n    \n    ax = sns.kdeplot(ts.reshape(-1,))\n    \n    sns.kdeplot(normal_from_dist(???), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n    ax.ticklabel_format(useOffset=False, style='plain')\n    ax.set_xlabel(m + \" (Transformed Distribution)\")\n    if ax.get_xlim()[1] > 999999: # <-- What does this do?\n        plt.xticks(rotation=45)\n    plt.title(\"Power-Transformed Data\")\n    plt.show();\n```\n\n# Principal Components Analysis\n\n::: {.callout-note}\n\n    **&#128279; Connections**: This is all about _dimensionality_ and the different ways that we can reduce dimensionality in our data in order to improve our models' robustness and gain a better understanding of whether (or not) there is structure in our data. We'll start with linear decomposition and then look at non-linear decomposition, which we also demonstrated with UMAP last week.\n:::\n\nNow we're going to ask the question: how can we represent our data using a smaller number of components that capture the variance in the original data. You should have covered PCA in Quantitative Methods.\n\n### Optional Reload\n\nUse this is your data gets messy...\n\n```python\ngdf = gpd.read_parquet(os.path.join('data','geo','MSOA_Atlas.geoparquet')).set_index('MSOA Code')\nprint(gdf.shape)\n```\n\n```python\ncategoricals = ['Borough','Subregion']\nfor c in categoricals:\n    gdf[c] = gdf[c].astype('category')\n```\n\n## Calculating Shares\n\n::: {.callout-caution collapse=\"true\"}\n\n#### Difficulty: Hard.\n\n:::\n\nSadly, there's no transformer to work this out for you automatically, but let's start by converting the raw population and household figures to shares so that our later dimensionality reduction steps aren't impacted by the size of the MSOA.\n\n```python\ngdf[['Age-All Ages','Households-All Households']].head(5)\n```\n\n### Specify Totals Columns\n\n```python\ntotal_pop = gdf['Age-All Ages']\ntotal_hh  = gdf['Households-All Households']\ntotal_vec = gdf['Vehicles-Sum of all cars or vans in the area']\n```\n\n### Specify Columns for Pop or HH Normalisation\n\n```python\npop_cols  = ['Age-', 'Composition-', 'Qualifications-', 'Economic Activity-', 'White', 'Mixed/multiple',\n             'Asian/Asian British', 'Black/African', 'BAME', 'Other ethnic',\n             'Country of Birth-']\nhh_cols   = [???, ???, ???, 'Detached', 'Semi-detached', 'Terraced', 'Flat, ']\n```\n\n```python\npopre = re.compile(r'^(?:' + \"|\".join(pop_cols) + r')')\nhhre  = re.compile(r'^(?:' + \"|\".join(???) + r')')\n```\n\n### Apply to Columns\n\n```python\ntr_gdf = gdf.copy()\ntr_gdf['Mean hh size'] = tr_gdf['Age-All Ages']/tr_gdf['Households-All Households']\n\nfor c in gdf.columns:\n    print(c)\n    if popre.match(c):\n        print(\"  Normalising by total population.\")\n        tr_gdf[c] = gdf[c]/???\n    elif ???.match(???):\n        print(\"  Normalising by total households.\")\n        tr_gdf[c] = gdf[c]/???\n    elif c.startswith('Vehicles-') and not c.startswith('Vehicles-Cars per hh'):\n        print(\"  Normalising by total vehicles.\")\n        tr_gdf[c] = gdf[c]/???\n    else:\n        print(\"  Passing through.\")\n```\n\n## Removing Columns\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nTo perform dimensionality we can only have numeric data. In theory, categorical data can be converted to numeric and retained, but there are two issues:\n\n1. Nominal data has no _innate_ order so we _can't_ convert > 2 categories to numbers and have to convert them to One-Hot Encoded values.\n2. A binary (i.e. One-Hot Encoded) variable will account for a _lot_ of variance in the data because it's only two values they are 0 and 1!\n\nSo in practice, it's probably a good idea to drop categorical data if you're planning to use PCA.\n\n### Drop Totals Columns\n\n```python\npcadf = tr_gdf.drop(columns=['Age-All Ages', 'Households-All Households',\n                             'Vehicles-Sum of all cars or vans in the area'])\npcadf = pcadf.set_index('MSOA Code')\n```\n\n### Drop Non-Numeric Columns\n\n```python\npcadf.select_dtypes(['category','object']).columns\n```\n\n```python\npcadf.drop(columns=pcadf.select_dtypes(['category','object']).columns.to_list(), inplace=True)\npcadf.drop(columns=['BNG_E','BNG_N','geometry', 'LONG', 'LAT','Shape__Are', 'Shape__Len'], inplace=True)\n```\n\n```python\npcadf.columns\n```\n\n## Rescale & Reduce\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nIn order to ensure that our results aren't dominated by a single scale (e.g. House Prices!) we need to rescale all of our data. You could easily try different scalers as well as a different parameters to see what effect this has on your results.\n\n### Robustly Rescale\n\nSet up the Robust Rescaler for inter-decile standardisation: 10th and 90th quantiles.\n\n```python\nrs = ???\n\nfor c in pcadf.columns.values:\n    pcadf[c] = rs.fit_transform(pcadf[c].values.reshape(-1, 1))\n```\n\n### PCA Reduce\n\n```python\nfrom sklearn.decomposition import PCA \n\npca = PCA(n_components=50, whiten=True) \n\npca.fit(pcadf)\n\nexplained_variance = pca.explained_variance_ratio_\nsingular_values = pca.singular_values_\n```\n\n### Examine Explained Variance\n\n```python\nx = np.arange(1,???)\nplt.plot(x, explained_variance)\nplt.ylabel('Share of Variance Explained')\nplt.show()\n```\n\n```python\nfor i in range(0, 20):\n    print(f\"Component {i:>2} accounts for {explained_variance[i]*100:>2.2f}% of variance\")\n```\n\nYou should get that Component 0 accounts for 31.35% of the variance and Component 19 accounts for 0.37%.\n\n###: How Many Components?\n\nThere are a number of ways that we could set a threshold for dimensionality reduction: \n- The most common is to look for the 'knee' in the Explained Variance plot above. That would put us at about 5 retained components.\n- Another is to just keep all components contributing more than 1% of the variance. That would put us at about 10 components.\n- You can also ([I discovered](https://medium.com/@nikolay.oskolkov/hi-jon-reades-my-sincere-apologies-for-this-very-late-reply-444f57054d14)) look to shuffle the data and repeatedly perform PCA to build confidence intervals. I have not implemented this (yet).\n\nIn order to _do_ anything with these components we need to somehow reattach them to the MSOAs. So that entails taking the transformed results (`X_train` and `X_test`) \n\n```python\nkn = knee_locator.KneeLocator(x, explained_variance, \n                              curve='convex', direction='decreasing', \n                              interp_method='interp1d')\nprint(f\"Knee detected at: {kn.knee}\")\nkn.plot_knee()\n```\n\n```python\nkeep_n_components = 7\n\n# If we weren't changing the number of components we\n# could re-use the pca object created above. \npca = PCA(n_components=keep_n_components, whiten=True)\n\nX_train = pca.fit_transform(???)\n\n# Notice that we get the _same_ values out,\n# so this is a *deterministic* process that\n# is fully replicable (allowing for algorithmic\n#Â and programming language differences).\nprint(f\"Total explained variance: {pca.explained_variance_ratio_.sum()*100:2.2f}%\")\nfor i in range(0, keep_n_components):\n    print(f\"  Component {i:>2} accounts for {pca.explained_variance_ratio_[i]*100:>5.2f}% of variance\")\n\n# Notice...\nprint(f\"X-train shape: {len(X_train)}\")\nprint(f\"PCA df shape: {pcadf.shape[0]}\")\n# So each observation has a row in X_train and there is \n# 1 column for each component. This defines the mapping\n# of the original data space into the reduced one\nprint(f\"Row 0 of X-train contains {len(X_train[0])} elements.\") \n```\n\n## Components to Columns\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nYou could actually do this more quickly (but less clearly) using `X_train.T` to transpose the matrix!\n\n```python\nfor i in range(0,keep_n_components):\n    s = pd.Series(X_train[:,???], index=pcadf.???)\n    pcadf[f\"Component {i+1}\"] = s\n```\n\n```python\npcadf.sample(3).iloc[:,-10:-4]\n```\n\n## (Re)Attaching GeoData\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\n```python\nmsoas = gpd.read_file(os.path.join('data','geo','Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip'), driver='ESRI Shapefile')\nmsoas = msoas.set_index('MSOA11CD')\nprint(msoas.columns)\n```\n\n```python\nmsoas.head(1)\n```\n\n```python\npcadf.head(1)\n```\n\n```python\ngpcadf = pd.merge(msoas.set_index(['MSOA11CD'], drop=True), pcadf, left_index=True, right_index=True, how='inner')\nprint(f\"Geo-PCA df has shape {gpcadf.shape[0]} x {gpcadf.shape[1]}\")\n```\n\nYou should get `PCA df has shape 983 x 89`.\n\n```python\ngpcadf['Borough'] = gpcadf.MSOA11NM.apply(???)\n```\n\n## Map the First _n_ Components\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty: Moderate.\n\n:::\n\nHow would you automate this so that the loop creates one plot for each of the first 3 components? How do you interpret these?\n\n```python\nfor comp in [f\"Component {x}\" for x in range(1,3)]:\n    ax = gpcadf.plot(column=???, cmap='plasma', \n         scheme='FisherJenks', k=7, edgecolor='None', legend=True, figsize=(9,7));\n    boros.plot(ax=ax, edgecolor='w', facecolor='none', linewidth=1, alpha=0.7)\n    ax.set_title(f'PCA {comp}')\n```\n\nYour first component map should look something like this:\n    \n![PCA Component 1](https://github.com/jreades/fsds/raw/master/practicals/img/PCA-Component-1.png)\n\n# UMAP\n\nUMAP is a non-linear dimensionality reduction technique. Technically, it's called *manifold* learning: imagine being able to roll a piece of paper up in more than just the 3rd dimension...). As a way to see if there is structure in your data this is a *much* better technique than one you might encounter in many tutorials: *t-SNE*. It has to do with how the two techniques 'learn' the manifold to use with your data. \n\n## UMAP on Raw Data\n\n::: {.callout-caution collapse=\"true\"}\n\n#### Difficulty: Hard.\n\n:::\n\n```python\nfrom umap import UMAP\n\n# You might want to experiment with all\n# 3 of these values -- it may make sense \n# to package a lot of this up into a function!\nkeep_dims=2\nrs=42\n\nu = UMAP(\n    n_neighbors=25,\n    min_dist=0.01,\n    n_components=keep_dims,\n    random_state=rs)\n\nX_embedded = u.fit_transform(???)\nprint(X_embedded.shape)\n```\n\n## Write to Data Frame\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\nCan probably also be solved using `X_embedded.T`.\n\n```python\nfor ix in range(0,X_embedded.shape[1]):\n    print(ix)\n    s = pd.Series(X_embedded[:,???], index=pcadf.???)\n    gpcadf[f\"Dimension {ix+1}\"] = s\n```\n\n## Visualise!\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\n```python\nrddf = gpcadf.copy() # Reduced Dimension Data Frame\n```\n\n### Simple Scatter\n\n```python\nf,ax = plt.subplots(1,1,figsize=(8,6))\nsns.scatterplot(x=rddf[???], y=rddf[???], hue=rddf['Borough'], legend=False, ax=ax)\n```\n\n### Seaborn Jointplot\n\nThat is *suggestive* of there being struccture in the data, but with 983 data points and 33 colours it's hard to make sense of what the structure *might* imply. Let's try this again using the Subregion instead and taking advantage of the Seaborn visualisation library's `jointplot` (joint distribution plot):\n\n```python\nrddf['Subregion'] = rddf.Borough.apply(lambda x: mapping[x])\n```\n\n```python\n# Sets some handy 'keywords' to tweak the Seaborn plot\nkwds = dict(s=7,alpha=0.95,edgecolor=\"none\")\n# Set the *hue order* so that all plots have some colouring by Subregion\nho = ['Inner East','Inner West','Outer West and North West','Outer South','Outer East and North East']\n```\n\n```python\ng = sns.jointplot(data=rddf, x=???, y=???, height=8, \n                  hue=???, hue_order=ho, joint_kws=kwds)\ng.ax_joint.legend(loc='upper right', prop={'size': 8});\n```\n\nYour jointplot should look like this:\n\n![UMAP Jointplot](https://github.com/jreades/fsds/raw/master/practicals/img/UMAP-Jointplot.png)\n\nWhat do you make of this?\n\nMaybe let's give this one last go splitting the plot out by subregion so that we can see how these vary:\n\n```python\nfor r in rddf.Subregion.unique():\n    g = sns.jointplot(data=rddf[rddf.Subregion==r], x='Dimension 1', y='Dimension 2', \n                  hue='Borough', joint_kws=kwds)\n    g.ax_joint.legend(loc='upper right', prop={'size': 8});\n    g.ax_joint.set_ylim(0,15)\n    g.ax_joint.set_xlim(0,15)\n    plt.suptitle(r)\n```\n\nWe can't unfortunately do any clustering at this point to create groups from the data (that's next week!) so for now note that there are several large-ish groups (in terms of membership) and few small ones picked up by t-SNE. Alos note that there is strong evidence of some incipient structure: Inner East and West largely clump together, while Outher East and Outer South also seem to group together, with Outer West being more distinctive. If you look back at the PCA Components (especially \\#1) you might be able to speculate about some reasons for this! Please note: this is _only_ speculation at this time!\n\nNext week we'll also add the listings data back in as part of the picture!\n\n## Map the _n_ Dimensions\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty: Low.\n\n:::\n\n```python\nfor comp in [f\"Dimension {x}\" for x in range(1,3)]:\n    f, ax = plt.subplots(1,1,figsize=(12,8))\n    rddf.plot(???);\n    boros.plot(edgecolor='w', facecolor='none', linewidth=1, alpha=0.7, ax=ax)\n    ax.set_title(f'UMAP {comp}')\n```\n\nYour first dimension map should look something like this:\n\n![UMAP Dimension 1](https://github.com/jreades/fsds/raw/master/practicals/img/UMAP-Dimension-1.png)\n\n## And Save\n\n```python\nrddf.to_parquet(os.path.join('data','clean','Reduced_Dimension_Data.geoparquet'))\n```\n\n### Questions\n\n- How would you compare/contrast PCA components with UMAP dimensions? Why do they not seem to show the same thing even though *both* seem to show *something*?\n- What might you do with the output of either the PCA or UMAP processes?\n\n## Credits!\n\n#### Contributors:\nThe following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk).\n\n#### License\nThese teaching materials are licensed under a mix of [The MIT License](https://opensource.org/licenses/mit-license.php) and the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n\n#### Potential Dependencies:\nThis notebook may depend on the following libraries: pandas, geopandas, sklearn, matplotlib, seaborn\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /Users/jreades/anaconda3/envs/sds/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.2\n---\n",
    "supporting": [
      "Practical-11-Dimensions_in_Data_files/figure-ipynb"
    ],
    "filters": []
  }
}