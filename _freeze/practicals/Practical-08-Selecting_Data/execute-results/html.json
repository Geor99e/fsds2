{
  "hash": "de072809223571e449a6f99fad1eacf9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Practical 8: Selecting Data\"\nsubtitle: \"Selecting & Joining Data\"\njupyter: python3\nfilters:\n  - qna\n  - quarto\n---\n\n\n| Complete | Part 1: Foundations | Part 2: Data | Part 3: Analysis |     |\n| :------- | :------------------ | :----------- | :--------------- | --: |\n| 90% | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9619;&#9619;&#9617;&#9617;&#9617; | 8/10\n\n::: {.callout-warning}\n\n#### Important\n\nThis practical focusses on two key bits of _implementation_: visualisation and data linkage! You will have seen quite a bit of each of these across the preceding three to four weeks, but they were picked up in an _ad-hoc_ way, here we try to systematise things a bit.\n\n:::\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nHere we're trying to tidy up the loose ends. You've already worked with basic data visualisations in Seaborn and Matplotlib (including (geo)panda's `plot` function), but we want you to have a better sense of how that _works_ as part of a coherent -- if altogether rather complex and overwhelming -- approach to managing a data visualisation. You've also already seen examples of joins and spatial joins before but, again, we just want to review them more formally now.\n\n:::\n\n# Preamble\n\n::: {#fe9dd47f .cell execution_count=1}\n``` {.python .cell-code}\nhost = 'http://orca.casa.ucl.ac.uk'\npath = '~jreades/data'\nymd  = '2023-09-06'\n```\n:::\n\n\n::: {#70371343 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#42445865 .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -> str:\n    \"\"\"Downloads and caches a remote file locally.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dest : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[-1] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    # Check if dest+filename does *not* exist -- \n    # that would mean we have to download it!\n    if not os.path.isfile(dfn) or os.path.getsize(dfn) < 1:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        # Convert the path back into a list (without)\n        # the filename -- we need to check that directories\n        # exist first.\n        path = os.path.split(dest)\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how\n        # to use the 'splat' operator! os.makedirs creates missing\n        # directories in a path automatically.\n        if len(path) >= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        # Download and write the file\n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(response.content)\n            \n        print('Done downloading...')\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n    return dfn\n```\n:::\n\n\n::: {#0cad1684 .cell execution_count=4}\n``` {.python .cell-code}\nddir = os.path.join('data','geo') # destination directory\npqt  = cache_data(f'{host}/{path}/{ymd}-listings.geoparquet', ddir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/2023-09-06-listings.geoparquet locally!\n```\n:::\n:::\n\n\n# Selecting Data\n\n## In Pandas\n\n### Recap: A First Query\n\n::: {#01090569 .cell execution_count=5}\n``` {.python .cell-code}\npd.read_parquet(f'{pqt}').head(3) \n```\n\n::: {.cell-output .cell-output-display execution_count=113}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_url</th>\n      <th>last_scraped</th>\n      <th>name</th>\n      <th>description</th>\n      <th>host_id</th>\n      <th>host_name</th>\n      <th>host_since</th>\n      <th>host_location</th>\n      <th>host_is_superhost</th>\n      <th>host_listings_count</th>\n      <th>...</th>\n      <th>price</th>\n      <th>minimum_nights</th>\n      <th>maximum_nights</th>\n      <th>availability_365</th>\n      <th>number_of_reviews</th>\n      <th>first_review</th>\n      <th>last_review</th>\n      <th>review_scores_rating</th>\n      <th>reviews_per_month</th>\n      <th>geometry</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92644</th>\n      <td>https://www.airbnb.com/rooms/92644</td>\n      <td>2023-09-06</td>\n      <td>Rental unit in Earlsfield · ★4.57 · 1 bedroom ...</td>\n      <td>&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Hi everyone! I have 2 ro...</td>\n      <td>498201</td>\n      <td>Dee Dee</td>\n      <td>2011-04-10</td>\n      <td>London, United Kingdom</td>\n      <td>False</td>\n      <td>1</td>\n      <td>...</td>\n      <td>42.0</td>\n      <td>2</td>\n      <td>730</td>\n      <td>217</td>\n      <td>216</td>\n      <td>2011-06-21</td>\n      <td>2022-10-29</td>\n      <td>4.57</td>\n      <td>1.45</td>\n      <td>b'\\x01\\x01\\x00\\x00\\x00\\xeax\\xcc@e\\xfc\\xc7\\xbf\\...</td>\n    </tr>\n    <tr>\n      <th>93015</th>\n      <td>https://www.airbnb.com/rooms/93015</td>\n      <td>2023-09-06</td>\n      <td>Rental unit in Hammersmith · ★4.82 · 2 bedroom...</td>\n      <td>Gorgeous 2 bed ground floor apartment with per...</td>\n      <td>499704</td>\n      <td>Sarah</td>\n      <td>2011-04-11</td>\n      <td>London, United Kingdom</td>\n      <td>False</td>\n      <td>1</td>\n      <td>...</td>\n      <td>175.0</td>\n      <td>5</td>\n      <td>240</td>\n      <td>40</td>\n      <td>38</td>\n      <td>2012-02-01</td>\n      <td>2022-09-30</td>\n      <td>4.82</td>\n      <td>0.27</td>\n      <td>b'\\x01\\x01\\x00\\x00\\x00\\r\\xabx#\\xf3\\xc8\\xcb\\xbf...</td>\n    </tr>\n    <tr>\n      <th>13913</th>\n      <td>https://www.airbnb.com/rooms/13913</td>\n      <td>2023-09-06</td>\n      <td>Rental unit in Islington · ★4.80 · 1 bedroom ·...</td>\n      <td>My bright double bedroom with a large window h...</td>\n      <td>54730</td>\n      <td>Alina</td>\n      <td>2009-11-16</td>\n      <td>London, United Kingdom</td>\n      <td>False</td>\n      <td>3</td>\n      <td>...</td>\n      <td>79.0</td>\n      <td>1</td>\n      <td>29</td>\n      <td>360</td>\n      <td>41</td>\n      <td>2010-08-18</td>\n      <td>2022-12-11</td>\n      <td>4.80</td>\n      <td>0.26</td>\n      <td>b'\\x01\\x01\\x00\\x00\\x00\\xeeZB&gt;\\xe8\\xd9\\xbc\\xbf\\...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\nThis should (I hope) be trivial to read now: we are loading a parquet file using pandas and taking advantage of Python's 'chaining' functionality (`<object>.<method>().<method>()...`) to return the first three rows using `head`. It is worth noticing that we're not even bothering to save the result of this command to a data frame (thus the lack of a `df =` in the code) and We're doing this here solely so that you can compare pandas and SQL/DuckDB syntax across each of the following steps.\n\n### Recap: Selecting some columns\n\nTo load a columnar subset of the data we have two options:\n\n1. Load all the data and *then* subset (which always happens with CSV files but is optional with other formats)\n2. Load only the columns we care about (which is possible with parquet files)\n\nAnd in code these are:\n\n#### Load *then* filter\n\n::: {#3f35b919 .cell execution_count=6}\n``` {.python .cell-code}\n%%time\npd.read_parquet(f'{pqt}')[['listing_url', 'price', 'number_of_reviews', 'last_review', 'host_name']].head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 179 ms, sys: 23.9 ms, total: 203 ms\nWall time: 165 ms\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=114}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_url</th>\n      <th>price</th>\n      <th>number_of_reviews</th>\n      <th>last_review</th>\n      <th>host_name</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92644</th>\n      <td>https://www.airbnb.com/rooms/92644</td>\n      <td>42.0</td>\n      <td>216</td>\n      <td>2022-10-29</td>\n      <td>Dee Dee</td>\n    </tr>\n    <tr>\n      <th>93015</th>\n      <td>https://www.airbnb.com/rooms/93015</td>\n      <td>175.0</td>\n      <td>38</td>\n      <td>2022-09-30</td>\n      <td>Sarah</td>\n    </tr>\n    <tr>\n      <th>13913</th>\n      <td>https://www.airbnb.com/rooms/13913</td>\n      <td>79.0</td>\n      <td>41</td>\n      <td>2022-12-11</td>\n      <td>Alina</td>\n    </tr>\n    <tr>\n      <th>15400</th>\n      <td>https://www.airbnb.com/rooms/15400</td>\n      <td>150.0</td>\n      <td>94</td>\n      <td>2023-05-01</td>\n      <td>Philippa</td>\n    </tr>\n    <tr>\n      <th>93734</th>\n      <td>https://www.airbnb.com/rooms/93734</td>\n      <td>46.0</td>\n      <td>180</td>\n      <td>2023-09-02</td>\n      <td>William</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n#### Filter *then* load\n\n::: {#6e7cc38a .cell execution_count=7}\n``` {.python .cell-code}\n%%time\npd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'last_review', 'host_name']).head(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 20.5 ms, sys: 1.35 ms, total: 21.9 ms\nWall time: 17.5 ms\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=115}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_url</th>\n      <th>price</th>\n      <th>number_of_reviews</th>\n      <th>last_review</th>\n      <th>host_name</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>92644</th>\n      <td>https://www.airbnb.com/rooms/92644</td>\n      <td>42.0</td>\n      <td>216</td>\n      <td>2022-10-29</td>\n      <td>Dee Dee</td>\n    </tr>\n    <tr>\n      <th>93015</th>\n      <td>https://www.airbnb.com/rooms/93015</td>\n      <td>175.0</td>\n      <td>38</td>\n      <td>2022-09-30</td>\n      <td>Sarah</td>\n    </tr>\n    <tr>\n      <th>13913</th>\n      <td>https://www.airbnb.com/rooms/13913</td>\n      <td>79.0</td>\n      <td>41</td>\n      <td>2022-12-11</td>\n      <td>Alina</td>\n    </tr>\n    <tr>\n      <th>15400</th>\n      <td>https://www.airbnb.com/rooms/15400</td>\n      <td>150.0</td>\n      <td>94</td>\n      <td>2023-05-01</td>\n      <td>Philippa</td>\n    </tr>\n    <tr>\n      <th>93734</th>\n      <td>https://www.airbnb.com/rooms/93734</td>\n      <td>46.0</td>\n      <td>180</td>\n      <td>2023-09-02</td>\n      <td>William</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice the difference in time!!!\n\n### Recap: Adding a constraint\n\n### Aggregating the return\n\n## In SQL\n\n### Parquet and DuckDB\n\nOne of the recent technical *revolutions* that has fundamentally reshaped my workflow is the combination of parquet files and in-memory databases. Parquet and Apache Arrow are [closely related](https://stackoverflow.com/a/56481636) but, in short, when you want to save large data sets in an easy-to-access format then Parquet should be your default choice. DuckDB gives you a way to treat Parquet files *as* a database **table** and run queries against it using standard SQL. You can [install DuckDB](https://duckdb.org/#quickinstall) on the command-line, but you can also query it from within Python using the appropriate module.\n\n::: {.callout-tip}\n\n#### Spatial DuckDB\n\nDuckDB also supports spatial queries via the [`SPATIAL` extension](https://duckdb.org/docs/extensions/spatial.html). Performance is *not* that of a tuned Postgres+PostGIS database, but the overhead of *creating* such a tuned database often exceeds the benefit for ad-hoc querying. Basically, Postgres+PostGIS is great if you're a company such as Booking.com, Airbnb, or OpenStreetMap, but it's most likely overkill for offline read-oriented applications.\n\n:::\n\n### A First Query\n\nLet's see a quick demonstration:\n\n::: {#7fd14a05 .cell execution_count=8}\n``` {.python .cell-code}\nimport duckdb as db\n\nquery = f'''\nSELECT *\nFROM read_parquet('{pqt}') \nLIMIT 3;\n'''\n\ndb.sql(query).to_df()\n```\n\n::: {.cell-output .cell-output-display execution_count=116}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_url</th>\n      <th>last_scraped</th>\n      <th>name</th>\n      <th>description</th>\n      <th>host_id</th>\n      <th>host_name</th>\n      <th>host_since</th>\n      <th>host_location</th>\n      <th>host_is_superhost</th>\n      <th>host_listings_count</th>\n      <th>...</th>\n      <th>minimum_nights</th>\n      <th>maximum_nights</th>\n      <th>availability_365</th>\n      <th>number_of_reviews</th>\n      <th>first_review</th>\n      <th>last_review</th>\n      <th>review_scores_rating</th>\n      <th>reviews_per_month</th>\n      <th>geometry</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.airbnb.com/rooms/92644</td>\n      <td>2023-09-06</td>\n      <td>Rental unit in Earlsfield · ★4.57 · 1 bedroom ...</td>\n      <td>&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Hi everyone! I have 2 ro...</td>\n      <td>498201</td>\n      <td>Dee Dee</td>\n      <td>2011-04-10</td>\n      <td>London, United Kingdom</td>\n      <td>False</td>\n      <td>1</td>\n      <td>...</td>\n      <td>2</td>\n      <td>730</td>\n      <td>217</td>\n      <td>216</td>\n      <td>2011-06-21</td>\n      <td>2022-10-29</td>\n      <td>4.57</td>\n      <td>1.45</td>\n      <td>[1, 1, 0, 0, 0, 234, 120, 204, 64, 101, 252, 1...</td>\n      <td>92644</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.airbnb.com/rooms/93015</td>\n      <td>2023-09-06</td>\n      <td>Rental unit in Hammersmith · ★4.82 · 2 bedroom...</td>\n      <td>Gorgeous 2 bed ground floor apartment with per...</td>\n      <td>499704</td>\n      <td>Sarah</td>\n      <td>2011-04-11</td>\n      <td>London, United Kingdom</td>\n      <td>False</td>\n      <td>1</td>\n      <td>...</td>\n      <td>5</td>\n      <td>240</td>\n      <td>40</td>\n      <td>38</td>\n      <td>2012-02-01</td>\n      <td>2022-09-30</td>\n      <td>4.82</td>\n      <td>0.27</td>\n      <td>[1, 1, 0, 0, 0, 13, 171, 120, 35, 243, 200, 20...</td>\n      <td>93015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.airbnb.com/rooms/13913</td>\n      <td>2023-09-06</td>\n      <td>Rental unit in Islington · ★4.80 · 1 bedroom ·...</td>\n      <td>My bright double bedroom with a large window h...</td>\n      <td>54730</td>\n      <td>Alina</td>\n      <td>2009-11-16</td>\n      <td>London, United Kingdom</td>\n      <td>False</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1</td>\n      <td>29</td>\n      <td>360</td>\n      <td>41</td>\n      <td>2010-08-18</td>\n      <td>2022-12-11</td>\n      <td>4.80</td>\n      <td>0.26</td>\n      <td>[1, 1, 0, 0, 0, 238, 90, 66, 62, 232, 217, 188...</td>\n      <td>13913</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 32 columns</p>\n</div>\n```\n:::\n:::\n\n\nAnd now let's unpack this:\n\n1. We import the `duckdb` library as `db`.\n2. We set up a SQL `query` using a multi-line f-string\n3. We use DuckDb to execute the query and return a pandas dataframe (`df`)\n\nWhat's particularly elegant here (and quite different from trying to talk to a Postres or MySQL database) is that there's no connect-execute-collect pattern; we just build the query and execute it!\n\n### Deciphering SQL\n\n::: {.callout-note}\n\n#### I *do* declare...\n\nNow let's take a look at the SQL query... SQL is what's called a [declarative language](https://en.wikipedia.org/wiki/Declarative_programming), meaning that it is about the logic we want the program to follow rather than the 'flow' of execution. Python supports *some* declarative elements but is more commonly seen as an imperative language supporting procedural or functional approaches. This is a long way of saying: SQL won't look like Python even though we're executing SQL from *within* Python.\n\n:::\n\nSo our query (with added line numbers for clarity) looked liked this:\n\n```{.sql code-line-numbers=\"true\"}\nSELECT *\nFROM read_parquet('{pqt}') \nLIMIT 3\n```\n\nLine-by-line this means:\n\n1. Select all columns (`SELECT <* == everything>`)\n2. From the parquet file (`FROM <table location>`)\n3. Limit the return to 3 rows (`LIMIT <row count>`)\n\nLet's look at some variations...\n\n### Choosing some columns\n\n::: {#02b10450 .cell execution_count=9}\n``` {.python .cell-code}\nquery = f'''\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nLIMIT 5;\n'''\n\ndb.sql(query).to_df()\n```\n\n::: {.cell-output .cell-output-display execution_count=117}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_url</th>\n      <th>price</th>\n      <th>number_of_reviews</th>\n      <th>last_review</th>\n      <th>host_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.airbnb.com/rooms/92644</td>\n      <td>42.0</td>\n      <td>216</td>\n      <td>2022-10-29</td>\n      <td>Dee Dee</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.airbnb.com/rooms/93015</td>\n      <td>175.0</td>\n      <td>38</td>\n      <td>2022-09-30</td>\n      <td>Sarah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.airbnb.com/rooms/13913</td>\n      <td>79.0</td>\n      <td>41</td>\n      <td>2022-12-11</td>\n      <td>Alina</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.airbnb.com/rooms/15400</td>\n      <td>150.0</td>\n      <td>94</td>\n      <td>2023-05-01</td>\n      <td>Philippa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.airbnb.com/rooms/93734</td>\n      <td>46.0</td>\n      <td>180</td>\n      <td>2023-09-02</td>\n      <td>William</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n```{.sql code-line-numbers=\"true\"}\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nLIMIT 5;\n```\n\nIt should be fairly easy to see how the query has changed from last time, but line-by-line this means:\n\n1. Select a set of columns from the table in the order specified (`SELECT <column 1>, <column 30>, <column 5>...`)\n2. From the parquet file (`FROM <table location>`)\n3. Limit the return to 5 rows (`LIMIT <row count>`)\n\n### Adding a constraint\n\n::: {#3b05597e .cell execution_count=10}\n``` {.python .cell-code}\nquery = f'''\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nWHERE price < 250 \nAND number_of_reviews > 0\nAND property_type='Entire home/apt'\nLIMIT 5;\n'''\n\ndb.sql(query).to_df()\n```\n\n::: {.cell-output .cell-output-display execution_count=118}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_url</th>\n      <th>price</th>\n      <th>number_of_reviews</th>\n      <th>last_review</th>\n      <th>host_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.airbnb.com/rooms/20296839</td>\n      <td>96.0</td>\n      <td>7</td>\n      <td>2017-10-01</td>\n      <td>Lira</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.airbnb.com/rooms/20349067</td>\n      <td>99.0</td>\n      <td>1</td>\n      <td>2017-11-12</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.airbnb.com/rooms/22959348</td>\n      <td>100.0</td>\n      <td>3</td>\n      <td>2018-02-04</td>\n      <td>Robert</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.airbnb.com/rooms/42969992</td>\n      <td>173.0</td>\n      <td>1</td>\n      <td>2021-10-24</td>\n      <td>Duda</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.airbnb.com/rooms/649784743352942906</td>\n      <td>91.0</td>\n      <td>9</td>\n      <td>2023-03-22</td>\n      <td>Travelnest</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIn this query we've added *three* constraints using a `WHERE`, which is asking DuckDB to find all of the rows *where* the following things are true:\n\n4. The `price` must be less than ($)250/night\n5. The `number_of_reviews` must be more than 0\n6. The `property_type` must be `Entire home/apt`\n\n### Aggregating the return\n\nSo far, we've seen a few ways (and hopefully enough to get you started) to *select* data, but databases also 'excel' at aggregating data in various ways. We aren't going to get into things like windowing functions or stored procedures here, but even simple aggregates done in DuckDB can vastly improve on the performance of pandas.\n\n::: {.callout-tip}\n\nWHen you aggregate data you need to retrieve *every* column in the `SELECT` portion that you `GROUP BY` in the `WHERE` portion of the query. This will make sense when you see the examples below... \n\n:::\n\n::: {#ad70d2a6 .cell execution_count=11}\n``` {.python .cell-code}\nquery = f'''\nSELECT property_type, room_type, COUNT(*) AS frequency, MEDIAN(price) \nFROM read_parquet('{pqt}') \nWHERE price < 1000 \nAND number_of_reviews > 0\nGROUP BY room_type, property_type\nORDER BY frequency DESC, room_type, property_type\nLIMIT 10;\n'''\n\ndb.sql(query).to_df()\n```\n\n::: {.cell-output .cell-output-display execution_count=119}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>property_type</th>\n      <th>room_type</th>\n      <th>frequency</th>\n      <th>median(price)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Entire rental unit</td>\n      <td>Entire home/apt</td>\n      <td>24637</td>\n      <td>136.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Private room in rental unit</td>\n      <td>Private room</td>\n      <td>9754</td>\n      <td>52.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Private room in home</td>\n      <td>Private room</td>\n      <td>7797</td>\n      <td>49.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Entire condo</td>\n      <td>Entire home/apt</td>\n      <td>7533</td>\n      <td>155.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Entire home</td>\n      <td>Entire home/apt</td>\n      <td>5228</td>\n      <td>200.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Private room in condo</td>\n      <td>Private room</td>\n      <td>2880</td>\n      <td>67.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Entire serviced apartment</td>\n      <td>Entire home/apt</td>\n      <td>1565</td>\n      <td>200.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Private room in townhouse</td>\n      <td>Private room</td>\n      <td>1204</td>\n      <td>55.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Entire townhouse</td>\n      <td>Entire home/apt</td>\n      <td>964</td>\n      <td>234.5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Private room in bed and breakfast</td>\n      <td>Private room</td>\n      <td>412</td>\n      <td>78.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere are quite a few changes to the query here so it's worth reviewing them in more detail:\n\n```{.sql code-line-numbers=\"true\"}\nSELECT property_type, room_type, COUNT(*) AS frequency, MEDIAN(price) \nFROM read_parquet('{pqt}') \nWHERE price < 1000 \nAND number_of_reviews > 0\nGROUP BY room_type, property_type\nORDER BY frequency DESC, room_type, property_type\nLIMIT 10;\n```\n\nKey things to note:\n\n1. We have two new aggregate *functions*:\n   - `COUNT(*)` returns a count of the number of rows in each group specified in the `GROUP BY` clause.\n   - `MEDIAN(price)` returns, unsurprisingly, the median value of the `price` column for each group specified in the `GROUP BY` clause.\n   - *Note* also the `AS frequency` which 'renames' the column returned by the query; it's the same concept as the `import x as y` in Python.\n5. `GROUP BY` is where the aggregation happens, and here we're asking DuckDB to take all of the rows selected (`WHERE price < 1000 AND number_of_rviews > 0`) and group them using the `room_type` and `property_type` fields.\n6. `ORDER BY` orders the returned records by the columns we specify, and they can be either `ASC`ending (the default) or `DSC`ending (descending).\n\nWhat you should also be noting here is that:\n\n- This query returns *very* quickly compared to the pandas equivalent.\n- We have been able to express our selection, grouping, and organising criteria very succinctly.\n\nTogether, these features mean that there can be quite substantial advantages to moving *some* of your workflow into a database or a database-like format such as Parquet.\n\n# Non-Spatial Joins\n\n## In Pandas\n\n::: {#7fa44dd8 .cell execution_count=12}\n``` {.python .cell-code}\nmsoa_nms = pd.read_csv( cache_data('https://houseofcommonslibrary.github.io/msoanames/MSOA-Names-1.20.csv', ddir) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/MSOA-Names-1.20.csv locally!\n```\n:::\n:::\n\n\n## In SQL\n\n# Spatial Joins\n\n## In Geopandas\n\n## In SQL\n\n# Worked Example\n\n## Load Geodata\n\nA lot of useful geo-data can be accessed from the [GeoPortal](https://geoportal.statistics.gov.uk/). And see also [my discussion](https://jreades.github.io/fsds/lectures/9.2-Linking_Spatial_Data.html#/think-it-through) on [lookup tables](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-november-2018-lookup-in-the-uk-2/about).\n\n::: {#e997ead7 .cell execution_count=13}\n``` {.python .cell-code}\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/Water.gpkg locally!\nFound data/geo/Boroughs.gpkg locally!\nFound data/geo/Greenspace.gpkg locally!\n```\n:::\n:::\n\n\n::: {#1399401e .cell execution_count=14}\n``` {.python .cell-code}\nmsoas = gpd.read_file( cache_data(spath+'Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip?raw=true', ddir) )\nmsoas.plot();\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip locally!\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-15-output-2.png){width=404 height=411}\n:::\n:::\n\n\n::: {#57bd58db .cell execution_count=15}\n``` {.python .cell-code}\nmsoas.sample(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=123}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>MSOA11CD</th>\n      <th>MSOA11NM</th>\n      <th>MSOA11NMW</th>\n      <th>BNG_E</th>\n      <th>BNG_N</th>\n      <th>LONG</th>\n      <th>LAT</th>\n      <th>Shape__Are</th>\n      <th>Shape__Len</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3178</th>\n      <td>3179</td>\n      <td>E02003267</td>\n      <td>Luton 010</td>\n      <td>Luton 010</td>\n      <td>507978</td>\n      <td>223189</td>\n      <td>-0.43212</td>\n      <td>51.89680</td>\n      <td>1.385458e+06</td>\n      <td>7693.666047</td>\n      <td>POLYGON ((507832.656 224006.453, 507875.469 22...</td>\n    </tr>\n    <tr>\n      <th>4922</th>\n      <td>4923</td>\n      <td>E02005032</td>\n      <td>Dartford 005</td>\n      <td>Dartford 005</td>\n      <td>555234</td>\n      <td>174323</td>\n      <td>0.23248</td>\n      <td>51.44662</td>\n      <td>1.559518e+06</td>\n      <td>6492.211612</td>\n      <td>POLYGON ((556173.375 174564.156, 556165.739 17...</td>\n    </tr>\n    <tr>\n      <th>6241</th>\n      <td>6242</td>\n      <td>E02006380</td>\n      <td>Reigate and Banstead 006</td>\n      <td>Reigate and Banstead 006</td>\n      <td>522438</td>\n      <td>154749</td>\n      <td>-0.24592</td>\n      <td>51.27873</td>\n      <td>1.396435e+07</td>\n      <td>16464.542311</td>\n      <td>POLYGON ((524058.423 156920.906, 523997.285 15...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Select London MSOAs\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nOne thing to remember here is that computers are _exact_. So if you say that the selection should only be of MSOAs _within_ London then you actually need to think about whether a shared border qualifies as 'within'. Watch [the lectures](https://jreades.github.io/fsds/sessions/week10.html#lectures) again if you're unsure, but that's why here we take this slightly clunk approach of buffering the London boundary _before_ doing the selection.\n\n:::\n\n### Union\n\nAs we don't have a boundary file for London, we can *generate* use using the `unary_union` operator (as we do here) or using the [dissolve()](https://geopandas.org/en/stable/docs/user_guide/aggregation_with_dissolve.html) approach. Consider the pros and cons of each approach in terms of performance, output format, and leigibility. \n\nSo here's approach 1, which is a function call (on which we call `plot`):\n\n::: {#dd1870fe .cell execution_count=16}\n``` {.python .cell-code}\nboros.dissolve().plot();\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-17-output-1.png){width=558 height=411}\n:::\n:::\n\n\nAnd here's approach 2, which is an *attribute* and returns a polygon (so no reason to call `plot`, but it's come back without the rest of the data frame!):\n\n::: {#8cd488e7 .cell execution_count=17}\n``` {.python .cell-code}\nboros.unary_union\n```\n\n::: {.cell-output .cell-output-display execution_count=125}\n![](Practical-08-Selecting_Data_files/figure-html/cell-18-output-1.svg){}\n:::\n:::\n\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nNotice how we're also demonstrating some additional ways of plotting 'on the fly' (without generating a data frame) as well as reminding you how to zoom in/out.\n\n:::\n\n::: {#76a683d6 .cell execution_count=18}\n``` {.python .cell-code}\nldn = gpd.GeoDataFrame(gpd.GeoSeries(data=boros.unary_union)).rename(columns={0:'geometry'}).set_geometry(\"geometry\")\nldn = ldn.set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5));\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-19-output-1.png){width=450 height=411}\n:::\n:::\n\n\n### Buffer\n\nIn order to ensure that we get all MSOAs _within_ London we will buffer the boundary by 250m. If _cover_ were easier to use then that option might be preferable.\n\n:::: {.qna}\n\n#### Question\n\n```python\nldn['buffered'] = ldn.geometry.???(???)\nldn = ldn.set_geometry('buffered').set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n#### Answer\n\n::: {#d4278282 .cell execution_count=19}\n``` {.python .cell-code}\nldn['buffered'] = ldn.geometry.buffer(250)\nldn = ldn.set_geometry('buffered').set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-20-output-1.png){width=450 height=411}\n:::\n:::\n\n\n::::\n\n### Spatial Join\n\nHere's our first spatial join. By default it will be an _inner_ join because we want to drop everything that doesn't line up between the two data sets (i.e. don't keep the thousands of *other* MSOAs).\n\n:::: {.qna}\n\n#### Question\n\n```python\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='???', how='inner')\nldn_msoas.head(2)\n```\n\n#### Answer\n\n::: {#f5c5b033 .cell execution_count=20}\n``` {.python .cell-code}\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='within', how='inner')\nldn_msoas.head(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=128}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>MSOA11CD</th>\n      <th>MSOA11NM</th>\n      <th>MSOA11NMW</th>\n      <th>BNG_E</th>\n      <th>BNG_N</th>\n      <th>LONG</th>\n      <th>LAT</th>\n      <th>Shape__Are</th>\n      <th>Shape__Len</th>\n      <th>geometry_left</th>\n      <th>index_right</th>\n      <th>geometry_right</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>E02000001</td>\n      <td>City of London 001</td>\n      <td>City of London 001</td>\n      <td>532384</td>\n      <td>181355</td>\n      <td>-0.093490</td>\n      <td>51.51561</td>\n      <td>2.906361e+06</td>\n      <td>8936.818478</td>\n      <td>POLYGON ((532135.138 182198.131, 532158.250 18...</td>\n      <td>0</td>\n      <td>POLYGON ((528150.200 159979.200, 528100.900 16...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>E02000002</td>\n      <td>Barking and Dagenham 001</td>\n      <td>Barking and Dagenham 001</td>\n      <td>548267</td>\n      <td>189685</td>\n      <td>0.138756</td>\n      <td>51.58652</td>\n      <td>2.166163e+06</td>\n      <td>8150.405928</td>\n      <td>POLYGON ((548881.563 190845.265, 548881.125 19...</td>\n      <td>0</td>\n      <td>POLYGON ((528150.200 159979.200, 528100.900 16...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\n### Plot\n\n::: {#c73e9eae .cell execution_count=21}\n``` {.python .cell-code}\nldn_msoas.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-22-output-1.png){width=579 height=425}\n:::\n:::\n\n\n:::: {.qna}\n\n#### Question\n\nHmmmm, not quite what you were expecting? See if you can figure out from the list of columns and the documentation for `set_geometry` what is going wrong?\n\n#### Answer\n\n::: {#8720b61d .cell execution_count=22}\n``` {.python .cell-code}\nax = ldn_msoas.set_geometry('geometry_left').plot(linewidth=2, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-23-output-1.png){width=450 height=411}\n:::\n:::\n\n\n::::\n\n::: {#f382f35d .cell execution_count=23}\n``` {.python .cell-code}\nldn_msoas = ldn_msoas.rename(columns={'geometry_left':'geometry'}).set_geometry('geometry')\n```\n:::\n\n\n::: {#03755e9f .cell execution_count=24}\n``` {.python .cell-code}\nldn_msoas.drop(columns='geometry_right', inplace=True)\n```\n:::\n\n\nWe no longer really need to keep the full MSOA data set hanging about.\n\n::: {#7a4f1752 .cell execution_count=25}\n``` {.python .cell-code}\ntry:\n    del(msoas)\nexcept NameError:\n    print(\"msoas already deleted.\")\n```\n:::\n\n\n:::: {.qna}\n\n### Question\n\n- Can you explain *why* the outputs of the `dissolve` and `unary_union` *look* differnet? And use that as the basis for explaining why they *are* different?\n\n> \n\n- How do you know that the units for the buffering operation are metres? 250 could be *anything* right?\n\n> \n\n- Why do we need to buffer the London geometry *before* performing the *within* spatial join?\n\n> \n\n#### Answer\n\n- Can you explain *why* the outputs of the `dissolve` and `unary_union` *look* differnet? And use that as the basis for explaining why they *are* different?\n\n> Dissolve is a method call that returns a new GeoDataFrame, while unary_union is a spatial operation that returns a primitive geometry. So we could capture the output of dissolve and just r\nename the columns, but from unary_union we need to write the primitive *into* a new GeoDataFrame as a geometry column.\n\n- How do you know that the units for the buffering operation are metres? 250 could be *anything* right?\n\n> It depends on the CRS! EPSG:27700 uses metres so the buffer operation is in metres.\n\n- Why do we need to buffer the London geometry *before* performing the *within* spatial join?\n\n> Because there may be small precision issues such that the MSOA borders cross the 'border' of the Thames and therefore no longer fall entirely within the London geometry. Try it yourself by\n changing the buffer amount!\n\n::::\n\n## Append Names\n\nWe don't actually make use of these in this session, but *both* operations could be relevant to your final reports:\n\n1. The Borough > Subregion mapping could help you to group your data into larger sets so that your resulst become more reobust. it also connects us to long-run patterns of socio-economic development in London.\n2. The MSOA Names data set gives you something that you could use to label one or more 'neighbourhoods' on a map with names that are *relevant*. So rather than talking about \"As you can see, Sutton 003, is...\", you can write \"The Wrythe neighbourhood [or area] of Sutton is significantly different from the surrounding areas...\"\n\nThey also usefully test your understanding of regular expressions and a few other aspects covered in previous weeks.\n\n### Replace\n\nYou've done this before: notice that the MSOA Name _contains_ the Borough name **with a space and some digits at the end**. Use a regex (in `str.replace()`) to extract the LA name from the MSOA name. See if you do this *without* having to find your previous answer!\n\n:::: {.qna}\n\n#### Question\n\n```python\nldn_msoas['Borough'] = ldn_msoas.MSOA11NM.str.replace(r'???','',regex=True)\n\n# Just check results look plausible; you should have:\n# - 33 boroughs\n# - A df shape of 983 x 13\nprint(ldn_msoas.Borough.unique())\nprint(f\"There are {len(ldn_msoas.Borough.unique())} boroughs.\")\nprint(f\"Overall shape of data frame is {' x '.join([str(x) for x in ldn_msoas.shape])}\")\n```\n\n#### Answer\n\n::: {#ef2057da .cell execution_count=26}\n``` {.python .cell-code}\nldn_msoas['Borough'] = ldn_msoas.MSOA11NM.str.replace(r' \\d+$','',regex=True)\n\n# Just check results look plausible; you should have:\n# - 33 boroughs\n# - A df shape of 983 x 13\nprint(ldn_msoas.Borough.unique())\nprint(f\"There are {len(ldn_msoas.Borough.unique())} boroughs.\")\nprint(f\"Overall shape of data frame is {' x '.join([str(x) for x in ldn_msoas.shape])}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['City of London' 'Barking and Dagenham' 'Barnet' 'Bexley' 'Brent'\n 'Bromley' 'Camden' 'Croydon' 'Ealing' 'Enfield' 'Greenwich' 'Hackney'\n 'Hammersmith and Fulham' 'Haringey' 'Harrow' 'Havering' 'Hillingdon'\n 'Hounslow' 'Islington' 'Kensington and Chelsea' 'Kingston upon Thames'\n 'Lambeth' 'Lewisham' 'Merton' 'Newham' 'Redbridge' 'Richmond upon Thames'\n 'Southwark' 'Sutton' 'Tower Hamlets' 'Waltham Forest' 'Wandsworth'\n 'Westminster']\nThere are 33 boroughs.\nOverall shape of data frame is 983 x 13\n```\n:::\n:::\n\n\n::::\n\n### Merge\n\nThe House of Commons Library provides a [MSOA Names](https://houseofcommonslibrary.github.io/msoanames/) data set that contains locally-relevant names applied to MSOAs. These seek to connect the Census geography (OA > LSOA > MSOA > LA) to a loosely-defined 'neighbourhood'.\n\n::: {#ec861f1a .cell execution_count=27}\n``` {.python .cell-code}\nmsoa_nms = pd.read_csv( cache_data('https://houseofcommonslibrary.github.io/msoanames/MSOA-Names-1.20.csv', ddir) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/MSOA-Names-1.20.csv locally!\n```\n:::\n:::\n\n\n::: {#9919ffed .cell execution_count=28}\n``` {.python .cell-code}\nprint(msoa_nms.columns.values)\nmsoa_nms.sample(3, random_state=42)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['msoa11cd' 'msoa11nm' 'msoa11nmw' 'msoa11hclnm' 'msoa11hclnmw' 'Laname']\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=136}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>msoa11cd</th>\n      <th>msoa11nm</th>\n      <th>msoa11nmw</th>\n      <th>msoa11hclnm</th>\n      <th>msoa11hclnmw</th>\n      <th>Laname</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4512</th>\n      <td>E02004616</td>\n      <td>Cotswold 002</td>\n      <td>Cotswold 002</td>\n      <td>Moreton &amp; Stow-on-the-Wold</td>\n      <td>NaN</td>\n      <td>Cotswold</td>\n    </tr>\n    <tr>\n      <th>4660</th>\n      <td>E02004768</td>\n      <td>Havant 007</td>\n      <td>Havant 007</td>\n      <td>Waterlooville East</td>\n      <td>NaN</td>\n      <td>Havant</td>\n    </tr>\n    <tr>\n      <th>1038</th>\n      <td>E02001074</td>\n      <td>Manchester 030</td>\n      <td>Manchester 030</td>\n      <td>Fallowfield West &amp; Whalley Range South</td>\n      <td>NaN</td>\n      <td>Manchester</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow that you've loaded the `msoa_nms` data you need to merge it with our `ldn_msoas`. You will need to deal with the fact that the left and right fields have different names and may also want to think about the `how` of the merge. In *this* case, the result of the merge *should* be a GeoDataFrame, but **this is not always guaranteed** so you may want to double-check or run multiple tests before assuming that you'll get back a geographically aware object.\n\n:::: {.qna}\n\n#### Question\n\n```python\nmsoas = pd.merge(ldn_msoas, msoa_nms, left_on='??', right_on='??', how='??')\nprint(f\"MSOAs shape is {' x '.join([str(x) for x in msoas.shape])}.\")\nprint(f\"Resulting class is a {type(msoas).__name__}.\") # You should check this -- result isn't always be a GeoDataFrame\nmsoas.sample(3, random_state=42)[['OBJECTID','MSOA11CD','MSOA11NM','msoa11hclnm']]\n```\n\n#### Answer\n\n::: {#c32e268f .cell execution_count=29}\n``` {.python .cell-code}\nmsoas = pd.merge(ldn_msoas, msoa_nms, left_on='MSOA11CD', right_on='msoa11cd', how='inner')\nprint(f\"MSOAs shape is {' x '.join([str(x) for x in msoas.shape])}.\")\nprint(f\"Resulting class is a {type(msoas).__name__}.\") # You should check this -- result isn't always be a GeoDataFrame\nmsoas.sample(3, random_state=42)[['OBJECTID','MSOA11CD','MSOA11NM','msoa11hclnm']]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMSOAs shape is 983 x 19.\nResulting class is a GeoDataFrame.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=137}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>MSOA11CD</th>\n      <th>MSOA11NM</th>\n      <th>msoa11hclnm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>810</th>\n      <td>811</td>\n      <td>E02000841</td>\n      <td>Sutton 002</td>\n      <td>St Helier South</td>\n    </tr>\n    <tr>\n      <th>801</th>\n      <td>802</td>\n      <td>E02000832</td>\n      <td>Southwark 026</td>\n      <td>Nunhead North</td>\n    </tr>\n    <tr>\n      <th>813</th>\n      <td>814</td>\n      <td>E02000844</td>\n      <td>Sutton 005</td>\n      <td>The Wrythe</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\nYour result should be:\n\n::: {#0122ec76 .cell execution_count=30}\n\n::: {.cell-output .cell-output-display execution_count=138}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>MSOA11CD</th>\n      <th>MSOA11NM</th>\n      <th>msoa11hclnm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>810</th>\n      <td>811</td>\n      <td>E02000841</td>\n      <td>Sutton 002</td>\n      <td>St Helier South</td>\n    </tr>\n    <tr>\n      <th>801</th>\n      <td>802</td>\n      <td>E02000832</td>\n      <td>Southwark 026</td>\n      <td>Nunhead North</td>\n    </tr>\n    <tr>\n      <th>813</th>\n      <td>814</td>\n      <td>E02000844</td>\n      <td>Sutton 005</td>\n      <td>The Wrythe</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Map\n\nSet up a `mapping` dict here so that you can apply it as part of the `groupby` operation below (you should have 33 keys when done):\n\n::: {#4e0fb637 .cell execution_count=31}\n``` {.python .cell-code}\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet','City of London']:\n    mapping[b]='Outer West and North West'\nprint(len(mapping.keys()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n33\n```\n:::\n:::\n\n\n:::: {.qna}\n\n#### Question\n\n```python\nmsoas['Subregion'] = msoas.Borough.map(???)\n```\n\n#### Answer\n\n::: {#587aee55 .cell execution_count=32}\n``` {.python .cell-code}\nmsoas['Subregion'] = msoas.Borough.map(mapping)\n```\n:::\n\n\n::::\n\n### Tidy Up\n\n::: {#30246f1f .cell execution_count=33}\n``` {.python .cell-code}\nmsoas.columns.to_list()\n```\n\n::: {.cell-output .cell-output-display execution_count=141}\n```\n['OBJECTID',\n 'MSOA11CD',\n 'MSOA11NM',\n 'MSOA11NMW',\n 'BNG_E',\n 'BNG_N',\n 'LONG',\n 'LAT',\n 'Shape__Are',\n 'Shape__Len',\n 'geometry',\n 'index_right',\n 'Borough',\n 'msoa11cd',\n 'msoa11nm',\n 'msoa11nmw',\n 'msoa11hclnm',\n 'msoa11hclnmw',\n 'Laname',\n 'Subregion']\n```\n:::\n:::\n\n\n::: {#1866eb8a .cell execution_count=34}\n``` {.python .cell-code}\nto_drop = ['MSOA11CD','MSOA11NM','MSOA11NMW','LONG','LAT','Shape__Are','Shape__Len','index_right',\n           'Laname','msoa11hclnmw','msoa11nmw']\nmsoas.drop(columns=to_drop, inplace=True)\nprint(msoas.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(983, 9)\n```\n:::\n:::\n\n\n### And Save\n\n::: {#d424e618 .cell execution_count=35}\n``` {.python .cell-code}\nmsoas.to_parquet(os.path.join('data','geo','London_MSOA_Names.geoparquet'))\n```\n:::\n\n\n## Load InsideAirbnb Data\n\n::: {#82623a69 .cell execution_count=36}\n``` {.python .cell-code}\nlistings = gpd.read_parquet( cache_data(f'{host}/{path}/{ymd}-listings.geoparquet', ddir) )\nlistings = listings.to_crs(epsg=27700)\nprint(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/2023-09-06-listings.geoparquet locally!\nData frame is 85,134 x 31\n```\n:::\n:::\n\n\n::: {#58f6cd8a .cell execution_count=37}\n``` {.python .cell-code}\nlistings = listings.to_crs('epsg:27700')\n```\n:::\n\n\n### Spatial Join\n\nAssociate LA (Local Authority) names to the listings using a spatial join, but **notice** the `how` here:\n\n:::: {.qna}\n\n#### Question\n\n```python\ngdf_la = gpd.sjoin(listings, ???, predicate='???', how='left')\nprint(gdf_la.columns.to_list())\n```\n\n#### Answer\n\n::: {#be7148e3 .cell execution_count=38}\n``` {.python .cell-code}\ngdf_la = gpd.sjoin(listings, boros, predicate='within', how='left')\nprint(gdf_la.columns.to_list())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['listing_url', 'last_scraped', 'name', 'description', 'host_id', 'host_name', 'host_since', 'host_location', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'availability_365', 'number_of_reviews', 'first_review', 'last_review', 'review_scores_rating', 'reviews_per_month', 'geometry', 'index_right', 'NAME', 'GSS_CODE', 'HECTARES', 'NONLD_AREA', 'ONS_INNER']\n```\n:::\n:::\n\n\n::::\n\n### Tidy Up\n\n::: {#7395c940 .cell execution_count=39}\n``` {.python .cell-code}\ngdf_la.drop(columns=['index_right','HECTARES','NONLD_AREA','ONS_INNER'], inplace=True)\n```\n:::\n\n\nYou'll need to look closely to check that the `value_counts` output squares with your expectations. If you don't get `33` then there's an issue:\n\n::: {#fc4e2c06 .cell execution_count=40}\n``` {.python .cell-code}\nif len(gdf_la.NAME.unique()) == 33:\n    print(\"All good...\")\nelse:\n    print(\"Need to run the next section of code...\")\n    print(f\"Now there are... {len(gdf_la.NAME.unique())} boroughs?\")\n    gdf_la.NAME.value_counts(dropna=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNeed to run the next section of code...\nNow there are... 34 boroughs?\n```\n:::\n:::\n\n\n### Find Problematic Listings\n\nIf you were told that you need to run the next sectin of code then see if you can work out what happened...\n\n```python\ntry:\n    print(gdf_la[gdf_la.NAME.isna()].sample(2)[['name', 'NAME']])\n    ax = gdf_la[gdf_la.NAME.isna()].plot(figsize=(9,6), markersize=5, alpha=0.5)\n    boros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\nexcept ValueError as e:\n   pass\n```\n\nIn short: in some cases there may be records that fall outside of London because of Airbnb's shuffling approach:\n\n::: {#12193c89 .cell execution_count=41}\n``` {.python .cell-code}\ngdf_la.drop(index=gdf_la[gdf_la.NAME.isna()].index, axis=1, inplace=True)\nprint(f\"Data frame is {gdf_la.shape[0]:,} x {gdf_la.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData frame is 84,615 x 33\n```\n:::\n:::\n\n\nYou should now have `{python} f\"{gdf_la.shape[0]:,}\" ` records.\n\n### Check\n\n::: {#4bfdd035 .cell execution_count=42}\n``` {.python .cell-code}\nax = gdf_la.plot(column='NAME', markersize=0.5, alpha=0.5, figsize=(9,7))\nboros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-43-output-1.png){width=749 height=559}\n:::\n:::\n\n\n### Save\n\n::: {#bfb3ebe2 .cell execution_count=43}\n``` {.python .cell-code}\ngdf_la.to_parquet(os.path.join('data','geo','Listings_with_LA.geoparquet'))\n```\n:::\n\n\n:::: {.qna}\n\n#### Question\n\n- Do you understand the difference between `how='inner'` and `how='left'`? \n\n> \n\n#### Answer\n\n- Do you understand the difference between `how='inner'` and `how='left'`?\n\n> Left joins preserve *all* records on the left table *regardless* of whether they match something in the right table. In this case, because it's a *spatial* join we keep the listings *regardless* of whether they fall within the London boroughs. Inner joins preserve *only* the records that match between left and right, so in this case if you did a spatial inner join you'd only get the records that fall within a London borough. Right joins do about what you'd expect (not very helpful in this example). Outer joins preserve everything in both tables which, in a spatial context, would probably be a little hard to interpret.\n\n::::\n\n## Create LA Data\n\n### Select LA\n\nSelect a LA that is relevant to _you_ to explore further...\n\n::: {#fbc708a3 .cell execution_count=44}\n``` {.python .cell-code}\nLA = 'Waltham Forest'\n```\n:::\n\n\n### Spatial Join\n\nThe first thing we want to do is join MSOA identifiers to each listing. In both cases we want to constrain the data to only be for 'our' LA of interest: \n\n::: {#dee31312 .cell execution_count=45}\n``` {.python .cell-code}\nmsoadf  = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            msoas[msoas.Borough==LA], predicate='within')\n```\n:::\n\n\n### Aggregate\n\nNow aggregate the data by MSOA, deriving median price and a count of the listings:\n\n::: {#83a05496 .cell execution_count=46}\n``` {.python .cell-code}\nmsoagrdf = msoadf.groupby('msoa11nm').agg({'price':['median','count']}).reset_index()\n```\n:::\n\n\n::: {#43a1675b .cell execution_count=47}\n``` {.python .cell-code}\n#msoagrdf.sample(3, random_state=42, replace=True)\n```\n:::\n\n\nYou should get something like the below (if you're using Waltham Forest):\n\n\n\n### Resolve Columns\n\nWhich level value is easier to use? 0? or 1?\n\n::: {#0c691f02 .cell execution_count=49}\n``` {.python .cell-code}\nmsoagrdf.columns = msoagrdf.columns.get_level_values(1)\nmsoagrdf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=157}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>median</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Waltham Forest 001</td>\n      <td>97.0</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Waltham Forest 002</td>\n      <td>56.0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Waltham Forest 003</td>\n      <td>82.0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Waltham Forest 004</td>\n      <td>44.0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Waltham Forest 005</td>\n      <td>50.0</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFix the missing column name:\n\n::: {#3f3d6559 .cell execution_count=50}\n``` {.python .cell-code}\nmsoagrdf.rename(columns={'':'msoa11nm', 'count':'listings'}, inplace=True)\nmsoagrdf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=158}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>msoa11nm</th>\n      <th>median</th>\n      <th>listings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Waltham Forest 001</td>\n      <td>97.0</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Waltham Forest 002</td>\n      <td>56.0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Waltham Forest 003</td>\n      <td>82.0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Waltham Forest 004</td>\n      <td>44.0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Waltham Forest 005</td>\n      <td>50.0</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Join (Again)\n\nHere we see the **difference between merge and join**. You'll notice that `join` operates by taking one data frame as the implicit '*left*' table (the one which *calls* join) while the one that is passed to the join function is, implicitly, the '*right*' table. Join operates only using indexes, so you'll need to insert the code to specify the same index on both data frames, but this can be done **on-the-fly** as part of the joining operation:\n\n::: {#08c2a058 .cell execution_count=51}\n``` {.python .cell-code}\nmsoa_gdf = msoagrdf.set_index('msoa11nm').join(\n                msoas[msoas.Borough==LA].set_index('msoa11nm'), \n                rsuffix='_r')\nmsoa_gdf.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=159}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>median</th>\n      <th>listings</th>\n      <th>OBJECTID</th>\n      <th>BNG_E</th>\n      <th>BNG_N</th>\n      <th>geometry</th>\n      <th>Borough</th>\n      <th>msoa11cd</th>\n      <th>msoa11hclnm</th>\n      <th>Subregion</th>\n    </tr>\n    <tr>\n      <th>msoa11nm</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Waltham Forest 001</th>\n      <td>97.0</td>\n      <td>21</td>\n      <td>863</td>\n      <td>537936</td>\n      <td>194880</td>\n      <td>POLYGON ((537919.442 195742.428, 538051.250 19...</td>\n      <td>Waltham Forest</td>\n      <td>E02000895</td>\n      <td>Chingford Green West</td>\n      <td>Outer East and North East</td>\n    </tr>\n    <tr>\n      <th>Waltham Forest 002</th>\n      <td>56.0</td>\n      <td>11</td>\n      <td>864</td>\n      <td>539350</td>\n      <td>194516</td>\n      <td>POLYGON ((539172.688 195540.000, 539696.813 19...</td>\n      <td>Waltham Forest</td>\n      <td>E02000896</td>\n      <td>Chingford Green East</td>\n      <td>Outer East and North East</td>\n    </tr>\n    <tr>\n      <th>Waltham Forest 003</th>\n      <td>82.0</td>\n      <td>9</td>\n      <td>865</td>\n      <td>539355</td>\n      <td>193522</td>\n      <td>POLYGON ((538862.624 194017.438, 539001.125 19...</td>\n      <td>Waltham Forest</td>\n      <td>E02000897</td>\n      <td>Friday Hill</td>\n      <td>Outer East and North East</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Resolve Geodata\n\nYou need to add a command in order to help python recognise that this should be a GeoDataFrame:\n\n::: {#4bce23c7 .cell execution_count=52}\n``` {.python .cell-code}\nmsoa_gdf = msoa_gdf.set_geometry('geometry')\n```\n:::\n\n\n```python\nmsoa_gdf.plot(column='median', legend=True, figsize=(8,8));\n```\n\nYou should get something like:\n\n::: {#ffbbda7b .cell execution_count=53}\n\n::: {.cell-output .cell-output-display}\n![](Practical-08-Selecting_Data_files/figure-html/cell-54-output-1.png){width=500 height=633}\n:::\n:::\n\n\n### Save\n\nJust so that we can pick up here without having to re-run all the preceding cells.\n\n::: {#88ac655e .cell execution_count=54}\n``` {.python .cell-code}\nmsoa_gdf.to_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n```\n:::\n\n\n:::: {.qna}\n\n### Question\n\n- Do you understand the differences between `pd.merge` and `df.join`? and `gpd.sjoin`?\n\n> \n\n- Do you understand why it may be necessary to `set_geometry` in some cases?\n\n> \n\n#### Answer\n\n- Do you understand the differences between `pd.merge` and `df.join`? and `gpd.sjoin`?\n\n> Obviously one of these three is spatial in nature, but beyond that there is a *lot* of overlap and great deal depends on the logic or the linkage. `join` is relatively limited in functionality but is fairly clear about the relationships: if the indexes don't match it doesn't produce anything. `merge` is actually (functionally, at least) more similar to `sjoin` than `sjoin` is to `join` -- this is slightly confusing but can probably be traced back to Pandas' origins in panel data research whereas Geopandas' is more technically correct in its terminology because of the influence of GIS.\n\n- Do you understand why it may be necessary to `set_geometry` in some cases?\n\n> Joins are DataFrame functionality and so are unaware of GeoDataFrames (so the return type devolves to a DataFrame), whereas merges seem to preserve the child class type and mean that a GeoDataFrame is returned even thought this is *also* a Pandas utility function.\n\n::::\n\n",
    "supporting": [
      "Practical-08-Selecting_Data_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}