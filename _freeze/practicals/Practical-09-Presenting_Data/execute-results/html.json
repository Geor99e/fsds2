{
  "hash": "b10e6dedc1d6e4de56f89488e9d0a1e9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Practical 10: Visualisation\"\nsubtitle: \"Linking & Visualising Data\"\njupyter: python3\nfilters:\n  - qna\n  - quarto\n---\n\n\n| Complete | Part 1: Foundations | Part 2: Data | Part 3: Analysis |     |\n| :------- | :------------------ | :----------- | :--------------- | --: |\n| 90% | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9619;&#9619;&#9619;&#9619;&#9617; | 10/10\n\n::: {.callout-warning}\n\n    **&#9888; Important**: This practical focusses on two key bits of _implementation_: visualisation and data linkage! You will have seen quite a bit of each of these across the preceding three to four weeks, but they were picked up in an _ad-hoc_ way, here we try to systematise things a bit.\n\n:::\n\n::: {.callout-note}\n\n    **&#128279; Connections**: Here we're trying to tidy up the loose ends. You've already worked with basic data visualisations in Seaborn and Matplotlib (including (geo)panda's `plot` function), but we want you to have a better sense of how that _works_ as part of a coherent -- if altogether rather complex and overwhelming -- approach to managing a data visualisation. You've also already seen examples of joins and spatial joins before but, again, we just want to review them more formally now.\n:::\n\n# Preamble\n\n```python\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n```\n\n```python\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -> str:\n    \"\"\"Downloads and caches a remote file locally.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dest : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[-1] # Extract the filename\n    dfn = os.path.join(dest,fn) #Â Destination filename\n    \n    # Check if dest+filename does *not* exist -- \n    # that would mean we have to download it!\n    if not os.path.isfile(dfn) or os.path.getsize(dfn) < 1:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        # Convert the path back into a list (without)\n        # the filename -- we need to check that directories\n        # exist first.\n        path = os.path.split(dest)\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how\n        # to use the 'splat' operator! os.makedirs creates missing\n        # directories in a path automatically.\n        if len(path) >= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        # Download and write the file\n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(response.content)\n            \n        print('Done downloading...')\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n    return dfn\n```\n\n# Spatial Joins (Recap)\n\n## Load Geodata\n\nA lot of useful geo-data can be accessed from the [GeoPortal](https://geoportal.statistics.gov.uk/). And see also [my discussion](https://jreades.github.io/fsds/lectures/9.2-Linking_Spatial_Data.html#/think-it-through) on [lookup tables](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-november-2018-lookup-in-the-uk-2/about).\n\n```python\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n```\n\n```python\nmsoas = gpd.read_file( cache_data(spath+'Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip?raw=true', ddir) )\nmsoas.plot();\n```\n\n```python\nmsoas.sample(3)\n```\n\n## Select London MSOAs\n\n::: {.callout-note}\n\n    **&#128279; Connections**: One thing to remember here is that computers are _exact_. So if you say that the selection should only be of MSOAs _within_ London then you actually need to think about whether a shared border qualifies as 'within'. Watch [the lectures](https://jreades.github.io/fsds/sessions/week10.html#lectures) again if you're unsure, but that's why here we take this slightly clunk approach of buffering the London boundary _before_ doing the selection.\n:::\n\n###. Union\n\nAs we don't have a boundary file for London, we can *generate* use using the `unary_union` operator (as we do here) or using the [dissolve()](https://geopandas.org/en/stable/docs/user_guide/aggregation_with_dissolve.html) approach. Consider the pros and cons of each approach in terms of performance, output format, and leigibility. \n\nSo here's approach 1, which is a function call (on which we call `plot`):\n\n```python\nboros.dissolve().plot();\n```\n\nAnd here's approach 2, which is an *attribute* and returns a polygon (so no reason to call `plot`, but it's come back without the rest of the data frame!):\n\n```python\nboros.unary_union\n```\n\n::: {.callout-note}\n\n    **&#128279; Connections**: Notice how we're also demonstrating some additional ways of plotting 'on the fly' (without generating a data frame) as well as reminding you how to zoom in/out.\n:::\n\n```python\nldn = gpd.GeoDataFrame(gpd.GeoSeries(data=boros.unary_union)).rename(columns={0:'geometry'}).set_geometry(\"geometry\")\nldn = ldn.set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5));\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n###. Buffer\n\nIn order to ensure that we get all MSOAs _within_ London we will buffer the boundary by 250m. If _cover_ were easier to use then that option might be preferable.\n\n```python\nldn['buffered'] = ldn.geometry.???(???)\nldn = ldn.set_geometry('buffered').set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n###. Spatial Join\n\nHere's our first spatial join. By default it will be an _inner_ join because we want to drop everything that doesn't line up between the two data sets (i.e. don't keep the thousands of *other* MSOAs).\n\n```python\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='???', how='inner')\nldn_msoas.head(2)\n```\n\n###. Plot\n\n```python\nldn_msoas.plot()\n```\n\nHmmmm, not quite what you were expecting? See if you can figure out from the list of columns and the documentation for `set_geometry` what is going wrong?\n\n```python\nax = ldn_msoas.set_geometry('geometry_left').plot(linewidth=2, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n```\n\n```python\nldn_msoas = ldn_msoas.rename(columns={'geometry_left':'geometry'}).set_geometry('geometry')\n```\n\n```python\nldn_msoas.drop(columns='geometry_right', inplace=True)\n```\n\nWe no longer really need to keep the full MSOA data set hanging about.\n\n```python\ntry:\n    del(msoas)\nexcept NameError:\n    print(\"msoas already deleted.\")\n```\n\n###. Questions\n\n- Can you explain *why* the outputs of the `dissolve` and `unary_union` *look* differnet? And use that as the basis for explaining why they *are* different?\n\n> \n\n- How do you know that the units for the buffering operation are metres? 250 could be *anything* right?\n\n> \n\n- Why do we need to buffer the London geometry *before* performing the *within* spatial join?\n\n> \n\n## Append Names\n\nWe don't actually make use of these in this session, but *both* operations could be relevant to your final reports:\n\n1. The Borough > Subregion mapping could help you to group your data into larger sets so that your resulst become more reobust. it also connects us to long-run patterns of socio-economic development in London.\n2. The MSOA Names data set gives you something that you could use to label one or more 'neighbourhoods' on a map with names that are *relevant*. So rather than talking about \"As you can see, Sutton 003, is...\", you can write \"The Wrythe neighbourhood [or area] of Sutton is significantly different from the surrounding areas...\"\n\nThey also usefully test your understanding of regular expressions and a few other aspects covered in previous weeks.\n\n###. Replace\n\nYou've done this before: notice that the MSOA Name _contains_ the Borough name **with a space and some digits at the end**. Use a regex (in `str.replace()`) to extract the LA name from the MSOA name. See if you do this *without* having to find your previous answer!\n\n```python\nldn_msoas['Borough'] = ldn_msoas.MSOA11NM.str.replace(r'???','',regex=True)\n\n# Just check results look plausible; you should have:\n# - 33 boroughs\n# - A df shape of 983 x 13\nprint(ldn_msoas.Borough.unique())\nprint(f\"There are {len(ldn_msoas.Borough.unique())} boroughs.\")\nprint(f\"Overall shape of data frame is {' x '.join([str(x) for x in ldn_msoas.shape])}\")\n```\n\n###. Merge\n\nThe House of Commons Library provides a [MSOA Names](https://houseofcommonslibrary.github.io/msoanames/) data set that contains locally-relevant names applied to MSOAs. These seek to connect the Census geography (OA > LSOA > MSOA > LA) to a loosely-defined 'neighbourhood'.\n\n```python\nmsoa_nms = pd.read_csv( cache_data('https://houseofcommonslibrary.github.io/msoanames/MSOA-Names-1.20.csv', ddir) )\n```\n\n```python\nprint(msoa_nms.columns.values)\nmsoa_nms.sample(3, random_state=42)\n```\n\nNow that you've loaded the `msoa_nms` data you need to merge it with our `ldn_msoas`. You will need to deal with the fact that the left and right fields have different names and may also want to think about the `how` of the merge. In *this* case, the result of the merge *should* be a GeoDataFrame, but **this is not always guaranteed** so you may want to double-check or run multiple tests before assuming that you'll get back a geographically aware object.\n\n```python\nmsoas = pd.merge(ldn_msoas, msoa_nms, left_on='MSOA11CD', right_on='msoa11cd', how='inner')\nprint(f\"MSOAs shape is {' x '.join([str(x) for x in msoas.shape])}.\")\nprint(f\"Resulting class is a {type(msoas).__name__}.\") # You should check this -- result isn't always be a GeoDataFrame\nmsoas.sample(3, random_state=42)[['OBJECTID','MSOA11CD','MSOA11NM','msoa11hclnm']]\n```\n\nYour result should be:\n\n|    | OBJECTID | MSOA11CD | MSOA11NM | msoa11hclnm |\n| -: | -------: | -------: | :------- | :---------- |\n| **810** | 811 | E02000841 | Sutton 002 | St Helier South |\n| **801** | 802 | E02000832 | Southwark 026 | Nunhead North |\n| **813** | 814 | E02000844 | Sutton 005 | The Wrythe |\n\n###. Map\n\nSet up a `mapping` dict here so that you can apply it as part of the `groupby` operation below (you should have 33 keys when done):\n\n```python\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet','City of London']:\n    mapping[b]='Outer West and North West'\nprint(len(mapping.keys()))\n```\n\n```python\nmsoas['Subregion'] = msoas.Borough.map(???)\n```\n\n###. Tidy Up\n\n```python\nmsoas.columns.to_list()\n```\n\n```python\nto_drop = ['MSOA11CD','MSOA11NM','MSOA11NMW','LONG','LAT','Shape__Are','Shape__Len','index_right',\n           'Laname','msoa11hclnmw','msoa11nmw']\nmsoas.drop(columns=to_drop, inplace=True)\nprint(msoas.shape)\n```\n\n###. And Save\n\n```python\nmsoas.to_parquet(os.path.join('data','geo','London_MSOA_Names.geoparquet'))\n```\n\n## Load InsideAirbnb Data\n\n```python\nhost = 'http://orca.casa.ucl.ac.uk'\npath = '~jreades/data'\nlistings = gpd.read_parquet( cache_data(f'{host}/{path}/2023-09-06-listings.geoparquet', ddir) )\nlistings = listings.to_crs(epsg=27700)\nprint(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")\n```\n\n```python\nlistings = listings.to_crs('epsg:27700')\n```\n\n###. Spatial Join\n\nAssociate LA (Local Authority) names to the listings using a spatial join, but **notice** the `how` here:\n\n```python\ngdf_la = gpd.sjoin(listings, ???, predicate='???', how='left')\n```\n\n```python\nprint(gdf_la.columns.to_list())\n```\n\n###. Tidy Up\n\n```python\ngdf_la.drop(columns=['index_right','HECTARES','NONLD_AREA','ONS_INNER'], inplace=True)\n```\n\nYou'll need to look closely to see what's wrong in the `value_counts` output, but here's a clue:\n\n```python\nprint(f\"Now there are... {len(gdf_la.NAME.unique())} boroughs?\")\n```\n\n```python\ngdf_la.NAME.value_counts(dropna=False)\n```\n\n```python\ngdf_la.columns\n```\n\n```python\ngdf_la[gdf_la.NAME.isna()].sample(2)[['name', 'NAME']]\n```\n\n###. Find Problematic Listings\n\nLet's map them and see if you can work out what happened...\n\n```python\nax = gdf_la[gdf_la.NAME.isna()].plot(figsize=(10,7), markersize=5, alpha=0.5)\nboros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\n```\n\n<img src=\"https://github.com/jreades/fsds/blob/master/practicals/img/Unmatched_Listings.png?raw=true\" alt=\"Unmatched Listings\" style=\"width:50%\" />\n\n###. Drop Problematic Results\n\n```python\ngdf_la.drop(index=gdf_la[gdf_la.NAME.isna()].index, axis=1, inplace=True)\nprint(f\"Data frame is {gdf_la.shape[0]:,} x {gdf_la.shape[1]}\")\n```\n\nYou should now have 67,993 records.\n\n###. Check\n\n```python\nax = gdf_la.plot(column='NAME', markersize=0.5, alpha=0.5, figsize=(10,8))\nboros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\n```\n\n###. Save\n\n```python\ngdf_la.to_parquet(os.path.join('data','geo','Listings_with_LA.geoparquet'))\n```\n\n###. Questions\n\n- Do you understand the difference between `how='inner'` and `how='left'`? \n\n> \n\n## Create LA Data\n\n###. Select LA\n\nSelect a LA that is relevant to _you_ to explore further...\n\n```python\nLA = 'Waltham Forest'\n```\n\n###. Spatial Join\n\nThe first thing we want to do is join MSOA identifiers to each listing. In both cases we want to constrain the data to only be for 'our' LA of interest: \n\n```python\nmsoadf  = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            msoas[msoas.Borough==LA], predicate='within')\n```\n\n###. Aggregate\n\nNow aggregate the data by MSOA, deriving median price and a count of the listings:\n\n```python\nmsoagrdf = msoadf.groupby('msoa11nm').agg({'price':['median','count']}).reset_index()\n```\n\n```python\nmsoagrdf.sample(3, random_state=42)\n```\n\nYou should get something like the below (if you're using Waltham Forest):\n\n|   | msoa11nm |  | price |\n| -: | ------: | ----: | ----: |\n|   | **median** | **count** |\n| 9 | Waltham Forest 010 | 55.0 | 25 |\n| 25 | Waltham Forest 026 | 75.0 | 72 |\n| 8 | Waltham Forest 009 | 51.5 | 26 |\n\n###. Resolve Columns\n\nWhich level value is easier to use? 0? or 1?\n\n```python\nmsoagrdf.columns = msoagrdf.columns.get_level_values(1)\nmsoagrdf.head()\n```\n\nFix the missing column name:\n\n```python\nmsoagrdf.rename(columns={'':'msoa11nm', 'count':'listings'}, inplace=True)\nmsoagrdf.head()\n```\n\n###. Join (Again)\n\nHere we see the **difference between merge and join**. You'll notice that `join` operates by taking one data frame as the implicit '*left*' table (the one which *calls* join) while the one that is passed to the join function is, implicitly, the '*right*' table. Join operates only using indexes, so you'll need to insert the code to specify the same index on both data frames, but this can be done **on-the-fly** as part of the joining operation:\n\n```python\nmsoa_gdf = msoagrdf.set_index('msoa11nm').join(\n                msoas[msoas.Borough==LA].set_index('msoa11nm'), \n                rsuffix='_r')\nmsoa_gdf.head(3)\n```\n\n###. Resolve Geodata\n\nYou need to add a command in order to help python recognise that this should be a GeoDataFrame:\n\n```python\nmsoa_gdf = msoa_gdf.set_geometry('geometry')\n```\n\n```python\nmsoa_gdf.plot(column='median', legend=True, figsize=(8,8));\n```\n\n###. Save\n\nJust so that we can pick up here without having to re-run all the preceding cells.\n\n```python\nmsoa_gdf.to_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n```\n\n###. Questions\n\n- Do you understand the differences between `pd.merge` and `df.join`? and `gpd.sjoin`?\n\n> \n\n- Do you understand why it may be necessary to `set_geometry` in some cases?\n\n> \n\n# Using Maplotlib\n\n## Anatomy of a Figure\n\n::: {.callout-tip}\n\n    You might want to bookmark the 'Anatomy of a Figure' image so that you can easily find and refer to it in the future. This structure is why `matplotlib` is so much nastier than `ggplot`, but it does also give you greater _control_ over the output if you really dig into the guts of things.\n:::\n\n*One* of the reasons that Matplotlib is so much more complex than `ggplot` is that it can actually *do* a plot more than ggplot, including image manipulation, axis translation, and even 3D. You can get a sense of this by looking at [the tutorials](https://matplotlib.org/stable/tutorials/index.html) since the [Users guide](https://matplotlib.org/stable/users/index.html) can be a bit overwhelming.\n\nNevertheless, the [core components of all matplotlib figures](https://matplotlib.org/stable/gallery/showcase/anatomy.html) can be seen here:\n\n<img src=\"https://matplotlib.org/stable/_images/sphx_glr_anatomy_001.png\" alt=\"Anatomy of a Figure\" style=\"width:50%\" />\n\n## Finding Fonts\n\nI find matplotlib's use of fonts to be _profoundly_ weird. If you use `conda` and install directly on to the computer then you _might_ have access to all of your computer's fonts (though there are different *types* of fonts as well, not all of which will show up), but for most users it will be those that were installed into Docker. \n\n###. Using Fontconfig\n\n`fontconfig` is the base Linux utility for managing fonts. We can list font using `fc-list` and then a set of 'switches' determining the kind of information we want back. Since fontconfig doesn't exist on OSX or Windows, you'll need to do some more investigating and poking around to get these details on a `conda` install (I'll show an option further down)...\n\n::: {.callout-tip}\n\n    notice that below we're able to capture the output of an external application (called via the Terminal) with `fonts = ! ...`. This can be useful when something is easy to do on the command line but hard to do in Python.\n:::\n\nHere we ask fontconfig to format the output so that we only get the first part of the family name, and then we pipe (recall `|` sends output from one utility to another!) the output of that to `sort`, which sorts the output, and `uniq` which removes duplicates (which there will be because there are **bold**, *italic*, small-caps, etc. versions of each font). To make better sense of this you can always try playing around with all three steps in the output below!\n\n```python\nfonts = ! fc-list --format=\"%{family[0]}\\n\" | sort | uniq\nprint(fonts)\n```\n\nThis also pipes output from fonctconfig, but to the `grep` utility which checks each line for the character sequence `Liberation`. Now we're asking fontconfig to include `style` details which will relate to both weight (regular, bold, extra bold, light, etc.) and italic, bold, small caps, etc.\n\n```python\nfonts = ! fc-list : family style | grep \"Liberation\"\nprint(sorted(fonts))\n```\n\nYou can find more examples [here](https://www.geeksforgeeks.org/fc-list-command-in-linux-with-examples/), a more detailed set of instructions [here](https://www.freedesktop.org/software/fontconfig/fontconfig-user.html), and even information about (for example) [supported languages](https://thottingal.in/blog/2016/03/04/fontconfig-language-matching/) based on [RFC 3066](http://www.i18nguy.com/unicode/language-identifiers.html).\n\nHere are the languages supported by the Ubuntu Light font:\n\n```python\n! fc-list \"Ubuntu Light\" : lang\n```\n\nHere are the monospace fonts installed: \n\n```python\n! fc-list :spacing=mono : family | sort | uniq\n```\n\n###. Using Python+Terminal\n\nAnother way to get at this information is to try asking matplotlib what fonts it *already* knows about in its cache:\n\n```python\nimport matplotlib\nloc = matplotlib.get_cachedir()\n!ls {loc}\n```\n\nHopefully you will see a list of installed fonts when you run this next block of code. See if you can make sense of what this code does!\n\n```python\nfonts = !cat {loc + '/fontlist-v330.json'}\nfonts = set(list(filter(lambda x:'\"name\"' in x, fonts)))\nfonts = [x.replace('      \"name\": \"','').replace('\",','') for x in fonts]\nprint(fonts)\n```\n\n```python\nfonts = ! fc-list : family style | grep \"Ubuntu\"\nprint(sorted(fonts))\n```\n\n###. Fontdicts\n\nNow that we know what's available, the next step is to set up some useful defaults that we can re-use across multiple plots to ensure consistency of output. The format for specifying fonts on a per-figure basis is a dictionary, so where you see `fontdict` in the `matplotlib` [documentation](https://matplotlib.org/stable/gallery/text_labels_and_annotations/text_fontdict.html) the following should work:\n\nHere's the example:\n\n```python\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n```\n\n```python\ntfont = {'fontname':'Liberation Sans Narrow', 'weight':'bold'}\nbfont = {'fontname':'Liberation Sans Narrow', 'weight':'normal', 'horizontalalignment':'left'}\nafont = {'fontname':'Liberation Sans Narrow', 'weight':'light'}\n```\n\nI am setting the 'title font' (`tfont`) and 'body copy font' (`bfont`) and 'axis font' (`afont`) here to use in the output below. You can pick another font and see what happens. \n\n### 2.3: Using Fonts\n\nAt this point we're going to work towards a kind of 'atlas' that would make it easy to compare some features for different London boroughs. I basically implemented a the basic `matplotlib` version of QGIS' Atlas functionality.\n\n```python\nLA = 'Waltham Forest'\n```\n\n```python\nmsoa_gdf = gpd.read_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n```\n\n```python\nmedian_gdf  = msoa_gdf[['msoa11cd','median','geometry']]\nlisting_gdf = msoa_gdf[['msoa11cd','listings','geometry']].rename(columns={'listings':'count'})\n```\n\n```python\nimport matplotlib.pyplot as plt\n```\n\n###. The Defaults\n\nHere is a demonstration of some of the ways you can adjust features in a Python matplotlib plot. I'm not suggesting either of these is a _good_ output, but that's not the point! The idea is to see the various ways you can tweak a plot... And notice that we've not yet changed any fonts. And it shows.\n\n```python\n# Set up a 1 x 2 plot (you can also leave off the nrows= and ncols=)\nf,axes = plt.subplots(nrows=1, ncols=2, figsize=(10,8))\n# ax1 will be the first plot on the left, ax2 will be on the right;\n# a 2 (or more) *row* plot will return a list of lists... 1 list/row.\nax1 = axes[0]\nax2 = axes[1]\n\n# Left plot is the median price\nmedian_gdf.plot(column='median', ax=ax1, legend=True, cmap='viridis')\nax1.set_title(\"Median Price per MSOA\");\n# Turn off the frame, one side of the plat at a time\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n# Set the labels\nax1.set_xlabel(\"Easting\");\nax1.set_ylabel(\"Northing\");\n\n# Right plot is the number of listings; note\n# here the use of both zorder (which is the \n# 'stacking order' of elements on the plot, and\n# the legend_kwds (keywords) to change the \n# orientation of the plot to horizontal\nlisting_gdf.plot(column='count', ax=ax2, legend=True, cmap='plasma', zorder=1, \n                 legend_kwds={\"orientation\": \"horizontal\"})\nax2.set_title(\"Count of Listings per MSOA\");\n# Set a background colour for the plot\nax2.set_facecolor((.4, .4, .4, .2))\n# Add grid lines and set their zorder to\n# below that of the data on the plot\nplt.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\nax2.set_axisbelow(True)\n\n# This is equivalent to the ax1.spines... \n# above, but if you use it here you lose\n# the background to the plot as well!\n#plt.gca().set(frame_on=False)\n\n# Remove the labels on the ticks of the \n# axes (meaning: remove the numbers on \n# x- and y-axes).\nax2.set_xticklabels([])\nax2.set_yticklabels([])\n\n# Set the labels\nax2.set_xlabel(\"Easting\");\nax2.set_ylabel(\"Northing\");\n```\n\n###: Improving on Defaults\n\n```python\nf,axes = plt.subplots(1,2,figsize=(12,8))\n\n# Set up the plots\nmedian_gdf.plot(column='median', ax=axes[0], legend=True, cmap='viridis')\nlisting_gdf.plot(column='count', ax=axes[1], legend=True, cmap='plasma')\nfor ax in axes:\n    ax.axis('off')\n    # Note that here, set_facebolor doesn't work,\n    # presumably because the axis is 'off'\n    ax.set_facecolor((.4, .4, .4, .2))\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expading the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\naxes[0].set_title('Median Price', size=20, **afont)\naxes[1].set_title('Count', size=20, **afont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.92, linespacing=1.4, va='top', size=12, \n            s=f\"Total listings: {listing_gdf['count'].sum():,.0f}\\nMedian price: ${median_gdf['median'].median():,.2f}\", **bfont);\n```\n\n# Create an Atlas\n\n## Adding Picture-in-Picture \n\nWe're now going to emulate a _bit_ of QGIS' Atlas function by creating two subplots and then adding a _third_ plot afterwards that shows where the borough is.\n\n```python\nf,axes = plt.subplots(1,3,gridspec_kw={'width_ratios':[1,4,4]}, figsize=(12,8))\n\n# Plot 0 is basically being used as a 'spacer' \n# as you'll see below\naxes[0].axis('off')\n\n# Plot 1 is the median price\nmedian_gdf.plot(column='median', ax=axes[1], legend=True, cmap='viridis')\naxes[1].set_title('Median Price', size=20, **afont)\n\n# Plot 2 is the count of listings\nlisting_gdf.plot(column='count', ax=axes[2], legend=True, cmap='plasma')\naxes[2].set_title('Count', size=20, **afont)\n\n# For plots 1 and 2... if you were doing this a lot it could be a function!\nfor ax in axes[1:]:\n    ax.set_facecolor((.9, .9, .9, .5))\n    ax.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\n    ax.set_axisbelow(True)\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.tick_params(axis='both', which='both', length=0)\n\n# Add a *third* chart that we use as a kind of 'PiP'\n# to show which borough we're talking about. The \n# add_axes call is here taking information about the\n# positioning and size of the additional figure.\n# Disable ax2.axis('off') if you want to see the\n# figure in full.\nax3 = f.add_axes([0.015, 0.7, 0.2, 0.2])\nboros.plot(facecolor='lightgrey', edgecolor='w', linewidth=1, ax=ax3)\nboros[boros.NAME==LA].plot(facecolor='r', edgecolor='none', hatch='///', ax=ax3)\nax3.axis('off')\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expading the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.65, s=f\"Total listings: {listing_gdf['count'].sum():,.0f}\", size=12, **bfont);\n```\n\n## Bonus Achievement Unlocked!\n\nIf you have the time and inclination, see if you can convert the above to an _actual_ atlas output: \n\n1. You'll want to turn this plot into a function so as to be able to produce (and save) the map for _every_ borough. \n2. You'll even need to parameterise the filename so that you save to _different_ PNG files as well as going back to see how we generated the listing and pricing data frames for the Local Authority... \n3. And you'll *also* need to make sure that you ensure a consistent colour bar (for all of London, because the median price and number of listings will vary rather a lot by LA)\n4. Then there's the placement of the PiP for some boroughs with long names\n5. And finally, you might consider adding some more text to atlas--maybe pull some content from Wikipedia using Beautiful Soup (`bs4`)?\n\n# Think Text!\n\nI also wanted to draw your attention to this [outstanding piece](https://blog.datawrapper.de/text-in-data-visualizations/) on using text effectively in data visualisation: we often add labels as afterthoughts without too much regard for where they go or how they look; however, getting the content, positioning, size, and even font/font-weight 'right' can make all the difference to the effectiveness of your chart! The illustrations are top-notch.\n\nAnd see the bibliography at the end!\n\n::: {.callout-tip}\n\n    Basically, bookmark this blog post and refer to it every time you are making a map or chart.\n:::\n\n# Using Bokeh\n\nBokeh can do a *lot* more than this, but I just wanted to give you a flavour of the other visualisation tools supported by Python. This obviously works *very* differently in setup and use.\n\n```python\nLA = 'Camden'\n```\n\n```python\ngdf_la = gpd.read_parquet(os.path.join('data','geo','Listings_with_LA.geoparquet'))\nmsoas  = gpd.read_parquet(os.path.join('data','geo','London_MSOA_Names.geoparquet'))\n```\n\n## For a Chart\n\nGroup the listings by Borough and Room Type, and aggregate by median price, also producing a count variable for the number of listings of each type in each Borough.\n\n```python\nla_tots = gdf_la[gdf_la.NAME==LA].groupby(by='room_type', observed=False).agg(\n                            {'price':'median', 'listing_url':'count'}\n                        ).reset_index().rename(columns={'listing_url':'count'})\nla_tots\n```\n\n```python\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.palettes import Spectral4\nfrom bokeh.models import CustomJS, Dropdown\n\noutput_notebook()\n\nroom_types = la_tots.room_type.to_list()\nprices     = la_tots.price.to_list()\ncounts     = la_tots['count'].to_list()\n\n# Add hover tooltip\nsource = ColumnDataSource(data=dict(\n    rt=room_types,\n    count=counts,\n    price=prices,\n))\n\nTOOLTIPS = [\n    (\"Room Type\", \"@rt\"),\n    (\"Number of Listings\", \"@count{,}\"),\n    (\"Median Price\", \"$@price{,}/night\")\n]\n\np = figure(x_range=room_types, height=300, tooltips=TOOLTIPS,\n           title=f\"Median Price by Room Type in {LA}\",\n           toolbar_location=None, tools=\"\")\n\np.vbar(x='rt', top='count', width=0.9, source=source)\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nshow(p)\n```\n\n## For a Map\n\nThis is not the prettiest code, but it should work...\n\n```python\nfrom bokeh.plotting import figure\nfrom bokeh.tile_providers import CARTODBPOSITRON, get_provider\n\nfrom bokeh.io import output_file, show, output_notebook, push_notebook, export_png\nfrom bokeh.models import ColumnDataSource, GeoJSONDataSource, LinearColorMapper, ColorBar, HoverTool\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import brewer\n\noutput_notebook()\n```\n\n```python\nmsoadf = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            msoas[msoas.Borough==LA], predicate='within')\n```\n\n```python\nmsoagrdf = msoadf.groupby('msoa11nm').agg({'price':['median','count']}).reset_index()\nmsoagrdf.columns=['msoa11nm','median','count']\n```\n\nI cobbled the mapping functions below together from two tutorials I found online ([this one](https://github.com/dmnfarrell/teaching/blob/master/geo/maps_python.ipynb) and [this one](https://widdowquinn.github.io/Teaching-Data-Visualisation/exercises/interactive_bokeh_map/interactive_bokeh_map.html)). As you can see, this is a very different approach to mapping data, but it has clear benefits for exploratory purposes and produces fast, interactive maps... and I've not even added selection and filtering tools!\n\n```python\ndef get_geodatasource(gdf):    \n    \"\"\"Get getjsondatasource from geopandas object\"\"\"\n    json_data = json.dumps(json.loads(gdf.to_json()))\n    return GeoJSONDataSource(geojson = json_data)\n\ndef bokeh_plot_map(gdf, column=None, title=''):\n    \"\"\"Plot bokeh map from GeoJSONDataSource \"\"\"\n    \n    tile_provider = get_provider(CARTODBPOSITRON)\n\n    geosource = get_geodatasource(gdf)\n    palette = brewer['OrRd'][8]\n    palette = palette[::-1]\n    vals = gdf[column]\n    \n    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n    color_mapper = LinearColorMapper(palette=palette, low=vals.min(), high=vals.max())\n    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8, width=500, height=10,\n                         location=(0,0), orientation='horizontal')\n\n    tools = 'wheel_zoom,pan,reset,hover'\n    \n    p = figure(title = title, height=700, width=850, toolbar_location='right', tools=tools)\n    p.add_tile(tile_provider)\n    p.xgrid.grid_line_color = None\n    p.ygrid.grid_line_color = None\n    \n    # Add patch renderer to figure\n    p.patches('xs','ys', source=geosource, fill_alpha=0.5, line_width=0.5, line_color='white',  \n              fill_color={'field' :column , 'transform': color_mapper})\n    \n    # Specify figure layout.\n    p.add_layout(color_bar, 'below')\n    \n    # Add hover\n    hover = p.select_one(HoverTool)\n    hover.point_policy = \"follow_mouse\"\n    hover.tooltips = [(\"Borough\", \"@Borough\"),\n                      (\"Neighbourhood\", \"@msoa11hclnm\"),\n                      (\"Count of Listings\", \"@count\"),\n                      (\"Median Price\", \"$@median\")]\n    \n    return p\n```\n\nReproject to Web Mercator:\n\n```python\nmsoa_gdf = pd.merge(msoagrdf, msoas, left_on='msoa11nm', right_on='msoa11nm', how='inner')\nmsoa_gdf = msoa_gdf.set_geometry('geometry').set_crs('epsg:27700')\n```\n\n```python\nmsoageo = msoa_gdf.to_crs('epsg:3785')\nmsoageo.total_bounds\n```\n\nAnd map it!\n\n```python\np = bokeh_plot_map(msoageo, 'median', title=f'MSOA-Level Activity in {LA}')\n\nhandle = show(p, notebook_handle=True)\npush_notebook(handle=handle)\n```\n\n::: {.callout-note}\n\n    **&#128279; Connections**: And that's it. That's all she wrote! You've now covered in 10 weeks what many people might take 10 _months_ to cover. So do not feel like either: 1) you know it all; or 2) you know nothing. You have learned a _lot_, but it's probably just enough to see how much you _don't_ know. That's the start of wisdom. Good luck, young Python-master!\n:::\n\n",
    "supporting": [
      "Practical-09-Presenting_Data_files"
    ],
    "filters": [],
    "includes": {}
  }
}