{
  "hash": "d29d5dda465b40dd61f11c9c11c304fa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Practical 7: Working with Text (Part 2)\"\nsubtitle: \"The basics of Text Mining and NLP\"\njupyter: python3\n#order: 1\n#format: html\nfilters:\n  - qna\n  - quarto\n---\n\n\n| Complete | Part 1: Foundations | Part 2: Data | Part 3: Analysis |     |\n| :------- | :------------------ | :----------- | :--------------- | --: |\n| 60% | &#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619;&#9619; | &#9619;&#9619;&#9619;&#9619;&#9619;&#9617; | &#9617;&#9617;&#9617;&#9617;&#9617;&#9617; | 7/10\n\nA lot of the content here is provided to help you *understand* what text-cleaning does and how it generates tokens that can be processed by the various analytical approaches commonly-used in NLP. The best way to think about this is as a practical in three parts, not all of which you should expect to complete in this session:\n\n1. Tasks 1–3: these are largely focussed on the basics: exploring text and using regular expressions to find and select text.\n2. Tasks 4–5: this might seem like a *bit* of a detour, but it's intended to show you in a more tangible way how 'normalisation' works when we're working with text. **You should feel free to stop here and return to the rest later.**\n3. Tasks 6–7: are about finding important vocabulary (think 'keywords' and 'significant terms') in documents so that you can start to think about what is *distinctive* about documents and groups of documents. **This is quite useful and relatively easier to understand than what comes next!**\n4. Tasks 8–9: are about fully-fledged NLP using Latent Direclecht Allocation (topic modelling) and Word2Vec (words embeddings for use in clustering or similarity work).\n\nThe later parts are largely complete and ready to run; however, that *doesn't* mean you should just skip over them and think you've grasped what's happening and it will be easy to apply in your own analyses. I would *not* pay as much attention to LDA topic mining since I don't think it's results are that good, but I've included it here as it's still commonly-used in the Digital Humanities and by Marketing folks. Word2Vec is much more powerful and forms the basis of the kinds of advances seen in ChatGPT and other LLMs.\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nWorking with text is unquestionably _hard_. In fact, _conceptually_ this is probaly the most challenging practical of the term! But data scientists are _always_ dealing with text because so much of the data that we collect (even more so thanks to the web) is not only text-based (URLs are text!) but, increasingly, unstructured (social media posts, tags, etc.). So while getting to grips with text is a challenge, it also uniquely positions you with respect to the skills and knowledge that other graduates are offering to employers.\n:::\n\n# Preamble\n\nThis practical has been written using `nltk`, but would be _relatively_ easy to rework using `spacy`. Most programmers tend to use one *or* the other, and the switch wouldn't be hard other than having to first load the requisite language models:\n\n::: {#431a409b .cell execution_count=1}\n``` {.python .cell-code}\nimport spacy\n\n# `...web_md` and `...web_lg` are also options\ncorp = \"en_core_web_sm\"\n\ntry: \n    nlp = spacy.load(corp)\nexcept OSError:\n    spacy.cli.download(corp)\n    nlp = spacy.load(corp)\n```\n:::\n\n\nYou can [read about the models](https://spacy.io/models/en), and note that they are also [available in other languages](https://spacy.io/usage/models) besides English.\n\n# Setup\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty Level: Low\n\nBut this is only because this has been worked out for you. Starting from sctach in NLP is _hard_ so people try to avoid it as much as possible.\n\n:::\n\n## Required Modules\n\n::: {.callout-note}\n\nNotice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: `sklearn`, `nltk`, and `spacy`.\n\n:::\n\nStandard libraries we've seen before.\n\n::: {#f361c574 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport re\nimport math\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nVectorisers we will use from the 'big beast' of Python machine learning: Sci-Kit Learn.\n\n::: {#592b5af8 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# We don't use this but I point out where you *could*\nfrom sklearn.preprocessing import OneHotEncoder \n```\n:::\n\n\nNLP-specific libraries that we will use for tokenisation, lemmatisation, and frequency analysis.\n\n::: {#e75de241 .cell execution_count=4}\n``` {.python .cell-code}\nimport nltk\nimport spacy\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\ntry:\n    from nltk.corpus import stopwords\nexcept:\n    nltk.download('stopwords')\n    from nltk.corpus import stopwords\nstopword_list = set(stopwords.words('english'))\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom nltk import ngrams, FreqDist\n\nlemmatizer = WordNetLemmatizer()\ntokenizer = ToktokTokenizer()\n```\n:::\n\n\nRemaining libraries that we'll use for processing and display text data. Most of this relates to dealing with the various ways that text data cleaning is *hard* because of the myriad formats it comes in.\n\n::: {#59013bff .cell execution_count=5}\n``` {.python .cell-code}\nimport string\nimport unicodedata\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud, STOPWORDS\n```\n:::\n\n\nThis next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:\n\n::: {#d62d262c .cell execution_count=6}\n``` {.python .cell-code}\nfrom IPython.display import display_markdown\n\ndef as_markdown(head='', body='Some body text'):\n    if head != '':\n        display_markdown(f\"##### {head}\\n\\n>{body}\\n\", raw=True)\n    else:\n        display_markdown(f\">{body}\\n\", raw=True)\n\nas_markdown('Result!', \"Here's my output...\")\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Result!\n\n>Here's my output...\n\n:::\n:::\n\n\n## Loading Data\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nBecause I generally want each practical to stand on its own (unless I'm trying to make a _point_), I've not moved this to a separate Python file (e.g. `utils.py`, but in line with what we covered back in the lectures on [Functions and Packages](https://jreades.github.io/fsds/sessions/week3.html#lectures), this sort of thing is a good candidate for being split out to a separate file to simplify re-use.\n\n:::\n\nRemember this function from last week? We use it to save downloading files that we already have stored locally. But notice I've made some small changes... what do these do to help the user?\n\n::: {#a95d9d15 .cell execution_count=7}\n``` {.python .cell-code}\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -> str:\n    \"\"\"Downloads and caches a remote file locally.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dest : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[-1] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    # Check if dest+filename does *not* exist -- \n    # that would mean we have to download it! We\n    # also check for *very* small files that are \n    # likely to represent an incomplete download.\n    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        # Convert the path back into a list (without)\n        # the filename -- we need to check that directories\n        # exist first.\n        path = os.path.split(dest)\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how\n        # to use the 'splat' operator! os.makedirs creates missing\n        # directories in a path automatically.\n        if len(path) >= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        # Download and write the file\n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(response.content)\n            \n        print(\"\\tDone downloading...\")\n\n        # What's this doing???\n        f_size = os.stat(dfn).st_size\n        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n        # And why is it here as well???\n        f_size = os.stat(dfn).st_size\n        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n        \n    return dfn\n```\n:::\n\n\n::: {.callout-tip}\n\nFor very large _non_-geographic data sets, remember that you can `use_cols` (or `columns` depending on the file type) to specify a subset of columns to load.\n\n:::\n\nLoad the main data set:\n\n::: {#d39596ce .cell execution_count=8}\n``` {.python .cell-code}\n# Set download URL\nymd  = '2023-09-06'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-listings.geoparquet'\n\ngdf = gpd.read_parquet( cache_data(url, os.path.join('data','geo')), \n                      columns=['geometry', 'listing_url', 'name', \n                               'description', 'amenities', 'price'])\n\ngdf = gdf.to_crs('epsg:27700')\n\nprint(f\"gdf has {gdf.shape[0]:,} rows and CRS is {gdf.crs.name}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/2023-09-06-listings.geoparquet locally!\n\tSize is 42 MB (44,152,609 bytes)\ngdf has 85,134 rows and CRS is OSGB36 / British National Grid.\n```\n:::\n:::\n\n\nLoad supporting Geopackages:\n\n::: {#7d98fe4b .cell execution_count=9}\n``` {.python .cell-code}\nddir  = os.path.join('data','geo') # destination directory\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\n\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound data/geo/Boroughs.gpkg locally!\n\tSize is 1 MB (905,216 bytes)\nFound data/geo/Water.gpkg locally!\n\tSize is 0 MB (208,896 bytes)\nFound data/geo/Greenspace.gpkg locally!\n\tSize is 1 MB (1,146,880 bytes)\nDone.\n```\n:::\n:::\n\n\n# Exploratory Textual Analysis\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nIf you plan to work with data post-graduation then you will _need_ to become comfortable with Regular Expressions (aka. regexes). These are the focus of the [Patterns in Text](https://jreades.github.io/fsds/sessions/week7.html#lectures) lecture but they barely even scratch the surface of what regexes can do. They are _hard_, but they are powerful. \n\n:::\n\n::: {.callout-tip}\n\nIn a full text-mining application I would spend a lot more time on this stage: sampling, looking at descriptions in full, performing my analysis (the rest of the steps) and then coming back with a deeper understanding of the data to make further changes to the analysis.\n\n:::\n\nIt's helpful to have a sense of what data look like before trying to do something with them, but by default pandas truncates quite a lot of output to keep it from overwhelming the display. For text processing, however, you should probably change the amount of preview text provided by pandas using the available options. *Note*: there are lots of other options that you can tweak in pandas.\n\n::: {#412d9582 .cell execution_count=10}\n``` {.python .cell-code}\nprint(f\"Default maximum column width: {pd.options.display.max_colwidth}\") # What's this currently set to?\npd.options.display.max_colwidth=250   # None = no maximum column width (you probably don't want to leave it at this)\nprint(f\"Now maximum column width set to: {pd.options.display.max_colwidth}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDefault maximum column width: 50\nNow maximum column width set to: 250\n```\n:::\n:::\n\n\n## The Description Field\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty level: Moderate, because of the questions.\n\n:::\n\nTo explore the description field properly you'll need to filter out any NA/NaN descriptions before sampling the result. *Hint*: you'll need to think about negation (`~`) of a method output that tells you if a field *is NA*.\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ngdf[???].sample(5, random_state=42)[['description']]\n```\n\n#### Answer\n\n::: {#e4099b86 .cell execution_count=11}\n``` {.python .cell-code}\ngdf[~gdf.description.isna()].sample(5, random_state=42)[['description']]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21898360</th>\n      <td>A wonderful double room in harrow on the hill.&lt;br /&gt;&lt;br /&gt;Very quite and secure area&lt;br /&gt;Guests will have acces to all comunes areas in the flat, bathroom, kitchen, &lt;br /&gt;very spacious natural garden with table and chairs to to take your breakfa...</td>\n    </tr>\n    <tr>\n      <th>40269638</th>\n      <td>- 3 Bedrooms 2 beds and 1 sofa bed&lt;br /&gt;- 2 Bathroom&lt;br /&gt;- 0.5 miles Underground Station &lt;br /&gt;- Free WiFi &lt;br /&gt;&lt;br /&gt;This is an exciting opportunity to stay in a unique part of West London, Maida Hill. &lt;br /&gt;&lt;br /&gt;This stunning 3-bedroom apart...</td>\n    </tr>\n    <tr>\n      <th>610813289456549376</th>\n      <td>St George Wharf is a landmark riverside development spanning across 7 acres of London’s newest area of regeneration. It is distinguishable by its striking gull-wing roofs and stylish terraces. Broadway Malyan Architects championed innovation by d...</td>\n    </tr>\n    <tr>\n      <th>917365608608238592</th>\n      <td>My place is in the Heart of London. It is only 15 mins to KINGS CROSS Underground station, 16 mins to EUROSTAR- St PANCRAS INT, and 2 mins to Angel Station by WALKING. If you choose my place you are in all the LONDON attractions, tourist amenitie...</td>\n    </tr>\n    <tr>\n      <th>6319621</th>\n      <td>This artsy and cozy small 1 bedroom split level flat is 4 min. from Seven Sisters tube/metro station, and close to Tottenham Hale station (ideal if you are arriving from Stansted Airport).  It's in an upcoming multicultural area  of London at beg...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\n::: {.callout-caution}\n\n#### Stop\n\nWhat do you notice about the above? Are they simple text? Are there patterns of problems? Are there characters that represent things other than words and simple punctuation?\n\n:::\n\n### Questions\n\n- What patterns can you see that might need 'dealing with' for text-mining to work?\n- What non-text characters can you see? (Things *other* than A-Z, a-z, and simple punctuation!)\n\n## The Amenities Field\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty level: Moderate, because of the questions.\n\n:::\n\nThis field presents a subtle issue that might not be obvious here:\n\n::: {#540cfe8f .cell execution_count=12}\n``` {.python .cell-code}\ngdf.amenities.sample(5, random_state=42)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nid\n23851484                                                                                                                                                                                             [\"Heating\", \"Washer\", \"Wifi\", \"Kitchen\", \"Dryer\", \"Hangers\", \"Essentials\"]\n930201284366445056    [\"Bathtub\", \"Hot water kettle\", \"Shampoo\", \"Luggage dropoff allowed\", \"Portable heater\", \"Microwave\", \"Bed linens\", \"Blender\", \"Cleaning available during stay\", \"Coffee\", \"Hot water\", \"Iron\", \"Host greets you\", \"Clothing storage: closet, wardrobe...\n19955089              [\"Shampoo\", \"Dryer\", \"Microwave\", \"Coffee maker\", \"Hot water\", \"Iron\", \"First aid kit\", \"Washer\", \"Fire extinguisher\", \"Cooking basics\", \"Heating\", \"Dishwasher\", \"Oven\", \"Refrigerator\", \"TV\", \"Hair dryer\", \"Essentials\", \"Private entrance\", \"Stove...\n886165743387675008                                                                                                                                                                            [\"Lock on bedroom door\", \"Wifi\", \"Free parking on premises\", \"Kitchen\", \"Washer\"]\n13921898              [\"Bathtub\", \"Indoor fireplace\", \"Shampoo\", \"Microwave\", \"Bed linens\", \"Hot water\", \"Iron\", \"Host greets you\", \"Free dryer \\u2013 In building\", \"First aid kit\", \"Paid parking off premises\", \"Washer\", \"Bread maker\", \"Fire extinguisher\", \"Cooking ba...\nName: amenities, dtype: object\n```\n:::\n:::\n\n\nBut look what happens now, can you see the issue a little more easily?\n\n::: {#9af95a46 .cell execution_count=13}\n``` {.python .cell-code}\ngdf.amenities.iloc[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n'[\"Heating\", \"TV with standard cable\", \"Wifi\", \"Smoke alarm\", \"Dryer\", \"Kitchen\", \"Washer\", \"Essentials\"]'\n```\n:::\n:::\n\n\n### Questions\n\n- What's the implicit format of the Amenities columns?\n- How could you represent the data contained in the column?\n\n## Remove NaN Values\n\n::: {.callout-note}\n\nI would be wary of doing the below in a 'proper' application without doing some careful research first, but to make our lives easier, we're going to drop rows where one of these values is NaN _now_ so it will simplify the steps below. In reality, I would spend quite a bit more time investigating which values are NaN and why before simply dropping them.\n\n:::\n\nAnyway, drop all rows where *either* the description or amenities (or both) are NA:\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ngdf = gdf.dropna(???)\nprint(f\"Now gdf has {gdf.shape[0]:,} rows.\")\n```\n\n#### Answer\n\n::: {#12b5f6f9 .cell execution_count=14}\n``` {.python .cell-code}\ngdf = gdf.dropna(subset=['description','amenities'])\nprint(f\"Now gdf has {gdf.shape[0]:,} rows.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNow gdf has 84,273 rows.\n```\n:::\n:::\n\n\n::::\n\nYou should get that there are 84\\,273 rows.\n\n# Using Regular Expressions\n\n::: {.callout-note}\n\n#### &#128279; Connections\n\nWe're building on the work done in Practical 6, but making use now of the lecture on [Patterns in Text](https://jreades.github.io/fsds/sessions/week6.html#lectures)) to quickly sort through the listings.\n\n:::\n\nThere is a _lot_ that can be done with Regular Expressions to identify relevant records in textual data and we're going to use this as a starting point for the rest of the analysis. I would normally consider the regexes here a 'first pass' at the data, but would look very carefully at the output of the TF/IDF vectorizer, Count vectorizer, and LDA to see if I could improve my regexes for further cycles of analysis... the main gain there is that regexes are _much_ faster than using the full NLP (Natural Language Processing) pipeline on the _full_ data set each time. As an alternative, you could develop the pipeline using a random subsample of the data and then process the remaining records sequentially -- in this context there is no justification for doing that, but with a larger corpus it might make sense.\n\n## Luxury Listings\n\n::: {.callout-caution collapse=\"true\"}\n\n#### Difficulty level: Hard, because of the regular expression and questions.\n\n:::\n\nI would like you to find listings that *might* (on the basis of word choice) indicate 'luxury' accommodation.\n\n### Create the Regular Expression\n\nYou should start with variations on 'luxury' (i.e. luxurious, luxuriate, ...) and work out a **single regular expression** that works for variations on this *one* word. **Later**, I would encourage you to come back to this and consider what other words might help to signal 'luxury'... perhaps words like 'stunning' or 'prestigious'? Could you add those to the regex as well?\n\n*Hints*: this is a toughy, but...\n\n1. All regular expressions work best using the `r'...'` (which means raw string) syntax.\n2. You need to be able to *group* terms. Recall, however, that in Python a 'group' of the form `r'(some text)'` refers to matching (`some text` will be 'memoized'/remembered), whereas what you need here is a \"non-capturing group\" of the **positive lookahead** type. That's a Google clue right there, but you've also seen this in the lecture.\n\nIn fact, in my real-world applications you might even need more than one group/non-capturing group in a *nested* structure.\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ngdf[\n    gdf.description.str.contains(r'???', regex=True, flags=re.IGNORECASE) # <-- The regex\n].sample(5, random_state=42)[['description']]\n```\n\n#### Answer\n\n::: {#56252e76 .cell execution_count=15}\n``` {.python .cell-code}\ngdf[\n    gdf.description.str.contains(r'(?:luxur(?:y|ious|iat)|prestigious|stunning)', regex=True, flags=re.IGNORECASE)\n].sample(5, random_state=42)[['description']]\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21052595</th>\n      <td>NEW 4 BEDROOM HOUSE WITH PRIVATE GARDEN IN WEST HAMPSTEAD, LONDON NW6&lt;br /&gt;&lt;br /&gt;Stunning high spec house overall 3 levels. &lt;br /&gt;&lt;br /&gt;Ground floor: Kitchen - Fully equipped with modern appliances. Living area - sofa, TV, dining table and chairs...</td>\n    </tr>\n    <tr>\n      <th>3798279</th>\n      <td>Beautiful home in Kingston upon Thames. Central location - short walk to the river &amp; kingston town centre &amp; kingston train station (35 mins to London).   Bus ride to Hampton Court and Richmond.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Three bedroom home ...</td>\n    </tr>\n    <tr>\n      <th>821457432530779648</th>\n      <td>Stunning spacious one bedroom apartment in one of London's most prime locations. You are a mere minutes away from Holborn station and British museum is right behind the building. Covent Gargen, a shopping and entertainment hub boasting some of th...</td>\n    </tr>\n    <tr>\n      <th>859336290217358976</th>\n      <td>Well thought out townhouse on a pedestrianised Avenue for families/groups of friends wanting lots of space in a fantastic  location super close to Clapham Junction station and Northcote Road.&lt;br /&gt;&lt;br /&gt;Secure underground parking available to pre...</td>\n    </tr>\n    <tr>\n      <th>959952321493154432</th>\n      <td>Perfectly located in Knightsbridge, convenient to most hot spots. This chic home will have you surrounded by the most iconic London sights when you're out and about in the neighborhood, and steeped in Scandinavian culture when you're relaxing in ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\n### Apply it to Select Data\n\nAssign it to a new data frame called `lux`:\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\nlux = gdf[gdf.description.str.contains(r'???', regex=True, flags=re.IGNORECASE)].copy()\nprint(f\"Found {lux.shape[0]:,} records for 'luxury' flats\")\n```\n\n#### Answer\n\n::: {#513e64ee .cell execution_count=16}\n``` {.python .cell-code}\nlux = gdf[gdf.description.str.contains(r'(?:luxur(?:y|ious|iat)|prestigious|stunning)', regex=True, flags=re.IGNORECASE)].copy()\nprint(f\"Found {lux.shape[0]:,} records for 'luxury' flats\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 10,368 records for 'luxury' flats\n```\n:::\n:::\n\n\n::::\n\nYou should get 10\\,368 rows.\n\n### Plot the Data\n\nNow we are going to create a more complex plot that will give space to both the spatial and price distributions using `subplot2grid`.\n\n```python\nhelp(plt.subplot2grid)\n```\n\nNotice that there are two ways to create the plot specified above. I chose route 1, but in some ways route 2 (where you specify a `gridspec` object and *then* add the axes might be a bit simpler to work out if you're starting from scratch.\n\nThe critical thing here is to understand how we'er initialising a plot that has **4 rows** and **1 column** even though it is only showing **2 plots**. What we're going to do is set the *first* plot to span **3 rows** so that it takes up 75% of the plot area (3/4), while the *second* plot only takes up 25% (1/4). They will appear one above the other, so there's only 1 column. Here's how to read the key parts of `subplot2grid`:\n\n- `nrows` -- how many rows *of plots* in the figure.\n- `ncols` -- how many columns *of plots* in the figure.\n- `row` -- what row of the figure does *this* plot start on (0-indexed like a list in Python).\n- `col` -- what column of the figure does *this* plot start on (0-indexed like a list in Python).\n- `rowspan` -- how many rows of the figure does *this* plot span (*not* 0-indexed because it's not list-like).\n- `colspan` -- how many columns of the figure does *this* plot span (*not* 0-indexed because it's not list-like).\n\nEvery time you call `subplot2grid` you are initialising a new axis-object into which you can then draw with your geopackage or pandas plotting methods.\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\n\nf,ax = plt.subplots(1,1,figsize=(9,6))\nax.remove()\n\n# The first plot \nax1 = plt.subplot2grid((4, 1), (???), rowspan=???)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nlux.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((???), (???), rowspan=1)\nlux.price.plot.hist(bins=250, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Luxury\") # <-- How does this differ from title? Change it and see!\nplt.tight_layout() # <-- Try creating the plot *without* this to see what it changes\nplt.show()\n```\n\n#### Answer\n\n::: {#9864ebb7 .cell execution_count=17}\n``` {.python .cell-code}\nf,ax = plt.subplots(1,1,figsize=(7,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nlux.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\nlux.price.plot.hist(bins=250, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Luxury\") # <-- How does this differ from title? Change it and see!\nplt.tight_layout() # <-- Try creating the plot *without* this to see what it changes\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-18-output-1.png){width=661 height=568}\n:::\n:::\n\n\n::::\n\nYour result should look similar to:\n\n::: {#f17be7b5 .cell execution_count=18}\n\n::: {.cell-output .cell-output-display execution_count=18}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-19-output-1.png){width=661 height=568}\n:::\n:::\n\n\n### Questions\n\n- What does `suptitle` do and how is it different from `title`? Could you use this as part of your plot-making process?\n- What does `tight_layout` do?\n\n## Budget Listings\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Easy, because you've worked out the hard bits already.\n\n:::\n\n### Create the Regular Expression\n\nWhat words can you think of that might help you to spot affordable and budget accommodation? Start with just a couple of words and then I would encourage you to consider what _other_ words might help to signal 'affordability'... perhaps words like 'cosy' or 'charming' and then think about how you could you add those to the regex?\n\n*Hints*: this just builds on what you did above with one exception:\n\n1. I'd try adding word boundary markers to the regex (`\\b`) where appropriate...\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ngdf[\n    gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)\n].sample(5, random_state=42)[['description']]\n```\n\n#### Answer\n\n::: {#6972897a .cell execution_count=19}\n``` {.python .cell-code}\ngdf[\n    gdf.description.str.contains(r'\\b(?:affordable|budget|cheap|cosy)\\b', \n    regex=True, flags=re.IGNORECASE)\n].sample(5, random_state=42)[['description']]\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10466073</th>\n      <td>This is a charming little flat in crouch end, North London. It is located 10min from crouch end \"village\" which is packed with coffee shops, nice restaurants, shops etc... It is on a very quiet street and easy access to central London.&lt;br /&gt;&lt;br /...</td>\n    </tr>\n    <tr>\n      <th>679538210387434496</th>\n      <td>This trendy and spacious 3 bed house in Bow, is ideal for people who want to experience London but without all the hustle and bustle.&lt;br /&gt;&lt;br /&gt;It is situated in a peaceful area, where the vast open space of Victoria Park is within easy reach an...</td>\n    </tr>\n    <tr>\n      <th>7560171</th>\n      <td>The spacious house offers double bedroom with ensuit bathroom, a large kitchen and cosy living room.&lt;br /&gt;This luxurious and modern apartment is located 3 mins walking from London Bridge station; walking distance from many of Londons attractions&lt;...</td>\n    </tr>\n    <tr>\n      <th>721518857445429120</th>\n      <td>Kick back and relax in this calm, stylish one-bedroom, ground-floor flat in Leyton Village. There’s a contemporary kitchen and cosy living room with a sofa bed. The location puts you between Leyton and Leytonstone tube stations in a quiet street ...</td>\n    </tr>\n    <tr>\n      <th>731014319309725184</th>\n      <td>Beautiful 1 bedroom apartment in the heart of Peckham, only 10 minutes walk away from Peckham Rye station with direct trains to London Victoria (15mins), London Bridge (10mins) and Clapham Junction (16mins) - bringing the whole of London to your ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\n### Apply it to Select Data\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\naff = gdf[gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)].copy()\nprint(f\"There are {aff.shape[0]:,} rows flagged as 'affordable'.\")\n```\n\n#### Answer\n\n::: {#d4a329d3 .cell execution_count=20}\n``` {.python .cell-code}\naff = gdf[gdf.description.str.contains(r'\\b(?:affordable|budget|cheap|cosy)\\b', \n          regex=True, flags=re.IGNORECASE)].copy()\nprint(f\"There are {aff.shape[0]:,} rows flagged as 'affordable'.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are 8,937 rows flagged as 'affordable'.\n```\n:::\n:::\n\n\n::::\n\nYou should get `{python} f\"{aff.shape[0]:,}\" ` rows.\n\n### Plot the Data\n\n::: {#85d4e682 .cell execution_count=21}\n``` {.python .cell-code}\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\naff.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\naff.price.plot.hist(bins=100, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Affordability\")\nplt.tight_layout()\n#plt.savefig(\"Affordable_Listings.png\", dpi=150)\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-22-output-1.png){width=757 height=568}\n:::\n:::\n\n\n### Questions\n\n- Do you think that this is a *good* way to select affordable options?\n- Do you understand what `dpi` means and how `savefig` works?\n- Copy the code from above but modify it to constrain the histogram on a more limited distribution by *filtering* out the outliers *before* drawing the plot. I would copy the cell above to one just below here so that you keep a working copy available and can undo any changes that break things.\n\n```python\n\n\n```\n\n## Near Bluespace\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty level: Medium, because you're still learning about regexes.\n\n:::\n\nNow see if you can work out a regular expression to find accommodation that emphasises accessibility to the Thames and other 'blue spaces' as part of the description? One thing you'll need to tackle is that some listings seem to say something about Thameslink and you wouldn't want those be returned as part of a regex looking for _rivers_. So by way of a hint:\n\n- You probably need to think about the Thames, rivers, and water.\n- These will probably be *followed* by a qualifier like a 'view' (e.g. Thames-view) or a front (e.g. water-front).\n- But you need to rule out things like \"close the Thameslink station...\"\n\n### Create the regular Expression\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ngdf[\n    gdf.description.str.contains(???, regex=???, flags=???)\n].sample(5, random_state=42)[['description']]\n```\n\n#### Answer\n\n::: {#2e1454eb .cell execution_count=22}\n``` {.python .cell-code}\ngdf[\n    gdf.description.str.contains(r'(?:Thames|river|water|canals?)(?:-|\\s+)(?: view|\\s?front)\\b', \n    regex=True, flags=re.IGNORECASE)\n].sample(5, random_state=42)[['description']]\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>906924808902274560</th>\n      <td>Brand new ultra spacious apartment just one minute walk from the Excel center and with great connections to anywhere in London.&lt;br /&gt;The area is full with cafés, supermarkets and restaurants just at your door step with lots of fun things to do li...</td>\n    </tr>\n    <tr>\n      <th>600645213392859136</th>\n      <td>Are you a woman?&lt;br /&gt;We offer a warm and friendly, Home from home, welcome to women travellers in our,  peaceful and centrally located home.&lt;br /&gt;&lt;br /&gt; Excellent location in Zone1, with river ferry, trains, tube, and buses within a 5 minute wal...</td>\n    </tr>\n    <tr>\n      <th>868117243960619264</th>\n      <td>Just across the street from water front, near shadwell basin. View of Canary Wharf and the Shard on the big balcony&lt;br /&gt;&lt;br /&gt;7mins to shadwell overground and DLR. just 1 stop from Bank, Tower Bridge/Tower Hill, White Chapel (which connects to E...</td>\n    </tr>\n    <tr>\n      <th>722937027420061056</th>\n      <td>There’s no better way to experience the beauty of London City than by sleeping right in the heart of it. This Ensuite is within a 2-bed property. Ideally located at the river front to enjoy the mesmerising view of Canary Wharf.&lt;br /&gt;&lt;br /&gt;Within ...</td>\n    </tr>\n    <tr>\n      <th>26748596</th>\n      <td>Welcome to Sunbury Lane in Battersea :)&lt;br /&gt;&lt;br /&gt;My house is just off the river front allowing a large, safe space from which to explore the best of South London whilst also having a bus that runs you from our road into central in just 15 minut...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::::\n\n### Apply it to the Select Data\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\nbluesp = gdf[\n    (gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)) |\n    (gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE))\n].copy()\nprint(f\"Found {bluesp.shape[0]:,} rows.\")\n```\n\n#### Answer\n\n::: {#3428b7a4 .cell execution_count=23}\n``` {.python .cell-code}\nbluesp = gdf[\n    (gdf.description.str.contains(r'(?:Thames|river|water|canals?)(?:-|\\s+)(?:view|front)\\b', regex=True, flags=re.IGNORECASE)) |\n    (gdf.description.str.contains(r'(?:walk|close|near) to (?:Thames|river|water|canal)', regex=True, flags=re.IGNORECASE))\n].copy()\nprint(f\"Found {bluesp.shape[0]:,} rows.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 408 rows.\n```\n:::\n:::\n\n\n::::\n\nYou should get `{python} f\"{bluesp.shape[0]:,}\" ` rows.\n\n### Plot the Data\n\n::: {#b5f213b2 .cell execution_count=24}\n``` {.python .cell-code}\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nwater.plot(edgecolor='none', facecolor=(.25, .25, .7, .25), ax=ax1)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nbluesp.plot(markersize=2, column='price', cmap='viridis', alpha=0.5, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\nbluesp.price.plot.hist(bins=100, ax=ax2)\n\nplt.suptitle(\"Bluespace Listings\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-25-output-1.png){width=758 height=568}\n:::\n:::\n\n\n### Questions\n\n- How else might you select listings with a view of the Thames or other bluespaces?\n\n# Illustrative Text Cleaning\n\nNow we're going to step through the _parts_ of the process that we apply to clean and transform text. We'll do this individually before using a function to apply them _all at once_.\n\n## Downloading a Web Page\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty Level: Low.\n\n:::\n\nThere is plenty of good economic geography research being done using web pages. Try using Google Scholar to look for work using the British Library's copy of the *Internet Archive*.\n\n::: {#f38a3de6 .cell execution_count=25}\n``` {.python .cell-code}\nfrom urllib.request import urlopen, Request\n\n# We need this so that the Bartlett web site 'knows'\n# what kind of browser it is deasling with. Otherwise\n# you get a Permission Error (403 Forbidden) because\n# the site doesn't know what to do.\nhdrs = {\n    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n}\nurl = 'https://www.ucl.ac.uk/bartlett/casa/about-0'\n```\n:::\n\n\n:::: {.panel-tabset}\n\n#### Question \n\n```python\n# Notice that here we have to assemble a request and\n# then 'open' it so that the request is properly issued\n# to the web server. Normally, we'd just use `urlopen`, \n# but that doesn't give you the ability to set the headers.\nrequest  = Request(url, None, hdrs) #The assembled request\nresponse = urlopen(request)\nhtml     = response.???.decode('utf-8') # The data you need\n\nprint(html[:1000])\n```\n\n#### Answer\n\n::: {#bea7e323 .cell execution_count=26}\n``` {.python .cell-code}\n# Notice that here we have to assemble a request and\n# then 'open' it so that the request is properly issued\n# to the web server. Normally, we'd just use `urlopen`, \n# but that doesn't give you the ability to set the headers.\nrequest  = Request(url, None, hdrs) #The assembled request\nresponse = urlopen(request)\nhtml     = response.read().decode('utf-8') # The data you need\n\nprint(html[:1000])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<!DOCTYPE html>\n<!--[if IE 7]>\n<html lang=\"en\" class=\"lt-ie9 lt-ie8 no-js\"> <![endif]-->\n<!--[if IE 8]>\n<html lang=\"en\" class=\"lt-ie9 no-js\"> <![endif]-->\n<!--[if gt IE 8]><!-->\n<html lang=\"en\" class=\"no-js\"> <!--<![endif]-->\n<head>\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n  <meta name=\"author\" content=\"UCL\"/>\n  <meta property=\"og:profile_id\" content=\"uclofficial\"/>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n<link rel=\"shortcut icon\" href=\"https://www.ucl.ac.uk/bartlett/casa/sites/all/themes/indigo/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n<meta name=\"description\" content=\"The Centre for Advanced Spatial Analysis (CASA) is an interdisciplinary research institute focusing on the science of cities within The Bartlett Faculty of the Built Environment at UCL.\" />\n<link rel=\"canonical\" href=\"https://www.ucl.ac.uk/bartlett/casa/about-0\" />\n<meta name=\"ucl:faculty\" content=\"Bartlett\" />\n<meta name=\"ucl:org_unit\" content=\"Cent\n```\n:::\n:::\n\n\n::::\n\n## Removing HTML\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty level: Moderate\n\nBecause what we're doing will seem really strange and uses some previously unseen libraries that you'll have to google.\n\n:::\n\n*Hint*: you need to need to **get the text** out of the each returned `<p>` and `<div>` element! I'd suggest also commenting this up since there is a *lot* going on on some of these lines of code!\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\ncleaned = []\n\nsoup = BeautifulSoup(html)\nbody = soup.find('body')\n\nfor c in body.findChildren(recursive=False):\n    if c.name in ['div','p'] and c.???.strip() != '': \n        # \\xa0 is a non-breaking space in Unicode (&nbsp; in HTML)\n        txt = [re.sub(r'(?:\\u202f|\\xa0|\\u200b)',' ',x.strip()) for x in c.get_text(separator=\" \").split('\\n') if x.strip() != '']\n        cleaned += txt\n\ncleaned\n```\n\n#### Answer\n\n::: {#433e7e9d .cell execution_count=27}\n``` {.python .cell-code}\ncleaned = []\n\nsoup = BeautifulSoup(html)\nbody = soup.find('body')\n\nfor c in body.findChildren(recursive=False):\n    if c.name in ['div','p'] and c.get_text().strip() != '':\n        # \\xa0 is a non-breaking space in Unicode (&nbsp; in HTML)\n        txt = [re.sub(r'(?:\\u202f|\\xa0|\\u200b)',' ',x.strip()) for x in c.get_text(separator=\" \").split('\\n') if x.strip() != '']\n        cleaned += txt\n\ncleaned\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n['UCL Home The Bartlett Centre for Advanced Spatial Analysis About',\n 'About',\n 'The Centre for Advanced Spatial Analysis (CASA) is an interdisciplinary research institute focusing on the science of cities within The Bartlett Faculty of the Built Environment at UCL.',\n 'The Centre for Advanced Spatial Analysis (CASA) was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science, sensing the urban environment, visualisation and computation.  Today, CASA’s research is still pushing boundaries to create better cities for everyone, both leading the intellectual agenda and working closely with government and industry partners to make real-world impact.  Our teaching reflects this, making the most of our cutting-edge research, tools, new forms of data, and long-standing non-academic partnerships to train the next generation of urban scientists with the skills and ideas they’ll need to have an impact in industry, government, academia, and the third sector.  The CASA community is closely connected, but strongly interdisciplinary. We bring together people from around the world with a unique variety of backgrounds – including physicists, planners, geographers, economists, data scientists, architects, mathematicians and computer scientists – united by our mission to tackle the biggest challenges facing cities and societies around the world.  We work across multiple scales: from the hyper-local environment of the low-powered sensor all the way up to satellite remote sensing of whole countries and regions.  Studying at CASA brings lifelong value, with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science.  By studying with us you will become part of our active and engaged alumni community, with access to job listings, networking and social activities, as well as continued contact with our outstanding teachers and researchers.   Location The UCL Centre for Advanced Spatial Analysis is located at 90 Tottenham Court Road, London, W1T 4TJ.',\n 'View Map',\n 'Contact Address: UCL Centre for Advanced Spatial Analysis First Floor, 90 Tottenham Court Road London W1T 4TJ Telephone:  +44 (0)20 3108 3877 Email:   casa@ucl.ac.uk',\n 'Tweets by CASAUCL']\n```\n:::\n:::\n\n\n::::\n\n## Lower Case\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty Level: Low.\n\n:::\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\nlower = [c.???() for ??? in cleaned]\nlower\n```\n\n#### Answer\n\n::: {#5313b564 .cell execution_count=28}\n``` {.python .cell-code}\nlower = [s.lower() for s in cleaned]\nlower\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n['ucl home the bartlett centre for advanced spatial analysis about',\n 'about',\n 'the centre for advanced spatial analysis (casa) is an interdisciplinary research institute focusing on the science of cities within the bartlett faculty of the built environment at ucl.',\n 'the centre for advanced spatial analysis (casa) was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science, sensing the urban environment, visualisation and computation.  today, casa’s research is still pushing boundaries to create better cities for everyone, both leading the intellectual agenda and working closely with government and industry partners to make real-world impact.  our teaching reflects this, making the most of our cutting-edge research, tools, new forms of data, and long-standing non-academic partnerships to train the next generation of urban scientists with the skills and ideas they’ll need to have an impact in industry, government, academia, and the third sector.  the casa community is closely connected, but strongly interdisciplinary. we bring together people from around the world with a unique variety of backgrounds – including physicists, planners, geographers, economists, data scientists, architects, mathematicians and computer scientists – united by our mission to tackle the biggest challenges facing cities and societies around the world.  we work across multiple scales: from the hyper-local environment of the low-powered sensor all the way up to satellite remote sensing of whole countries and regions.  studying at casa brings lifelong value, with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science.  by studying with us you will become part of our active and engaged alumni community, with access to job listings, networking and social activities, as well as continued contact with our outstanding teachers and researchers.   location the ucl centre for advanced spatial analysis is located at 90 tottenham court road, london, w1t 4tj.',\n 'view map',\n 'contact address: ucl centre for advanced spatial analysis first floor, 90 tottenham court road london w1t 4tj telephone:  +44 (0)20 3108 3877 email:   casa@ucl.ac.uk',\n 'tweets by casaucl']\n```\n:::\n:::\n\n\n::::\n\n## Stripping 'Punctuation'\n\n::: {.callout-caution collapse=\"true\"}\n\n#### Difficulty level: Hard\n\nThis is because you need to understand: 1) why we're _compiling_ the regular expression and how to use character classes; and 2) how the NLTK tokenizer differs in approach to the regex.\n\n:::\n\n### Regular Expression Approach\n\nWe want to clear out punctuation using a regex that takes advantage of the `[...]` (character class) syntax. The really tricky part is remembering how to specify the 'punctuation' when some of that punctuation has 'special' meanings in a regular expression context. For instance, `.` means 'any character', while `[` and `]` mean 'character class'. So this is another *escaping* problem and it works the *same* way it did when we were dealing with the Terminal... \n\n*Hints*: some other factors... \n\n1. You will want to match more than one piece of punctuation at a time, so I'd suggest add a `+` to your pattern.\n2. You will need to look into *metacharacters* for creating a kind of 'any of the characters *in this class*' bag of possible matches.\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\npattern = re.compile(r'[???]+')\nprint(pattern)\n```\n\n#### Answer\n\n::: {#29e4a438 .cell execution_count=29}\n``` {.python .cell-code}\npattern = re.compile(r'[,\\.!\\-><=\\(\\)\\[\\]\\/&\\'\\\"’;\\+\\–\\—]+')\nprint(pattern)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nre.compile('[,\\\\.!\\\\-><=\\\\(\\\\)\\\\[\\\\]\\\\/&\\\\\\'\\\\\"’;\\\\+\\\\–\\\\—]+')\n```\n:::\n:::\n\n\n::::\n\n### Tokenizer\n\nThe other way to do this, which is probably *easier* but produces more complex output, is to draw on the tokenizers [already provided by NLTK](https://www.nltk.org/api/nltk.tokenize.html). For our purposes `word_tokenize` is probably fine, but depending on your needs there are other options and you can also write your own.\n\n::: {#d4fda0a7 .cell execution_count=30}\n``` {.python .cell-code}\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nprint(word_tokenize)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<function word_tokenize at 0x3112ee160>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package punkt to /Users/jreades/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/jreades/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n```\n:::\n:::\n\n\n### Compare\n\nLook at how these outputs differ in subtle ways:\n\n::: {#9667d17a .cell execution_count=31}\n``` {.python .cell-code}\nsubbed = []\ntokens = []\nfor l in lower:\n    subbed.append(re.sub(pattern, ' ', l))\n    tokens.append(word_tokenize(l))\n\nfor s in subbed:\n    as_markdown(\"Substituted\", s)\n\nfor t in tokens:\n    as_markdown(\"Tokenised\", t)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>ucl home the bartlett centre for advanced spatial analysis about\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>about\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>the centre for advanced spatial analysis  casa  is an interdisciplinary research institute focusing on the science of cities within the bartlett faculty of the built environment at ucl \n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>the centre for advanced spatial analysis  casa  was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science  sensing the urban environment  visualisation and computation   today  casa s research is still pushing boundaries to create better cities for everyone  both leading the intellectual agenda and working closely with government and industry partners to make real world impact   our teaching reflects this  making the most of our cutting edge research  tools  new forms of data  and long standing non academic partnerships to train the next generation of urban scientists with the skills and ideas they ll need to have an impact in industry  government  academia  and the third sector   the casa community is closely connected  but strongly interdisciplinary  we bring together people from around the world with a unique variety of backgrounds   including physicists  planners  geographers  economists  data scientists  architects  mathematicians and computer scientists   united by our mission to tackle the biggest challenges facing cities and societies around the world   we work across multiple scales: from the hyper local environment of the low powered sensor all the way up to satellite remote sensing of whole countries and regions   studying at casa brings lifelong value  with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science   by studying with us you will become part of our active and engaged alumni community  with access to job listings  networking and social activities  as well as continued contact with our outstanding teachers and researchers    location the ucl centre for advanced spatial analysis is located at 90 tottenham court road  london  w1t 4tj \n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>view map\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>contact address: ucl centre for advanced spatial analysis first floor  90 tottenham court road london w1t 4tj telephone:   44  0 20 3108 3877 email:   casa@ucl ac uk\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Substituted\n\n>tweets by casaucl\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['ucl', 'home', 'the', 'bartlett', 'centre', 'for', 'advanced', 'spatial', 'analysis', 'about']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['about']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['the', 'centre', 'for', 'advanced', 'spatial', 'analysis', '(', 'casa', ')', 'is', 'an', 'interdisciplinary', 'research', 'institute', 'focusing', 'on', 'the', 'science', 'of', 'cities', 'within', 'the', 'bartlett', 'faculty', 'of', 'the', 'built', 'environment', 'at', 'ucl', '.']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['the', 'centre', 'for', 'advanced', 'spatial', 'analysis', '(', 'casa', ')', 'was', 'established', 'in', '1995', 'to', 'lead', 'the', 'development', 'of', 'a', 'science', 'of', 'cities', 'drawing', 'upon', 'methods', 'and', 'ideas', 'in', 'modelling', 'and', 'data', 'science', ',', 'sensing', 'the', 'urban', 'environment', ',', 'visualisation', 'and', 'computation', '.', 'today', ',', 'casa', '’', 's', 'research', 'is', 'still', 'pushing', 'boundaries', 'to', 'create', 'better', 'cities', 'for', 'everyone', ',', 'both', 'leading', 'the', 'intellectual', 'agenda', 'and', 'working', 'closely', 'with', 'government', 'and', 'industry', 'partners', 'to', 'make', 'real-world', 'impact', '.', 'our', 'teaching', 'reflects', 'this', ',', 'making', 'the', 'most', 'of', 'our', 'cutting-edge', 'research', ',', 'tools', ',', 'new', 'forms', 'of', 'data', ',', 'and', 'long-standing', 'non-academic', 'partnerships', 'to', 'train', 'the', 'next', 'generation', 'of', 'urban', 'scientists', 'with', 'the', 'skills', 'and', 'ideas', 'they', '’', 'll', 'need', 'to', 'have', 'an', 'impact', 'in', 'industry', ',', 'government', ',', 'academia', ',', 'and', 'the', 'third', 'sector', '.', 'the', 'casa', 'community', 'is', 'closely', 'connected', ',', 'but', 'strongly', 'interdisciplinary', '.', 'we', 'bring', 'together', 'people', 'from', 'around', 'the', 'world', 'with', 'a', 'unique', 'variety', 'of', 'backgrounds', '–', 'including', 'physicists', ',', 'planners', ',', 'geographers', ',', 'economists', ',', 'data', 'scientists', ',', 'architects', ',', 'mathematicians', 'and', 'computer', 'scientists', '–', 'united', 'by', 'our', 'mission', 'to', 'tackle', 'the', 'biggest', 'challenges', 'facing', 'cities', 'and', 'societies', 'around', 'the', 'world', '.', 'we', 'work', 'across', 'multiple', 'scales', ':', 'from', 'the', 'hyper-local', 'environment', 'of', 'the', 'low-powered', 'sensor', 'all', 'the', 'way', 'up', 'to', 'satellite', 'remote', 'sensing', 'of', 'whole', 'countries', 'and', 'regions', '.', 'studying', 'at', 'casa', 'brings', 'lifelong', 'value', ',', 'with', 'our', 'students', 'poised', 'to', 'take', 'on', 'leadership', 'and', 'integration', 'roles', 'at', 'the', 'forefront', 'of', 'urban', 'and', 'spatial', 'data', 'science', '.', 'by', 'studying', 'with', 'us', 'you', 'will', 'become', 'part', 'of', 'our', 'active', 'and', 'engaged', 'alumni', 'community', ',', 'with', 'access', 'to', 'job', 'listings', ',', 'networking', 'and', 'social', 'activities', ',', 'as', 'well', 'as', 'continued', 'contact', 'with', 'our', 'outstanding', 'teachers', 'and', 'researchers', '.', 'location', 'the', 'ucl', 'centre', 'for', 'advanced', 'spatial', 'analysis', 'is', 'located', 'at', '90', 'tottenham', 'court', 'road', ',', 'london', ',', 'w1t', '4tj', '.']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['view', 'map']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['contact', 'address', ':', 'ucl', 'centre', 'for', 'advanced', 'spatial', 'analysis', 'first', 'floor', ',', '90', 'tottenham', 'court', 'road', 'london', 'w1t', '4tj', 'telephone', ':', '+44', '(', '0', ')', '20', '3108', '3877', 'email', ':', 'casa', '@', 'ucl.ac.uk']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Tokenised\n\n>['tweets', 'by', 'casaucl']\n\n:::\n:::\n\n\n## Stopword Removal\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty Level: Moderate \n\nYou need to remember how list comprehensions work to use the `stopword_list`.\n\n:::\n\n::: {#5000c572 .cell execution_count=32}\n``` {.python .cell-code}\nstopword_list = set(stopwords.words('english'))\nprint(stopword_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'our', 'before', 'does', 't', 'was', 'not', 'or', 'himself', 'more', 'i', 'y', 'has', 'wouldn', 'themselves', 'each', \"you'll\", 'such', \"needn't\", 'and', 'then', 'as', 'o', 'over', 'is', 'did', 'up', 'be', 'me', 'were', \"you'd\", \"haven't\", \"wouldn't\", 'them', 'about', 'down', \"mustn't\", 'do', 'hasn', 'mustn', 'can', 'those', 'again', 'it', 'if', 'they', \"hadn't\", \"wasn't\", 'few', 'so', 'an', 'should', 'myself', 'here', 'very', 'the', 'until', 'why', 'you', 'ma', 'been', 'mightn', 'than', 'yourself', 'all', 'at', 'no', 'of', 'into', 'out', \"don't\", \"weren't\", \"won't\", \"should've\", 'aren', 'after', 'their', \"that'll\", 'needn', \"mightn't\", 've', \"doesn't\", \"it's\", 'am', \"she's\", 'its', 'she', 'above', 'for', 'wasn', 'that', 'yours', 'whom', 'm', 'a', 'who', 'on', 'from', 'his', 'below', 'just', 'will', 'isn', 'he', 'nor', 'couldn', 'which', 'how', 'shan', 'while', 's', 'll', 'ourselves', 'had', 'off', \"isn't\", 'against', 'don', 'being', 'itself', 'shouldn', 'these', 'having', 'both', 'ours', 'doesn', 'theirs', 'we', \"shan't\", 'won', 'yourselves', 'there', 'through', 'by', 'have', \"didn't\", 'what', 'with', 'in', 'ain', 'same', 'because', 'once', 'where', 'weren', 'most', 'any', 'between', 'only', 'to', \"couldn't\", 'him', 'didn', 'd', \"you've\", 're', 'other', 'this', \"you're\", 'now', \"aren't\", 'hers', 'my', \"hasn't\", 'but', 'further', 'some', 'haven', 'hadn', 'doing', 'your', 'too', 'herself', 'own', 'under', 'are', 'during', 'when', \"shouldn't\", 'her'}\n```\n:::\n:::\n\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\nstopped = []\nfor p in tokens[2:4]: # <-- why do I just take these items from the list?\n    stopped.append([x for x in p if x not in ??? and len(x) > 1])\n\nfor s in stopped:\n    as_markdown(\"Line\", s)\n```\n\n#### Answer\n\n::: {#4b10b3fa .cell execution_count=33}\n``` {.python .cell-code}\nstopped = []\nfor p in tokens[2:4]: # <-- why do I just take these items from the list?\n    stopped.append([x for x in p if x not in stopword_list and len(x) > 1])\n\nfor s in stopped:\n    as_markdown(\"Line\", s)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Line\n\n>['centre', 'advanced', 'spatial', 'analysis', 'casa', 'interdisciplinary', 'research', 'institute', 'focusing', 'science', 'cities', 'within', 'bartlett', 'faculty', 'built', 'environment', 'ucl']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Line\n\n>['centre', 'advanced', 'spatial', 'analysis', 'casa', 'established', '1995', 'lead', 'development', 'science', 'cities', 'drawing', 'upon', 'methods', 'ideas', 'modelling', 'data', 'science', 'sensing', 'urban', 'environment', 'visualisation', 'computation', 'today', 'casa', 'research', 'still', 'pushing', 'boundaries', 'create', 'better', 'cities', 'everyone', 'leading', 'intellectual', 'agenda', 'working', 'closely', 'government', 'industry', 'partners', 'make', 'real-world', 'impact', 'teaching', 'reflects', 'making', 'cutting-edge', 'research', 'tools', 'new', 'forms', 'data', 'long-standing', 'non-academic', 'partnerships', 'train', 'next', 'generation', 'urban', 'scientists', 'skills', 'ideas', 'need', 'impact', 'industry', 'government', 'academia', 'third', 'sector', 'casa', 'community', 'closely', 'connected', 'strongly', 'interdisciplinary', 'bring', 'together', 'people', 'around', 'world', 'unique', 'variety', 'backgrounds', 'including', 'physicists', 'planners', 'geographers', 'economists', 'data', 'scientists', 'architects', 'mathematicians', 'computer', 'scientists', 'united', 'mission', 'tackle', 'biggest', 'challenges', 'facing', 'cities', 'societies', 'around', 'world', 'work', 'across', 'multiple', 'scales', 'hyper-local', 'environment', 'low-powered', 'sensor', 'way', 'satellite', 'remote', 'sensing', 'whole', 'countries', 'regions', 'studying', 'casa', 'brings', 'lifelong', 'value', 'students', 'poised', 'take', 'leadership', 'integration', 'roles', 'forefront', 'urban', 'spatial', 'data', 'science', 'studying', 'us', 'become', 'part', 'active', 'engaged', 'alumni', 'community', 'access', 'job', 'listings', 'networking', 'social', 'activities', 'well', 'continued', 'contact', 'outstanding', 'teachers', 'researchers', 'location', 'ucl', 'centre', 'advanced', 'spatial', 'analysis', 'located', '90', 'tottenham', 'court', 'road', 'london', 'w1t', '4tj']\n\n:::\n:::\n\n\n::::\n\n## Lemmatisation vs Stemming\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low.\n\n:::\n\n::: {#6923bc7f .cell execution_count=34}\n``` {.python .cell-code}\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer \n```\n:::\n\n\n::: {#7eba7e4e .cell execution_count=35}\n``` {.python .cell-code}\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('monkeys'))\nprint(lemmatizer.lemmatize('cities'))\nprint(lemmatizer.lemmatize('complexity'))\nprint(lemmatizer.lemmatize('Reades'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmonkey\ncity\ncomplexity\nReades\n```\n:::\n:::\n\n\n::: {#8b9404a9 .cell execution_count=36}\n``` {.python .cell-code}\nstemmer = PorterStemmer()\nprint(stemmer.stem('monkeys'))\nprint(stemmer.stem('cities'))\nprint(stemmer.stem('complexity'))\nprint(stemmer.stem('Reades'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmonkey\nciti\ncomplex\nread\n```\n:::\n:::\n\n\n::: {#b6ffdf5a .cell execution_count=37}\n``` {.python .cell-code}\nstemmer = SnowballStemmer(language='english')\nprint(stemmer.stem('monkeys'))\nprint(stemmer.stem('cities'))\nprint(stemmer.stem('complexity'))\nprint(stemmer.stem('Reades'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmonkey\nciti\ncomplex\nread\n```\n:::\n:::\n\n\n::: {#4ca9fde7 .cell execution_count=38}\n``` {.python .cell-code}\nlemmatizer = WordNetLemmatizer()\nlemmas  = []\nstemmed = []\n\n# This would be better if we passed in a PoS (Part of Speech) tag as well,\n# but processing text for parts of speech is *expensive* and for the purposes\n# of this tutorial, not necessary.\nfor s in stopped:\n    lemmas.append([lemmatizer.lemmatize(x) for x in s])\n\nfor s in stopped:\n    stemmed.append([stemmer.stem(x) for x in s])\n\nfor l in lemmas:\n    as_markdown('Lemmatised',l)\n\nfor s in stemmed:\n    as_markdown('Stemmed',s)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Lemmatised\n\n>['centre', 'advanced', 'spatial', 'analysis', 'casa', 'interdisciplinary', 'research', 'institute', 'focusing', 'science', 'city', 'within', 'bartlett', 'faculty', 'built', 'environment', 'ucl']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Lemmatised\n\n>['centre', 'advanced', 'spatial', 'analysis', 'casa', 'established', '1995', 'lead', 'development', 'science', 'city', 'drawing', 'upon', 'method', 'idea', 'modelling', 'data', 'science', 'sensing', 'urban', 'environment', 'visualisation', 'computation', 'today', 'casa', 'research', 'still', 'pushing', 'boundary', 'create', 'better', 'city', 'everyone', 'leading', 'intellectual', 'agenda', 'working', 'closely', 'government', 'industry', 'partner', 'make', 'real-world', 'impact', 'teaching', 'reflects', 'making', 'cutting-edge', 'research', 'tool', 'new', 'form', 'data', 'long-standing', 'non-academic', 'partnership', 'train', 'next', 'generation', 'urban', 'scientist', 'skill', 'idea', 'need', 'impact', 'industry', 'government', 'academia', 'third', 'sector', 'casa', 'community', 'closely', 'connected', 'strongly', 'interdisciplinary', 'bring', 'together', 'people', 'around', 'world', 'unique', 'variety', 'background', 'including', 'physicist', 'planner', 'geographer', 'economist', 'data', 'scientist', 'architect', 'mathematician', 'computer', 'scientist', 'united', 'mission', 'tackle', 'biggest', 'challenge', 'facing', 'city', 'society', 'around', 'world', 'work', 'across', 'multiple', 'scale', 'hyper-local', 'environment', 'low-powered', 'sensor', 'way', 'satellite', 'remote', 'sensing', 'whole', 'country', 'region', 'studying', 'casa', 'brings', 'lifelong', 'value', 'student', 'poised', 'take', 'leadership', 'integration', 'role', 'forefront', 'urban', 'spatial', 'data', 'science', 'studying', 'u', 'become', 'part', 'active', 'engaged', 'alumnus', 'community', 'access', 'job', 'listing', 'networking', 'social', 'activity', 'well', 'continued', 'contact', 'outstanding', 'teacher', 'researcher', 'location', 'ucl', 'centre', 'advanced', 'spatial', 'analysis', 'located', '90', 'tottenham', 'court', 'road', 'london', 'w1t', '4tj']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stemmed\n\n>['centr', 'advanc', 'spatial', 'analysi', 'casa', 'interdisciplinari', 'research', 'institut', 'focus', 'scienc', 'citi', 'within', 'bartlett', 'faculti', 'built', 'environ', 'ucl']\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stemmed\n\n>['centr', 'advanc', 'spatial', 'analysi', 'casa', 'establish', '1995', 'lead', 'develop', 'scienc', 'citi', 'draw', 'upon', 'method', 'idea', 'model', 'data', 'scienc', 'sens', 'urban', 'environ', 'visualis', 'comput', 'today', 'casa', 'research', 'still', 'push', 'boundari', 'creat', 'better', 'citi', 'everyon', 'lead', 'intellectu', 'agenda', 'work', 'close', 'govern', 'industri', 'partner', 'make', 'real-world', 'impact', 'teach', 'reflect', 'make', 'cutting-edg', 'research', 'tool', 'new', 'form', 'data', 'long-stand', 'non-academ', 'partnership', 'train', 'next', 'generat', 'urban', 'scientist', 'skill', 'idea', 'need', 'impact', 'industri', 'govern', 'academia', 'third', 'sector', 'casa', 'communiti', 'close', 'connect', 'strong', 'interdisciplinari', 'bring', 'togeth', 'peopl', 'around', 'world', 'uniqu', 'varieti', 'background', 'includ', 'physicist', 'planner', 'geograph', 'economist', 'data', 'scientist', 'architect', 'mathematician', 'comput', 'scientist', 'unit', 'mission', 'tackl', 'biggest', 'challeng', 'face', 'citi', 'societi', 'around', 'world', 'work', 'across', 'multipl', 'scale', 'hyper-loc', 'environ', 'low-pow', 'sensor', 'way', 'satellit', 'remot', 'sens', 'whole', 'countri', 'region', 'studi', 'casa', 'bring', 'lifelong', 'valu', 'student', 'pois', 'take', 'leadership', 'integr', 'role', 'forefront', 'urban', 'spatial', 'data', 'scienc', 'studi', 'us', 'becom', 'part', 'activ', 'engag', 'alumni', 'communiti', 'access', 'job', 'list', 'network', 'social', 'activ', 'well', 'continu', 'contact', 'outstand', 'teacher', 'research', 'locat', 'ucl', 'centr', 'advanc', 'spatial', 'analysi', 'locat', '90', 'tottenham', 'court', 'road', 'london', 'w1t', '4tj']\n\n:::\n:::\n\n\n::: {#f60b0f11 .cell execution_count=39}\n``` {.python .cell-code}\n# What are we doing here?\nfor ix, p in enumerate(stopped):\n    stopped_set = set(stopped[ix])\n    lemma_set   = set(lemmas[ix])\n    print(sorted(stopped_set.symmetric_difference(lemma_set)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['cities', 'city']\n['activities', 'activity', 'alumni', 'alumnus', 'architect', 'architects', 'background', 'backgrounds', 'boundaries', 'boundary', 'challenge', 'challenges', 'cities', 'city', 'countries', 'country', 'economist', 'economists', 'form', 'forms', 'geographer', 'geographers', 'idea', 'ideas', 'listing', 'listings', 'mathematician', 'mathematicians', 'method', 'methods', 'partner', 'partners', 'partnership', 'partnerships', 'physicist', 'physicists', 'planner', 'planners', 'region', 'regions', 'researcher', 'researchers', 'role', 'roles', 'scale', 'scales', 'scientist', 'scientists', 'skill', 'skills', 'societies', 'society', 'student', 'students', 'teacher', 'teachers', 'tool', 'tools', 'u', 'us']\n```\n:::\n:::\n\n\n# Applying Normalisation\n\nThe above approach is fairly hard going since you need to loop through every list element applying these changes one at a time. Instead, we could convert the column to a corpus (or use pandas `apply`) together with a function imported from a library to do the work.\n\n## Downloading the Custom Module\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low.\n\n:::\n\nThis custom module is not perfect, but it gets the job done... mostly and has some additional features that you could play around with for a final project (e.g. `detect_entities` and `detect_acronyms`).\n\n::: {#46ebbe97 .cell execution_count=40}\n``` {.python .cell-code}\nimport urllib.request\nhost  = 'https://orca.casa.ucl.ac.uk'\nturl  = f'{host}/~jreades/__textual__.py'\ntdirs = os.path.join('textual')\ntpath = os.path.join(tdirs,'__init__.py')\n\nif not os.path.exists(tpath):\n    os.makedirs(tdirs, exist_ok=True)\n    urllib.request.urlretrieve(turl, tpath)\n```\n:::\n\n\n## Importing the Custom Module\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty Level: Low. \n\nBut only because you didn't have to write the module! However, the questions could be hard...\n\n:::\n\nIn a Jupyter notebook, this code allows us to edit and reload the library dynamically:\n\n```python\n%load_ext autoreload\n%autoreload 2\n```\n\nNow let's import it.\n\n::: {#35913b11 .cell execution_count=41}\n``` {.python .cell-code}\nfrom textual import *\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAll NLTK libraries installed...\n```\n:::\n:::\n\n\n::: {#68d81c68 .cell execution_count=42}\n``` {.python .cell-code}\nas_markdown('Input', cleaned)\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Input\n\n>['UCL Home The Bartlett Centre for Advanced Spatial Analysis About', 'About', 'The Centre for Advanced Spatial Analysis (CASA) is an interdisciplinary research institute focusing on the science of cities within The Bartlett Faculty of the Built Environment at UCL.', 'The Centre for Advanced Spatial Analysis (CASA) was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science, sensing the urban environment, visualisation and computation.  Today, CASA’s research is still pushing boundaries to create better cities for everyone, both leading the intellectual agenda and working closely with government and industry partners to make real-world impact.  Our teaching reflects this, making the most of our cutting-edge research, tools, new forms of data, and long-standing non-academic partnerships to train the next generation of urban scientists with the skills and ideas they’ll need to have an impact in industry, government, academia, and the third sector.  The CASA community is closely connected, but strongly interdisciplinary. We bring together people from around the world with a unique variety of backgrounds – including physicists, planners, geographers, economists, data scientists, architects, mathematicians and computer scientists – united by our mission to tackle the biggest challenges facing cities and societies around the world.  We work across multiple scales: from the hyper-local environment of the low-powered sensor all the way up to satellite remote sensing of whole countries and regions.  Studying at CASA brings lifelong value, with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science.  By studying with us you will become part of our active and engaged alumni community, with access to job listings, networking and social activities, as well as continued contact with our outstanding teachers and researchers.   Location The UCL Centre for Advanced Spatial Analysis is located at 90 Tottenham Court Road, London, W1T 4TJ.', 'View Map', 'Contact Address: UCL Centre for Advanced Spatial Analysis First Floor, 90 Tottenham Court Road London W1T 4TJ Telephone:  +44 (0)20 3108 3877 Email:   casa@ucl.ac.uk', 'Tweets by CASAUCL']\n\n:::\n:::\n\n\n::: {#b2f9073b .cell execution_count=43}\n``` {.python .cell-code}\nas_markdown('Normalised', [normalise_document(x, remove_digits=True) for x in cleaned])\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Normalised\n\n>['home bartlett centre advanced spatial analysis', '', 'centre advanced spatial analysis . casa . interdisciplinary research institute focus science city within bartlett faculty built environment .', 'centre advanced spatial analysis . casa . establish lead development science city draw upon method idea modelling data science  sense urban environment  visualisation computation . today  research still push boundary create good city everyone  lead intellectual agenda work closely government industry partner make realworld impact . teaching reflect  make cuttingedge research  tool  form data  longstanding nonacademic partnership train next generation urban scientist skill idea need impact industry  government  academia  third sector . casa community closely connect  strongly interdisciplinary . bring together people around world unique variety background include physicist  planner  geographer  economist  data scientist  architect  mathematician computer scientist unite mission tackle challenge face city society around world . work across multiple scale  hyperlocal environment lowpowered sensor satellite remote sensing whole country region . studying casa bring lifelong value  student poise take leadership integration role forefront urban spatial data science . study become part active engage alumnus community  access listing  network social activity  well continue contact outstanding teacher researcher . location centre advanced spatial analysis locate tottenham court road  london  .', 'view', 'contact address  centre advanced spatial analysis first floor  tottenham court road london telephone  . email  casa ucl.ac.uk', 'tweets casaucl']\n\n:::\n:::\n\n\n::: {#a9ee8f4c .cell execution_count=44}\n``` {.python .cell-code}\nhelp(normalise_document)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHelp on function normalise_document in module textual:\n\nnormalise_document(doc: str, html_stripping=True, contraction_expansion=True, accented_char_removal=True, text_lower_case=True, text_lemmatization=True, special_char_removal=False, punctuation_removal=True, keep_sentences=True, stopword_removal=True, remove_digits=False, infer_numbers=True, shortest_word=3) -> str\n    Apply all of the functions above to a document using their\n    default values so as to demonstrate the NLP process.\n\n    doc: a document to clean.\n\n```\n:::\n:::\n\n\n### Questions\n\nLet's assume that you want to analyse web page content... \n\n- Based on the above output, what stopwords do you think are missing?\n- Based on the above output, what should be removed but isn't?\n- Based on the above output, how do you think a computer can work with this text?\n\n::: {.callout-caution}\n\n#### Stop!\n\nBeyond this point, we are moving into Natural Language Processing. If you are already struggling with regular expressions, I would recommend _stopping here_. You can come back to revisit the NLP components and creation of word clouds later.\n\n:::\n\n# Revenons à Nos Moutons\n\nNow that you've seen how the steps are applied to a 'random' HTML document, let's get back to the problem at hand (revenons à nos moutons == let's get back to our sheep).\n\n## Process the Selected Listings\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low, but you'll need to be patient!\n\n:::\n\nNotice the use of `%%time` here -- this will tell you how long each block of code takes to complete. It's a really useful technique for reminding *yourself* and others of how long something might take to run. I find that with NLP this is particularly important since you have to do a *lot* of processing on each document in order to normalise it.\n\n::: {.callout-tip}\n\nNotice how we can change the default parameters for `normalise_document` even when using `apply`, but that the syntax is different. So whereas we'd use `normalise_document(doc, remove_digits=True)` if calling the function directly, here it's `.apply(normalise_document, remove_digits=True)`!\n\n:::\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\n%%time \n# I get about 1 minute on a M2 Mac\nlux['description_norm'] = lux.???.apply(???, remove_digits=True)\n```\n\n```python\n%%time \n# I get about 1 minute on a M2 Mac\naff['description_norm'] = aff.???.apply(???, remove_digits=True)\n```\n\n```python\n%%time  \n# I get about 2 seconds on a M2 Mac\nbluesp['description_norm'] = bluesp.???.apply(???, remove_digits=True)\n```\n\n#### Answer\n\n::: {#5195b04c .cell execution_count=45}\n``` {.python .cell-code}\n%%time\n# I get about 1 minute on a M2 Mac\nlux['description_norm'] = lux.description.apply(normalise_document, remove_digits=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/jreades/Documents/git/fsds/practicals/textual/__init__.py:606: MarkupResemblesLocatorWarning:\n\nThe input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 44 s, sys: 733 ms, total: 44.7 s\nWall time: 44.8 s\n```\n:::\n:::\n\n\n::: {#4884f59e .cell execution_count=46}\n``` {.python .cell-code}\n%%time\n# I get about 1 minute on a M2 Mac\naff['description_norm'] = aff.description.apply(normalise_document, remove_digits=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/jreades/Documents/git/fsds/practicals/textual/__init__.py:606: MarkupResemblesLocatorWarning:\n\nThe input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 37.3 s, sys: 639 ms, total: 37.9 s\nWall time: 37.9 s\n```\n:::\n:::\n\n\n::: {#c1735f85 .cell execution_count=47}\n``` {.python .cell-code}\n%%time\n# I get about 1 seconds on a M2 Mac\nbluesp['description_norm'] = bluesp.description.apply(normalise_document, remove_digits=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 1.62 s, sys: 28.5 ms, total: 1.65 s\nWall time: 1.65 s\n```\n:::\n:::\n\n\n::::\n\n## Select and Tokenise\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low, except for the double list-comprehension.\n\n:::\n\n### Select and Extract Corpus\n\nSee useful tutorial [here](https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275). Although we shouldn't have any empty descriptions, by the time we've finished normalising the textual data we may have _created_ some empty values and we need to ensure that we don't accidentally pass a NaN to the vectorisers and frequency distribution functions.\n\n::: {#51cc8927 .cell execution_count=48}\n``` {.python .cell-code}\nsrcdf = bluesp \n```\n:::\n\n\n::: {.callout-tip}\n\n#### Coding Tip\n\nNotice how you only need to change the value of the variable here to try any of the different selections we did above? This is a simple kind of parameterisation somewhere between a function and hard-coding everything.\n\n:::\n\n::: {#1f223224 .cell execution_count=49}\n``` {.python .cell-code}\ncorpus = srcdf.description_norm.fillna(' ').values\nprint(corpus[0:3])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['house garden close thames river . walk  private road river nearby .  district line underground . walk . direct access central london  near gardens . kids playground walk distance along thames path . space residential neighborhood  english corporate expat family . house culdesac private road river thames . river foot away . walking distance subway . central london underground   district line . gardens stop walk zone . addition  overground stratford also stop gardens underground station . gardens stop walk .  overland railway station bridge . walk . take waterloo railway station minute . bicycle follow towpath hammersmith bridge continue putney bridge . lastly  several stree'\n 'space appartment upper floor modernised secure building near canary wharf  fantastic view river thames london . offer home home experience  light breafast fresh juice coffee . double room plenty storage  fresh towels linen . shared bathroom step shower  hairdryer essentials . wifi service . appartment  minutes walk nearest . south quay . london underground . canary wharf . network central london right next stop . close asda tesco supermarkets chains  plenty local shop restaurants . walking distance city farm  greenwich foot tunnel fabulous walkway along river front . also hour medical center close appartment  questions'\n 'newly renovate  totally equipped furnished modern apartment heart london . easily accessible kind transport . walk waterloo station  london  . space newly renovate  totally equipped furnished modern apartment heart london . easily accessible kind transport . walk waterloo station  london  . apartment flat   broadband  wash machine  microwave  iron  nespresso coffee machine  . large live room easily divide space . modern comfortable bathroom whirlpool japanese . bedroom modern design closet . shared terrace view picturesque vivid lower marsh street   london . lower marsh london best hidden jewel  special alive place bridget jones  bourne identit']\n```\n:::\n:::\n\n\n### Tokenise\n\nThere are different forms of tokenisation and different algorithms will expect differing inputs. Here are two:\n\n::: {#5ef0d144 .cell execution_count=50}\n``` {.python .cell-code}\nsentences = [nltk.sent_tokenize(text) for text in corpus]\nwords     = [[nltk.tokenize.word_tokenize(sentence) \n                  for sentence in nltk.sent_tokenize(text)] \n                  for text in corpus]\n```\n:::\n\n\nNotice how this has turned every sentence into an array and each document into an array of arrays:\n\n::: {#34a87d22 .cell execution_count=51}\n``` {.python .cell-code}\nprint(f\"Sentences 0: {sentences[0]}\")\nprint()\nprint(f\"Words 0: {words[0]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSentences 0: ['house garden close thames river .', 'walk  private road river nearby .', 'district line underground .', 'walk .', 'direct access central london  near gardens .', 'kids playground walk distance along thames path .', 'space residential neighborhood  english corporate expat family .', 'house culdesac private road river thames .', 'river foot away .', 'walking distance subway .', 'central london underground   district line .', 'gardens stop walk zone .', 'addition  overground stratford also stop gardens underground station .', 'gardens stop walk .', 'overland railway station bridge .', 'walk .', 'take waterloo railway station minute .', 'bicycle follow towpath hammersmith bridge continue putney bridge .', 'lastly  several stree']\n\nWords 0: [['house', 'garden', 'close', 'thames', 'river', '.'], ['walk', 'private', 'road', 'river', 'nearby', '.'], ['district', 'line', 'underground', '.'], ['walk', '.'], ['direct', 'access', 'central', 'london', 'near', 'gardens', '.'], ['kids', 'playground', 'walk', 'distance', 'along', 'thames', 'path', '.'], ['space', 'residential', 'neighborhood', 'english', 'corporate', 'expat', 'family', '.'], ['house', 'culdesac', 'private', 'road', 'river', 'thames', '.'], ['river', 'foot', 'away', '.'], ['walking', 'distance', 'subway', '.'], ['central', 'london', 'underground', 'district', 'line', '.'], ['gardens', 'stop', 'walk', 'zone', '.'], ['addition', 'overground', 'stratford', 'also', 'stop', 'gardens', 'underground', 'station', '.'], ['gardens', 'stop', 'walk', '.'], ['overland', 'railway', 'station', 'bridge', '.'], ['walk', '.'], ['take', 'waterloo', 'railway', 'station', 'minute', '.'], ['bicycle', 'follow', 'towpath', 'hammersmith', 'bridge', 'continue', 'putney', 'bridge', '.'], ['lastly', 'several', 'stree']]\n```\n:::\n:::\n\n\n## Frequencies and Ngrams\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty level: Moderate.\n\n:::\n\nOne new thing you'll see here is the `ngram`: ngrams are 'simply' pairs, or triplets, or quadruplets of words. You may come across the terms unigram (`ngram(1,1)`), bigram (`ngram(2,2)`), trigram (`ngram(3,3)`)... typically, you will rarely find anything beyond trigrams, and these present real issues for text2vec algorithms because the embedding for `geographical`, `information`, and `systems` is _not_ the same as for `geographical information systetms`.\n\n### Build Frequency Distribution\n\nBuild counts for ngram range 1..3:\n\n::: {#76858c0c .cell execution_count=52}\n``` {.python .cell-code}\nfcounts = dict()\n\n# Here we replace all full-stops... can you think why we might do this?\ndata = nltk.tokenize.word_tokenize(' '.join([text.replace('.','') for text in corpus]))\n\nfor size in 1, 2, 3:\n    fdist = FreqDist(ngrams(data, size))\n    print(fdist)\n    # If you only need one note this: https://stackoverflow.com/a/52193485/4041902\n    fcounts[size] = pd.DataFrame.from_dict({f'Ngram Size {size}': fdist})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<FreqDist with 2692 samples and 26245 outcomes>\n<FreqDist with 14173 samples and 26244 outcomes>\n<FreqDist with 19540 samples and 26243 outcomes>\n```\n:::\n:::\n\n\n### Output Top-n Ngrams\n\nAnd output the most common ones for each ngram range:\n\n::: {#a6a1a414 .cell execution_count=53}\n``` {.python .cell-code}\nfor dfs in fcounts.values():\n    print(dfs.sort_values(by=dfs.columns.values[0], ascending=False).head(10))\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Ngram Size 1\nwalk                594\nroom                472\nlondon              469\nriver               418\nbedroom             412\nspace               391\nminute              382\napartment           373\nstation             307\nflat                303\n\n                 Ngram Size 2\nminute  walk              252\nriver   thames            138\n        view              138\nliving  room              134\ncanary  wharf             112\nguest   access            110\ncentral london            107\nfully   equip              76\nequip   kitchen            71\nthames  river              65\n\n                        Ngram Size 3\nfully  equip  kitchen             68\nwalk   river  thames              37\nclose  river  thames              35\nwalk   thames river               27\nminute walk   river               23\nopen   plan   kitchen             20\nthames river  view                20\nsofa   living room                19\nwithin walk   distance            19\nopen   plan   live                18\n\n```\n:::\n:::\n\n\n### Questions\n\n- Can you think why we don't care about punctuation for frequency distributions and n-grams?\n- Do you understand what n-grams *are*?\n\n## Count Vectoriser\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low, but the output needs some thought!\n\n:::\n\nThis is a big foray into sklearn (sci-kit learn) which is the main machine learning and clustering module for Python. For processing text we use *vectorisers* to convert terms to a vector representation. We're doing this on the smallest of the derived data sets because these processes can take a while to run and generate *huge* matrices (remember: one row and one column for each term!).\n\n### Fit the Vectoriser\n\n::: {#2820632e .cell execution_count=54}\n``` {.python .cell-code}\ncvectorizer = CountVectorizer(ngram_range=(1,3))\ncvectorizer.fit(corpus)\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(ngram_range=(1, 3))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(ngram_range=(1, 3))</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n### Brief Demonstration\n\nFind the number associated with a word in the vocabulary and how many times it occurs in the original corpus:\n\n::: {#63471fee .cell execution_count=55}\n``` {.python .cell-code}\nterm = 'stratford'\npd.options.display.max_colwidth=750\n# Find the vocabulary mapping for the term\nprint(f\"Vocabulary mapping for {term} is {cvectorizer.vocabulary_[term]}\")\n# How many times is it in the data\nprint(f\"Found {srcdf.description_norm.str.contains(term).sum():,} rows containing {term}\")\n# Print the descriptions containing the term\nfor x in srcdf[srcdf.description_norm.str.contains(term)].description_norm:\n    as_markdown('Stratford',x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary mapping for stratford is 29373\nFound 10 rows containing stratford\n```\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>house garden close thames river . walk  private road river nearby .  district line underground . walk . direct access central london  near gardens . kids playground walk distance along thames path . space residential neighborhood  english corporate expat family . house culdesac private road river thames . river foot away . walking distance subway . central london underground   district line . gardens stop walk zone . addition  overground stratford also stop gardens underground station . gardens stop walk .  overland railway station bridge . walk . take waterloo railway station minute . bicycle follow towpath hammersmith bridge continue putney bridge . lastly  several stree\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>please read things note  comfortable  clean  bright brand flat east london  minute central london tube  quite central great transport link  major london attraction  minute walk river  park  undergroundtubedlr station  supermarket  docklands  stratford olympic stadium westfield shopping centre . enjoy brick lane indian restaurant  spitalfields market  colombian flower market  historical whitechapel . space please read things note  nice  clean  fresh  bright airy . space perfect professional  single person couple . make feel like home choice anything like wake  relaxing  cooking . guest access please read things note  entire flat . please treat home away . please treat .\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>comfortable fairly flat east london  travel zone  minute central london  quite central great transport link  major london attraction  minute walk river  park  undergroundtube station minute  supermarket minute  docklands  stratford olympic stadium westfield shopping centre . enjoy brick lane indian restaurant  spitalfields market  colombian flower market  historical whitechapel . space spacious  comfortable  tidy  clean  airy relaxing . live flat sleep open plan lounge balcony . guest access bathroom share . welcome microwave ready meal  toaster  make drink till fridge store food . please make sure clean clear immediately . dining table . stay present morning  evening weekend . also\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>entire appartment double bedroom large living area.the apartment feature kitchen come free wifi flat screen tv.to make exicting luxurious even free free sauna well . space stunning apartment london docklands bank thames river  close thames barrier park canary wharf . apartment floor spacious living room balcony . nearest station pontoon dock walkable distance direct train stratford . mins . bank . excel centre . walk .  arena canary wharf mins train mins central london . world heritage sitethames barrier thames barrier park walk appartment . london city airport train station away . fully kitchen bathroom broadband internet   underground secure parking  onsite . attract\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>luxurious bedroom apartment zone love hidden secret part town  minute away everywhere river view slow pace main artery town  right doorstep well hidden beauty park waterway . easy . walk tube route center town well stratford olympic park  canary wharf much much right doorstep space welcome home place love bedroom hold personal belonging  bedroom give  guest idea size . bedroom large double accommodate comfortably . sofa chair accommodate guest extra extra charge . welcome guest personally wish know . therefore  important check time convenient . midnight arrival . time ehich discuss good\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>place close mile tube station  brick lane shoreditch  queen mary university london  stratford westfield  minute tube central london  . love place newly renovate flat amazing canal view guest bedroom  clean friendly environment . place good couple  solo adventurer  business traveller .\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>locate high street  give amazing water view stadium sight amazing architectural structure walk pudding mill lane walk abba walk stratford westfield walk stratfordstratford international station mins walk mins train ride central london\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>modern spacious bedroom suite apartment close river thames wimbledon . situate wandsworth district london  england lawn tennis club centre court  .km clapham junction . stratford bridge chelsea . city view free wifi throughout property . apartment feature bedroom  kitchen fridge  oven wash machine  flat screen  seating area bathroom shower . eventim .km away .\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>perfect group trip . modern spacious suite apartment close river thames wimbledon . situated wandsworth district london  england lawn tennis club centre court  .km clapham junction . stratford bridge chelsea . city view free wifi throughout property . apartment feature bedroom  kitchen wfridge  oven  wash machine  flat screen  seating area bathroom wshower . eventim .km away .\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Stratford\n\n>flat locate zone east london near canary wharf . nice quiet residential area canal . flat amazing canal view balcony . enjoy morning coffee swan goose everyday . huge park opposite flat picnic . canary wharf shop mall . mins bank stratford westfield . mins central oxford circus tube . locate convenient transportation link .\n\n:::\n:::\n\n\n### Transform the Corpus \n\nYou can only *tranform* the entire corpus *after* the vectoriser has been fitted. There is an option to `fit_transform` in one go, but I wanted to demonstrate a few things here and some vectorisers are don't support the one-shot fit-and-transform approach. **Note the type of the transformed corpus**:\n\n::: {#aae4be89 .cell execution_count=56}\n``` {.python .cell-code}\ncvtcorpus = cvectorizer.transform(corpus)\ncvtcorpus # cvtcorpus for count-vectorised transformed corpus\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n<408x35278 sparse matrix of type '<class 'numpy.int64'>'\n\twith 71420 stored elements in Compressed Sparse Row format>\n```\n:::\n:::\n\n\n### Single Document\n\nHere is the **first** document from the corpus:\n\n::: {#d33a5511 .cell execution_count=57}\n``` {.python .cell-code}\ndoc_df = pd.DataFrame(cvtcorpus[0].T.todense(), \n                      index=cvectorizer.get_feature_names_out(), columns=[\"Counts\"]\n                     ).sort_values('Counts', ascending=False)\ndoc_df.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>walk</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>gardens</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>river</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>bridge</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>stop</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>thames</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>station</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>underground</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>railway</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>central london</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Transformed Corpus\n\n::: {#ff763a98 .cell execution_count=58}\n``` {.python .cell-code}\ncvdf = pd.DataFrame(data=cvtcorpus.toarray(),\n                        columns=cvectorizer.get_feature_names_out())\nprint(f\"Raw count vectorised data frame has {cvdf.shape[0]:,} rows and {cvdf.shape[1]:,} columns.\")\ncvdf.iloc[0:5,0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRaw count vectorised data frame has 408 rows and 35,278 columns.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=58}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaathe</th>\n      <th>aaathe apartment</th>\n      <th>aaathe apartment quiet</th>\n      <th>aand</th>\n      <th>aand comfy</th>\n      <th>aand comfy sofa</th>\n      <th>abba</th>\n      <th>abba arena</th>\n      <th>abba arena entertainmentconcerts</th>\n      <th>abba walk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Filter Low-Frequency Words\n\nThese are likely to be artefacts of text-cleaning or human input error. As well, if we're trying to look across an entire corpus then we might not want to retain words that only appear in a couple of documents.\n\nLet's start by getting the *column* sums:\n\n::: {#7226c0af .cell execution_count=59}\n``` {.python .cell-code}\nsums = cvdf.sum(axis=0)\nprint(f\"There are {len(sums):,} terms in the data set.\")\nsums.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are 35,278 terms in the data set.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\naaathe                    1\naaathe apartment          1\naaathe apartment quiet    1\naand                      1\naand comfy                1\ndtype: int64\n```\n:::\n:::\n\n\nRemove columns (i.e. terms) appearing in less than 1% of documents. You can do this by thinking about what the shape of the data frame means (rows and/or columns) and how you'd get 1% of that!\n\n:::: {.panel-tabset}\n\n#### Question\n\n```python\nfilter_terms = sums >= cvdf.shape[0] * ???\n```\n\n#### Answer\n\n::: {#be21f2e6 .cell execution_count=60}\n``` {.python .cell-code}\nfilter_terms = sums >= cvdf.shape[0] * 0.01\n```\n:::\n\n\n::::\n\nNow see how we can use this to strip out the columns corresponding to low-frequency terms:\n\n::: {#7deabc24 .cell execution_count=61}\n``` {.python .cell-code}\nfcvdf = cvdf.drop(columns=cvdf.columns[~filter_terms].values)\nprint(f\"Filtered count vectorised data frame has {fcvdf.shape[0]:,} rows and {fcvdf.shape[1]:,} columns.\")\nfcvdf.iloc[0:5,0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFiltered count vectorised data frame has 408 rows and 2,043 columns.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=61}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>access</th>\n      <th>access access</th>\n      <th>access bathroom</th>\n      <th>access central</th>\n      <th>access central london</th>\n      <th>access entire</th>\n      <th>access everything</th>\n      <th>access full</th>\n      <th>access guest</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#60595851 .cell execution_count=62}\n``` {.python .cell-code}\nfcvdf.sum(axis=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\nable                         8\naccess                     242\naccess access                7\naccess bathroom              5\naccess central               8\n                          ... \nzone comfortable cosy        7\nzone near                    5\nzone near underground        5\nzone recently               10\nzone recently refurbish     10\nLength: 2043, dtype: int64\n```\n:::\n:::\n\n\nWe're going to pick this up again in Task 7.\n\n### Questions\n\n- Can you explain what `doc_df` contains?\n- What does `cvdf` contain? Explain the rows and columns.\n- What is the function of `filter_terms`?\n\n## TF/IDF Vectoriser\n\n::: {.callout-warning collapse=\"true\"}\n\n#### Difficulty level: Moderate\n\nBut only if you want to understand how `max_df` and `min_df` work!\n\n:::\n\n### Fit and Transform\n\n::: {#27fe92ed .cell execution_count=63}\n``` {.python .cell-code}\ntfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), \n                               max_df=0.75, min_df=0.01) # <-- these matter!\ntftcorpus    = tfvectorizer.fit_transform(corpus) # TF-transformed corpus\n```\n:::\n\n\n### Single Document\n\n::: {#5bb74a63 .cell execution_count=64}\n``` {.python .cell-code}\ndoc_df = pd.DataFrame(tftcorpus[0].T.todense(), index=tfvectorizer.get_feature_names_out(), columns=[\"Weights\"])\ndoc_df.sort_values('Weights', ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>gardens</th>\n      <td>0.414885</td>\n    </tr>\n    <tr>\n      <th>stop</th>\n      <td>0.241659</td>\n    </tr>\n    <tr>\n      <th>district line</th>\n      <td>0.239192</td>\n    </tr>\n    <tr>\n      <th>railway</th>\n      <td>0.232131</td>\n    </tr>\n    <tr>\n      <th>underground</th>\n      <td>0.201738</td>\n    </tr>\n    <tr>\n      <th>district</th>\n      <td>0.197221</td>\n    </tr>\n    <tr>\n      <th>bridge</th>\n      <td>0.191983</td>\n    </tr>\n    <tr>\n      <th>walk</th>\n      <td>0.189485</td>\n    </tr>\n    <tr>\n      <th>road</th>\n      <td>0.151163</td>\n    </tr>\n    <tr>\n      <th>distance</th>\n      <td>0.142999</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Transformed Corpus\n\n::: {#ee440031 .cell execution_count=65}\n``` {.python .cell-code}\ntfidf = pd.DataFrame(data=tftcorpus.toarray(),\n                        columns=tfvectorizer.get_feature_names_out())\nprint(f\"TF/IDF data frame has {tfidf.shape[0]:,} rows and {tfidf.shape[1]:,} columns.\")\ntfidf.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTF/IDF data frame has 408 rows and 1,911 columns.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=65}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>access</th>\n      <th>access access</th>\n      <th>access bathroom</th>\n      <th>access central</th>\n      <th>access central london</th>\n      <th>access entire</th>\n      <th>access everything</th>\n      <th>access full</th>\n      <th>access guest</th>\n      <th>...</th>\n      <th>young</th>\n      <th>zone</th>\n      <th>zone central</th>\n      <th>zone central city</th>\n      <th>zone comfortable</th>\n      <th>zone comfortable cosy</th>\n      <th>zone near</th>\n      <th>zone near underground</th>\n      <th>zone recently</th>\n      <th>zone recently refurbish</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.043972</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.11031</td>\n      <td>0.11031</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.069702</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.044127</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1911 columns</p>\n</div>\n```\n:::\n:::\n\n\n### Questions\n\n- What does the TF/IDF score *represent*?\n- What is the role of `max_df` and `min_df`?\n\n# Word Clouds\n\n## For Counts\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Easy!\n\n:::\n\n::: {#0ef9d2b1 .cell execution_count=66}\n``` {.python .cell-code}\nfcvdf.sum().sort_values(ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\nwalk                      595\nroom                      472\nlondon                    471\nriver                     418\nbedroom                   412\n                         ... \nchic                        5\nterm                        5\nchoice                      5\nteddington                  5\nlondon aquarium minute      5\nLength: 2043, dtype: int64\n```\n:::\n:::\n\n\n::: {#91d36171 .cell execution_count=67}\n``` {.python .cell-code}\nff = 'RobotoMono-VariableFont_wght.ttf'\ndp = '/home/jovyan/.local/share/fonts/'\ntp = os.path.join(os.path.expanduser('~'),'Library','Fonts')\nif os.path.exists(tp):\n    fp = os.path.join(tp,ff)\nelse:\n    fp = os.path.join(dp,ff)\n```\n:::\n\n\n::: {#ddb64c85 .cell execution_count=68}\n``` {.python .cell-code}\nf,ax = plt.subplots(1,1,figsize=(8, 8))\nplt.gcf().set_dpi(150)\nCloud = WordCloud(\n    background_color=\"white\", \n    max_words=75,\n    font_path=fp\n).generate_from_frequencies(fcvdf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n#plt.savefig(\"Wordcloud 1.png\")\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-69-output-1.png){width=959 height=495}\n:::\n:::\n\n\n## For TF/IDF Weighting\n\n::: {.callout-tip collapse=\"true\"}\n\n#### Difficulty level: Low, but you'll need to be patient!\n\n:::\n\n::: {#9a04dfad .cell execution_count=69}\n``` {.python .cell-code}\ntfidf.sum().sort_values(ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\nwalk                     23.037285\nroom                     19.135852\nlondon                   18.744519\nminute                   18.650909\napartment                18.082855\n                           ...    \nstation apartment one     0.401426\nstation also close        0.401426\napartment one benefit     0.401426\napartment one             0.401426\nalso close station        0.401426\nLength: 1911, dtype: float64\n```\n:::\n:::\n\n\n::: {#ece00142 .cell execution_count=70}\n``` {.python .cell-code}\nf,ax = plt.subplots(1,1,figsize=(8, 8))\nplt.gcf().set_dpi(150)\nCloud = WordCloud(\n    background_color=\"white\", \n    max_words=100,\n    font_path=fp\n).generate_from_frequencies(tfidf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n#plt.savefig(\"Wordcloud 2.png\")\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-71-output-1.png){width=959 height=495}\n:::\n:::\n\n\n### Questions\n\n- What does the `sum` represent for the count vectoriser?\n- What does the `sum` represent for the TF/IDF vectoriser?\n\n# Latent Dirchlet Allocation\n\n::: {.callout-tip}\n\nI would give this a _low_ priority. It's a commonly-used method, but on small data sets it really isn't much use and I've found its answers to be... unclear... even on large data sets.\n\n:::\n\nAdapted from [this post](https://stackabuse.com/python-for-nlp-topic-modeling/) on doing LDA using sklearn. Most other examples use the `gensim` library.\n\n::: {#80dc2dac .cell execution_count=71}\n``` {.python .cell-code}\n# Notice change to ngram range \n# (try 1,1 and 1,2 for other options)\nvectorizer = CountVectorizer(ngram_range=(1,2))\n```\n:::\n\n\n## Calculate Topics\n\n::: {#0f297891 .cell execution_count=72}\n``` {.python .cell-code}\nvectorizer.fit(corpus) \ntcorpus = vectorizer.transform(corpus) # tcorpus for transformed corpus\n\nLDA = LatentDirichletAllocation(n_components=3, random_state=42) # Might want to experiment with n_components too\nLDA.fit(tcorpus)\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```{=html}\n<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LatentDirichletAllocation<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">?<span>Documentation for LatentDirichletAllocation</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LatentDirichletAllocation(n_components=3, random_state=42)</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n::: {#67560c39 .cell execution_count=73}\n``` {.python .cell-code}\nfirst_topic = LDA.components_[0]\ntop_words = first_topic.argsort()[-25:]\n\nfor i in top_words:\n    print(vectorizer.get_feature_names_out()[i])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nriver thames\nmodern\narea\nwharf\nflat\naccess\nbathroom\nguest\nhouse\nminute\nprivate\nkitchen\nlarge\nthames\nliving\nview\nstation\nfloor\nbedroom\napartment\nwalk\nlondon\nriver\nroom\nspace\n```\n:::\n:::\n\n\n::: {#14e46d34 .cell execution_count=74}\n``` {.python .cell-code}\nfor i,topic in enumerate(LDA.components_):\n    as_markdown(f'Top 10 words for topic #{i}', ', '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]]))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Top 10 words for topic #0\n\n>river thames, modern, area, wharf, flat, access, bathroom, guest, house, minute, private, kitchen, large, thames, living, view, station, floor, bedroom, apartment, walk, london, river, room, space\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Top 10 words for topic #1\n\n>park, fully, modern, private, living, guest, view, double, close, access, area, flat, thames, bathroom, station, apartment, kitchen, space, minute walk, room, london, river, bedroom, minute, walk\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n##### Top 10 words for topic #2\n\n>living, river view, canary wharf, canary, central, wharf, stay, close, bathroom, guest, kitchen, double, minute, thames, access, station, flat, space, view, bedroom, apartment, river, walk, london, room\n\n:::\n:::\n\n\n## Maximum Likelihood Topic\n\n::: {#8aeeee04 .cell execution_count=75}\n``` {.python .cell-code}\ntopic_values = LDA.transform(tcorpus)\ntopic_values.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=75}\n```\n(408, 3)\n```\n:::\n:::\n\n\n::: {#115397ff .cell execution_count=76}\n``` {.python .cell-code}\npd.options.display.max_colwidth=20\nsrcdf['Topic'] = topic_values.argmax(axis=1)\nsrcdf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=76}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geometry</th>\n      <th>listing_url</th>\n      <th>name</th>\n      <th>description</th>\n      <th>amenities</th>\n      <th>price</th>\n      <th>description_norm</th>\n      <th>Topic</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36299</th>\n      <td>POINT (519476.34...</td>\n      <td>https://www.airb...</td>\n      <td>Townhouse in Ric...</td>\n      <td>3 Bed House with...</td>\n      <td>[\"Bathtub\", \"Sha...</td>\n      <td>245.0</td>\n      <td>house garden clo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>170702</th>\n      <td>POINT (537106.00...</td>\n      <td>https://www.airb...</td>\n      <td>Rental unit in L...</td>\n      <td>&lt;b&gt;The space&lt;/b&gt;...</td>\n      <td>[\"Heating\", \"Ele...</td>\n      <td>60.0</td>\n      <td>space appartment...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>608438</th>\n      <td>POINT (530947.63...</td>\n      <td>https://www.airb...</td>\n      <td>Rental unit in G...</td>\n      <td>Newly renovated,...</td>\n      <td>[\"Bathtub\", \"Sha...</td>\n      <td>169.0</td>\n      <td>newly renovate  ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>827436</th>\n      <td>POINT (539810.14...</td>\n      <td>https://www.airb...</td>\n      <td>Rental unit in L...</td>\n      <td>Brand new rivers...</td>\n      <td>[\"Waterfront\", \"...</td>\n      <td>191.0</td>\n      <td>brand riverside ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1580666</th>\n      <td>POINT (538100.19...</td>\n      <td>https://www.airb...</td>\n      <td>Rental unit in L...</td>\n      <td>PLEASE READ OTHE...</td>\n      <td>[\"Heating\", \"Ele...</td>\n      <td>79.0</td>\n      <td>please read thin...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#e9399c99 .cell execution_count=77}\n``` {.python .cell-code}\npd.options.display.max_colwidth=75\nsrcdf[srcdf.Topic==1].description_norm.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\nid\n36299      house garden close thames river . walk  private road river nearby .  di...\n827436     brand riverside apartment greenwich peninsula . perfect explore london ...\n1611173    mint walk thames river  mint tower bridge  mint greenwich bus mint walk...\n1910741    bright  spacious chill bedroom cosy apartment level friendly quiet bloc...\n5284354    fabulous bathshower flat modern development east putney  london literal...\n5498948    large double room river view available large spacious townhouse hampton...\n6479011    private quiet bright  large double room ensuite bathroom . location stu...\n6619558    bright  airy bedroom  ground floor apartment quiet street . modernly fu...\n7495122    perfect claphambattersea modern decor . brand kitchenbathroom minute wa...\n8352150    room front house overlook balcony  road river . bright sunny house view...\nName: description_norm, dtype: object\n```\n:::\n:::\n\n\n::: {#c5879e0a .cell execution_count=78}\n``` {.python .cell-code}\nvectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', analyzer='word', max_df=0.7, min_df=0.05)\ntopic_corpus = vectorizer.fit_transform(srcdf[srcdf.Topic==1].description.values) # tcorpus for transformed corpus\n```\n:::\n\n\n::: {#80d1dfd6 .cell execution_count=79}\n``` {.python .cell-code}\ntopicdf = pd.DataFrame(data=topic_corpus.toarray(),\n                        columns=vectorizer.get_feature_names_out())\n```\n:::\n\n\n::: {#117aceae .cell execution_count=80}\n``` {.python .cell-code}\nf,ax = plt.subplots(1,1,figsize=(8,8))\nplt.gcf().set_dpi(150)\nCloud = WordCloud(\n            background_color=\"white\", \n            max_words=75).generate_from_frequencies(topicdf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n# plt.savefig('Wordcloud 3.png')\n```\n\n::: {.cell-output .cell-output-display}\n![](Practical-07-Textual_Data-Pt2_files/figure-html/cell-81-output-1.png){width=959 height=495}\n:::\n:::\n\n\n# Word2Vec\n\n::: {.callout-tip}\n\nThis algorithm works almost like magic. You should play with the configuration parameters and see how it changes your results.\n\n:::\n\n## Configure\n\n::: {#c1cb9e6b .cell execution_count=81}\n``` {.python .cell-code}\nfrom gensim.models.word2vec import Word2Vec\n```\n:::\n\n\n::: {#a6df75c1 .cell execution_count=82}\n``` {.python .cell-code}\ndims = 100\nprint(f\"You've chosen {dims} dimensions.\")\n\nwindow = 3\nprint(f\"You've chosen a window of size {window}.\")\n\nmin_v_freq  = 0.005 # Don't keep words appearing less than 0.5% frequency\nmin_v_count = math.ceil(min_v_freq * srcdf.shape[0])\nprint(f\"With a minimum frequency of {min_v_freq} and {srcdf.shape[0]:,} documents, minimum vocab frequency is {min_v_count:,}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYou've chosen 100 dimensions.\nYou've chosen a window of size 3.\nWith a minimum frequency of 0.005 and 408 documents, minimum vocab frequency is 3.\n```\n:::\n:::\n\n\n## Train\n\n::: {#dc0d3bbf .cell execution_count=83}\n``` {.python .cell-code}\n%%time \n\ncorpus      = srcdf.description_norm.fillna(' ').values\n#corpus_sent = [nltk.sent_tokenize(text) for text in corpus] # <-- with more formal writing this would work well\ncorpus_sent = [d.replace('.',' ').split(' ') for d in corpus] # <-- deals better with many short sentences though context may end up... weird\nmodel       = Word2Vec(sentences=corpus_sent, vector_size=dims, window=window, epochs=200, \n                 min_count=min_v_count, seed=42, workers=1)\n\n#model.save(f\"word2vec-d{dims}-w{window}.model\") # <-- You can then Word2Vec.load(...) which is useful with large corpora\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 4.22 s, sys: 68.1 ms, total: 4.29 s\nWall time: 4.27 s\n```\n:::\n:::\n\n\n## Explore Similarities\n\nThis next bit of code only runs if you have calculated the frequencies above in the [Frequencies and Ngrams](#frequencies-and-ngrams) section.\n\n::: {#c760d33c .cell execution_count=84}\n``` {.python .cell-code}\npd.set_option('display.max_colwidth',150)\n\ndf = fcounts[1] # <-- copy out only the unigrams as we haven't trained anything else\n\nn     = 14 # number of words\ntopn  = 7  # number of most similar words\n\nselected_words = df[df['Ngram Size 1'] > 5].reset_index().level_0.sample(n, random_state=42).tolist()\n\nwords = []\nv1    = []\nv2    = []\nv3    = []\nsims  = []\n\nfor w in selected_words:\n    try: \n        vector = model.wv[w]  # get numpy vector of a word\n        #print(f\"Word vector for '{w}' starts: {vector[:5]}...\")\n    \n        sim = model.wv.most_similar(w, topn=topn)\n        #print(f\"Similar words to '{w}' include: {sim}.\")\n    \n        words.append(w)\n        v1.append(vector[0])\n        v2.append(vector[1])\n        v3.append(vector[2])\n        sims.append(\", \".join([x[0] for x in sim]))\n    except KeyError:\n        print(f\"Didn't find {w} in model. Can happen with low-frequency terms.\")\n    \nvecs = pd.DataFrame({\n    'Term':words,\n    'V1':v1, \n    'V2':v2, \n    'V3':v3,\n    f'Top {topn} Similar':sims\n})\n\nvecs\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Term</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>Top 7 Similar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>complimentary</td>\n      <td>-0.957243</td>\n      <td>-1.486986</td>\n      <td>-1.123290</td>\n      <td>professionally, sustainable, considerate, hair, mbps, essentials, unlimited</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>equip</td>\n      <td>-1.839925</td>\n      <td>0.654449</td>\n      <td>-0.097641</td>\n      <td>equipped, diner, fullyequipped, flatscreen, plan, bathroom, sofabed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>shower</td>\n      <td>0.048209</td>\n      <td>0.343932</td>\n      <td>2.158660</td>\n      <td>separate, corridor, toilet, shared, bathroom, multiple, bathtub</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>smart</td>\n      <td>1.519984</td>\n      <td>-1.943256</td>\n      <td>-0.310231</td>\n      <td>flatscreen, wireless, oled, inch, netflix, streaming, comfy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>design</td>\n      <td>0.511491</td>\n      <td>1.845216</td>\n      <td>-1.068899</td>\n      <td>interior, beautifully, comfort, standout, chic, decor, feeling</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>appliance</td>\n      <td>0.281022</td>\n      <td>0.608689</td>\n      <td>-1.014925</td>\n      <td>functional, essential, necessary, kitchenette, equipped, fridge, timber</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>living</td>\n      <td>-1.026039</td>\n      <td>0.176004</td>\n      <td>-1.438652</td>\n      <td>live, separate, room, modern, table, double, ensuit</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>the</td>\n      <td>-3.042268</td>\n      <td>-3.073883</td>\n      <td>1.338165</td>\n      <td>londonelephant, furnished, both, equip, zone, couple, castle</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>directly</td>\n      <td>0.288156</td>\n      <td>-1.780736</td>\n      <td>-0.016401</td>\n      <td>pick, excel, crossharbour, incredible, themes, center, undergroundtube</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fridgefreezer</td>\n      <td>-0.665757</td>\n      <td>0.406782</td>\n      <td>0.152970</td>\n      <td>cutlery, americanstyle, refrigerator, kettle, freezer, toaster, stave</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>bathtub</td>\n      <td>-1.493979</td>\n      <td>1.130018</td>\n      <td>0.971623</td>\n      <td>bath, walkin, ensuite, mirror, toilet, fridge, bathshower</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>train</td>\n      <td>-1.821561</td>\n      <td>-1.496900</td>\n      <td>-0.311242</td>\n      <td>taxi, elizabeth, district, tube, crossharbour, kingston, greenland</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>palace</td>\n      <td>1.628689</td>\n      <td>1.310075</td>\n      <td>-1.671952</td>\n      <td>buckingham, hyde, sloane, min, parliament, houses, cathedral</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>shard</td>\n      <td>0.224605</td>\n      <td>-0.295689</td>\n      <td>-1.599193</td>\n      <td>globe, cathedral, shoredich, borough, tower, southbank, gatwick</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n```python\n#print(model.wv.index_to_key) # <-- the full vocabulary that has been trained\n```\n\n## Apply\n\nWe're going to make *use* of this further next week...\n\n### Questions\n\n- What happens when *dims* is very small (e.g. 25) or very large (e.g. 300)?\n- What happens when *window* is very small (e.g. 2) or very large (e.g. 8)?\n\n#0. Processing the Full File\n\n::: {.callout-caution}\n\nThis code can take *some time* (**&gt; 5 minutes on a M2 Mac**) to run, so **don't run this** until you've understood what we did before!\n\n:::\n\nYou will get a warning about `\".\" looks like a filename, not markup` — this looks a little scary, but is basically suggesting that we have a description that consists only of a '.' or that looks like some kind of URL (which the parser thinks means you're trying to pass it something to download). \n\n```python\n%%time \n# This can take up to 8 minutes on a M2 Mac\ngdf['description_norm'] = ''\ngdf['description_norm'] = gdf.description.apply(normalise_document, remove_digits=True, special_char_removal=True)\n```\n\n```python\ngdf.to_parquet(os.path.join('data','geo',f'{fn.replace(\".\",\"-with-nlp.\")}'))\n```\n\n::: {.callout-tip}\n\nSaving an intermediate file at this point is useful because you've done quite a bit of _expensive_ computation. You _could_ restart-and-run-all and then go out for the day, but probably easier to just save this output and then, if you need to restart your analysis at some point in the future, just remember to deserialise amenities back into a list format.\n\n:::\n\n## Applications\n\nThe above is _still_ only the results for the 'luxury' apartments _alone_. At this point, you would probably want to think about how your results might change if you changed any of the following:\n\n1. Using one of the other data sets that we created, or even the entire data set!\n2. Applying the CountVectorizer or TfidfVectorizer _before_ selecting out any of our 'sub' data sets.\n3. Using the visualisation of information from \\#2 to improve our regex selection process.\n4. Reducing, increasing, or constraining (i.e. `ngrams=(2,2)`) the size of the ngrams while bearing in mind the impact on processing time and interpretability.\n5. Filtering by type of listing or host instead of keywords found in the description (for instance, what if you applied TF/IDF to the entire data set and then selected out 'Whole Properties' before splitting into those advertised by hosts with only one listing vs. those with multiple listings?).\n6. Linking this back to the geography.\n\nOver the next few weeks we'll also consider alternative means of visualising the data!\n\n## Resources\n\nThere is a lot more information out there, including a [whole book](https://www.nltk.org/book/) and your standard [O'Reilly text](http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf).\n\nAnd some more useful links:\n\n- [Pandas String Contains Method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html)\n- [Using Regular Expressions with Pandas](https://kanoki.org/2019/11/12/how-to-use-regex-in-pandas/)\n- [Summarising Chapters from Frankenstein using TF/IDF](https://towardsdatascience.com/using-tf-idf-to-form-descriptive-chapter-summaries-via-keyword-extraction-4e6fd857d190)\n\n",
    "supporting": [
      "Practical-07-Textual_Data-Pt2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}